{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4d45314b-4640-4b28-a007-5ce7fc69d10e","_uuid":"803ba111ec20647635e573ab49538ceb061bcd3d","collapsed":true,"trusted":false},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","from sklearn.decomposition import PCA\n","from sklearn.cluster import KMeans\n","from sklearn.manifold import TSNE\n","from sklearn.preprocessing import StandardScaler\n","\n","import plotly.graph_objs as go\n","import plotly.offline as offline\n","offline.init_notebook_mode()"]},{"cell_type":"markdown","metadata":{},"source":["Credits: based on https: // www.kaggle.com/crawford/principle-component-analysis-gene-expression/notebook"]},{"cell_type":"markdown","metadata":{},"source":["# PARTE 1: PCA con los datos de genes de (Golub et al.) https://www.kaggle.com/crawford/principle-component-analysis-gene-expression/\n","\n","Datos usados para clasificar pacientes con acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL).\n","\n","Golub et al \"Molecular Classification of Cancer: Class Discovery and Class\n","Prediction by Gene Expression Monitoring\"\n","\n","There are two datasets containing the initial (training, 38 samples) and independent (test, 34 samples) datasets used in the paper. These datasets contain measurements corresponding to ALL and AML samples from Bone Marrow and Peripheral Blood. Intensity values have been re-scaled such that overall intensities for each chip are equivalent."]},{"cell_type":"markdown","metadata":{"_cell_guid":"2491bdb1-ee55-4545-b41a-c427e1dc7718","_uuid":"b9b5f2a9ce59d7086e508751553e7d8392c390ad"},"source":["# Analysis steps\n","\n","1. Remove columns that contain \"Call\" data\n","2. Transpose the dataframe so that each row is a patient and each column is a gene\n","3. Remove gene description header and set the gene accession numbers as the column headers\n","4. Split into train/test sets\n","5. Scale values to zero mean and unit varaince\n","6. PCA analysis\n","7. To do: K-means cluster"]},{"cell_type":"markdown","metadata":{},"source":["## (1) Load data"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6a0cfafc-8afa-41c6-ba7a-fa61596bab1f","_execution_state":"idle","_kg_hide-input":false,"_uuid":"05345f350192b24c9ea7dac7e36e673f5f6a9db8","collapsed":true,"trusted":false},"outputs":[],"source":["testfile = '../Datasets/genes/data_set_ALL_AML_independent.csv'\n","trainfile = '../Datasets/genes/data_set_ALL_AML_train.csv'\n","labels = '../Datasets/genes/genes.actual.csv'\n","\n","X_train = pd.read_csv(trainfile)\n","X_test = pd.read_csv(testfile)\n","y = pd.read_csv(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f07b2efe-d87c-42b9-8d7d-f774cd3cb78b","_execution_state":"idle","_kg_hide-input":false,"_uuid":"a0f8f43756a6947faa1d4aed16035ac944c1c397","collapsed":true,"trusted":false},"outputs":[],"source":["# 1)  Remove \"call\" columns from training a test\n","train_keepers = [col for col in X_train.columns if \"call\" not in col]\n","test_keepers = [col for col in X_test.columns if \"call\" not in col]\n","\n","X_train = X_train[train_keepers]\n","X_test = X_test[test_keepers]"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b123fe25-65c4-42b9-8da4-99d93f3b11fc","_uuid":"f444169669f997a125418a3cb900ec421396dd94","collapsed":true,"trusted":false},"outputs":[],"source":["# 2) Transpose\n","X_train = X_train.T\n","X_test = X_test.T\n","X_train"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"15eefd6d-4962-4277-814a-95bfa7a06dc7","_uuid":"d7a9b8fc01f3e0022f55394e87b651a962b99ab5","collapsed":true,"trusted":false},"outputs":[],"source":["# 3) Clean up the column names for training data\n","X_train.columns = X_train.iloc[1]\n","X_train = X_train.drop([\"Gene Description\", \"Gene Accession Number\"]).apply(pd.to_numeric)\n","\n","# Clean up the column names for training data\n","X_test.columns = X_test.iloc[1]\n","X_test = X_test.drop([\"Gene Description\", \"Gene Accession Number\"]).apply(pd.to_numeric)\n","\n","X_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"16fbe1c5-0952-4eb5-81b4-d452d0430c5e","_kg_hide-input":false,"_uuid":"15906fae2bdd07f787d2b48a175ccc0859be1ded","collapsed":true,"trusted":false},"outputs":[],"source":["# 4) Split into train and test \n","X_train = X_train.reset_index(drop=True)\n","y_train = y[y.patient <= 38].reset_index(drop=True)\n","\n","# Subet the rest for testing\n","X_test = X_test.reset_index(drop=True)\n","y_test = y[y.patient > 38].reset_index(drop=True)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Exploratory data analysis\n","\n","Realiza un análisis exploratorio de los datos (correlaciones entre sí y con las clases, distribuciones,...). Usa las técnicas y gráficos que te parezcan más representativos."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b26459eb-e2bb-4a06-bd71-60579d021e2f","_uuid":"dabc60230623be1c1723862cc264316d8d43a943","collapsed":true,"trusted":false},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"_cell_guid":"0f04cd2c-4f37-440b-adee-aa352ce864cb","_uuid":"f134ec8203af75850e46201261b733587554e8d9"},"source":["## (2) Principle Component Analysis\n","\n","The analysis reveals that 21 principle components are needed to account for 80% of the variance. PC 1-3 add up to  about ~33% and the rest is a slow burn where each component after PC8 contributes between 1-2% of the variance up until PC38 which is essentially zero. 1% is a decent amonut of variance and so the number of important PCs is up for interpretation. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 5) Scale data \n","# (1) YOUR CODE HERE: Use the StandardScaler (separately for train and test sets)\n"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"112a28ba-721e-4480-bd1d-eb0a5f6a7491","_uuid":"d1545b4d8e8f8d11d328198e0cc1bc9680c76be4","collapsed":true,"trusted":false},"outputs":[],"source":["# 6) PCA Analysis and projection\n","components = 21\n","# YOUR CODE HERE: \n","# (2) Use PCA with this number of components on train set, with Y the result of the procedure\n","\n","# (3) Retrieve the explained variance ratio, and compute its accumulative sum\n","# save those values in variables var_exp and cum_var_exp\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(var_exp)\n","print(cum_var_exp)"]},{"cell_type":"markdown","metadata":{},"source":["**Pregunta (1)**: ¿Qué pauta puede observarse en los valores de var_exp? ¿Cuál es la interpretación relativa de esos valores?"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f93ca547-4c67-4cc5-bb32-c3f1b611f661","_uuid":"7dd74290b8cbca1f466799df21259eced15c19b1","collapsed":true,"trusted":false},"outputs":[],"source":["# Plot the explained variance using var_exp and cum_var_exp\n","x = [\"PC%s\" %i for i in range(1,components)]\n","trace1 = go.Bar(\n","    x=x,\n","    y=list(var_exp),\n","    name=\"Explained Variance\")\n","\n","trace2 = go.Scatter(\n","    x=x,\n","    y=cum_var_exp,\n","    name=\"Cumulative Variance\")\n","\n","layout = go.Layout(\n","    title='Explained variance',\n","    xaxis=dict(title='Principle Components', tickmode='linear'))\n","\n","data = [trace1, trace2]\n","fig = go.Figure(data=data, layout=layout)\n","offline.iplot(fig)"]},{"cell_type":"markdown","metadata":{"_cell_guid":"71846cd4-4779-48ea-9713-2c73672d4e13","_uuid":"7b843fcd4ecd2538fc1545573cb49f5ee77ab8b7"},"source":["## (3) Projection of first three components\n","The first three components only explain 33% of the variance but we'll go ahead plot the projection to get a visual of it. "]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0f78182b-9238-4954-86c3-ee2a316c07eb","_uuid":"e2c8e7221b1a52029a71b578c15c7f4d43675610","collapsed":true,"trusted":false},"outputs":[],"source":["# Project first three components\n","Y_train_pca = pca.fit_transform(X_train_scl)\n","\n","traces = []\n","for name in ['ALL', 'AML']:\n","    trace = go.Scatter3d(\n","        x=Y_train_pca[y_train.cancer == name, 0],\n","        y=Y_train_pca[y_train.cancer == name, 1],\n","        z=Y_train_pca[y_train.cancer == name, 2],\n","        mode='markers',\n","        name=name,\n","        marker=go.Marker(size=10, line=go.Line(width=1), opacity=1))\n","\n","    traces.append(trace)\n","\n","layout = go.Layout(\n","    xaxis=dict(title='PC1'),\n","    yaxis=dict(title='PC2'),\n","    title=\"Projection of First Three Principle Components\"\n",")\n","\n","data = traces\n","fig = go.Figure(data=data, layout=layout)\n","offline.iplot(fig)\n"]},{"cell_type":"markdown","metadata":{},"source":["**Pregunta(2)**: Modificando la perspectiva de la figura con el ratón, ¿qué observas en cuanto a la separabilidad de las clases? Adjunta una imagen que apoye tus conclusiones."]},{"cell_type":"markdown","metadata":{},"source":["# Parte 2: Linear Discriminant Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train_scl.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# LDA\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n","sklearn_lda = LDA(n_components=1)\n","\n","# Y_train_pca = pca.fit_transform(X_train_scl)\n","\n","X_lda_sklearn = sklearn_lda.fit_transform(X_train_scl, y['cancer'][:38])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","from matplotlib import pyplot as plt \n","\n","def plot_step_lda():\n","\n","    ax = plt.subplot(111)\n","    for label, marker, color in zip(\n","            range(1, 4), ('^', 's', 'o'), ('blue', 'red', 'green')):\n","\n","        plt.scatter(x=X_lda_sklearn[:, 0].real[y == label],\n","                    y=X_lda_sklearn[:, 1].real[y == label],\n","                    marker=marker,\n","                    color=color,\n","                    alpha=0.5,\n","                    label=label_dict[label]\n","                    )\n","\n","    plt.xlabel('LD1')\n","    plt.ylabel('LD2')\n","\n","    leg = plt.legend(loc='upper right', fancybox=True)\n","    leg.get_frame().set_alpha(0.5)\n","    plt.title('LDA: Iris projection onto the first 2 linear discriminants')\n","\n","    # hide axis ticks\n","    plt.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",\n","                    labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n","\n","    # remove axis spines\n","    ax.spines[\"top\"].set_visible(False)\n","    ax.spines[\"right\"].set_visible(False)\n","    ax.spines[\"bottom\"].set_visible(False)\n","    ax.spines[\"left\"].set_visible(False)\n","\n","    plt.grid()\n","    plt.tight_layout\n","    plt.show()\n","\n","def plot_scikit_lda(X, title):\n","\n","   ax = plt.subplot(111)\n","   for label, marker, color in zip(\n","           range(1, 4), ('^', 's', 'o'), ('blue', 'red', 'green')):\n","\n","       plt.scatter(x=X[:, 0][y == label],\n","                   y=X[:, 1][y == label] * -1,  # flip the figure\n","                   marker=marker,\n","                   color=color,\n","                   alpha=0.5,\n","                   label=label_dict[label])\n","\n","   plt.xlabel('LD1')\n","   plt.ylabel('LD2')\n","\n","   leg = plt.legend(loc='upper right', fancybox=True)\n","   leg.get_frame().set_alpha(0.5)\n","   plt.title(title)\n","\n","   # hide axis ticks\n","   plt.tick_params(axis='both', which='both', bottom='off', top='off',\n","                   labelbottom='on', left='off', right='off', labelleft='on')\n","\n","   # remove axis spines\n","   ax.spines['top'].set_visible(False)\n","   ax.spines['right'].set_visible(False)\n","   ax.spines['bottom'].set_visible(False)\n","   ax.spines['left'].set_visible(False)\n","\n","   plt.grid()\n","   plt.tight_layout\n","   plt.show()\n","\n","\n","plot_step_lda()\n","plot_scikit_lda(X_lda_sklearn, title='Default LDA via scikit-learn')\n"]}],"metadata":{"interpreter":{"hash":"53abb0a204625f87b7c69b83c094afa33ea1fd7b02fd512194445a1d520985dc"},"kernelspec":{"display_name":"Python 3.7.10 64-bit ('base': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":1}
