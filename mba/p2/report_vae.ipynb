{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20d9a985",
   "metadata": {},
   "source": [
    "<div style=\"font-weight: bold; color:#5D8AA8\" align=\"center\">\n",
    "    <div style=\"font-size: xx-large\">Applied Bayesian Methods</div><br>\n",
    "    <div style=\"font-size: x-large; color:gray\">Auto-encoding Variational Bayes</div><br>\n",
    "    <div style=\"font-size: large\">Mar√≠a Barroso - Gloria del Valle</div><br></div><hr>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7607e91",
   "metadata": {},
   "source": [
    "In this practice we implement a VAE to be trained on the MNIST dataset. The code is found in the file *vae.py*.\n",
    "\n",
    "\n",
    "## Task 1: Complete the missing parts of the code *vae.py*\n",
    "\n",
    "\n",
    "\n",
    "In this task we implement the following functions: \n",
    "\n",
    "- **sample_latent_variables_from_posterior()**\n",
    "\n",
    "Let $q_\\phi(\\pmb{z}|\\pmb{x})$ be a distribution factoring Gaussian with mean $\\mu_j^{\\phi}(\\pmb{x})$ and variance $\\nu_j^\\phi(\\pmb{x})$, this function samples $\\pmb{z}^i$ generated from $q_\\phi(\\pmb{z}|\\pmb{x}_i)$:\n",
    "\n",
    "\n",
    "$$z_j^i = \\mu_j^{\\phi}(\\pmb{x}_i) + \\sqrt{\\nu_j^\\phi(\\pmb{x}_i)\\epsilon_i^j} \\quad \\mbox{where} \\quad \\epsilon_i^j\\sim \\mathcal{N}(0,1)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f0424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_latent_variables_from_posterior(encoder_output):\n",
    "\n",
    "    # Params of a diagonal Gaussian.\n",
    "\n",
    "    D = np.shape(encoder_output)[-1] // 2\n",
    "    mean, log_std = encoder_output[:, :D], encoder_output[:, D:]\n",
    "\n",
    "    # TODO use the reparametrization trick to generate one sample from q(z|x) per each batch datapoint\n",
    "    # use npr.randn for that.\n",
    "    # The output of this function is a matrix of size the batch x the number of latent dimensions\n",
    "\n",
    "    Z = mean + np.exp(log_std)*npr.randn(*mean.shape)\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc9e933",
   "metadata": {},
   "source": [
    "- **bernoulli_log_prob()**\n",
    "\n",
    "This function implements the log probability of the targets $\\pmb{x}$ given the generator output $f_j^\\theta(\\pmb{z})$ specified in logits:\n",
    "$$\\log p_\\theta(\\pmb{x}|\\pmb{z}) = \\sum_{j=1}^D \\log (x_i\\sigma(f_j^\\theta(\\pmb{z})) + (1-x_j)(1-\\sigma(f_j^\\theta(\\pmb{z})))) $$\n",
    "\n",
    "where $\\sigma(\\cdot)$ is the sigmoid activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078c19b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli_log_prob(targets, logits):\n",
    "\n",
    "    # logits are in R\n",
    "    # Targets must be between 0 and 1\n",
    "\n",
    "    # TODO compute the log probability of the targets given the generator output specified in logits\n",
    "    # sum the probabilities across the dimensions of each image in the batch. The output of this function \n",
    "    # should be a vector of size the batch size\n",
    "\n",
    "    probs = sigmoid(logits)\n",
    "\n",
    "    log_prob = np.sum(\n",
    "                    np.log(\n",
    "                        targets*probs + (1-targets)*(1-probs)\n",
    "                    ),\n",
    "                    axis = -1\n",
    "                )\n",
    "\n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874dd7d2",
   "metadata": {},
   "source": [
    "- **compute_KL()**\n",
    "\n",
    "This functions compute the Kullback-Leibler divergence between the posterior approximation $q_\\phi(\\pmb{z}|\\pmb{x}_i)$ and the prior $p(\\pmb{z})$:\n",
    "\n",
    "$$\\mbox{KL}(q_\\phi(\\pmb{z}|\\pmb{x}_i) | p(\\pmb{z}))= \\sum_{j=1}^L \\frac{1}{2} (\\nu_j^\\phi(\\pmb{x}_i) + \\mu_j^\\phi(\\pmb{x}_i)^2 -1 - \\log\\nu_j^\\phi(\\pmb{x}_i))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28424bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_KL(q_means_and_log_stds):\n",
    "    \n",
    "    D = np.shape(q_means_and_log_stds)[-1] // 2\n",
    "    mean, log_std = q_means_and_log_stds[:, :D], q_means_and_log_stds[:, D:]\n",
    "\n",
    "    # TODO compute the KL divergence between q(z|x) and the prior (use a standard Gaussian for the prior)\n",
    "    # Use the fact that the KL divervence is the sum of KL divergence of the marginals if q and p factorize\n",
    "    # The output of this function should be a vector of size the batch size\n",
    "\n",
    "    KL_divergence = np.sum(\n",
    "                        0.5*(np.exp(2*log_std) + mean**2 - 1 - 2*log_std), \n",
    "                        axis = -1\n",
    "                    )\n",
    "\n",
    "    return KL_divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a62b40",
   "metadata": {},
   "source": [
    "- **vae_lower_bound()**\n",
    "\n",
    "This function computes a noisy estimate of the lower bound of the objetive $$O(\\phi,\\theta)=\\sum_{i=1}^N \\mathcal{E}_{q_\\phi(\\pmb{z}|\\pmb{x}_i)} [\\log p_\\theta(\\pmb{x}|\\pmb{z})] - \\mbox{KL}(q_\\phi(\\pmb{z}|\\pmb{x}_i) |\\pmb{x}_i)$$ by using a single Monte Carlo sample with mini-batchs of data points $\\mathcal{B}$:\n",
    "\n",
    "$$\\hat{O}(\\phi, \\theta) = \\frac{N}{|\\mathcal{B}|}\\sum_{i\\in\\mathcal{B}} \\mathcal{E}_{q_\\phi(\\pmb{z}|\\pmb{x}_i)} [\\log p_\\theta(\\pmb{x}|\\pmb{z})] - \\mbox{KL}(q_\\phi(\\pmb{z}|\\pmb{x}_i) |\\pmb{x}_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a961b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_lower_bound(gen_params, rec_params, data):\n",
    "\n",
    "    # TODO compute a noisy estiamte of the lower bound by using a single Monte Carlo sample:\n",
    "\n",
    "    # 1 - compute the encoder output using neural_net_predict given the data and rec_params\n",
    "    encoder_output = neural_net_predict(rec_params, data)\n",
    "\n",
    "    # 2 - sample the latent variables associated to the batch in data \n",
    "    #     (use sample_latent_variables_from_posterior and the encoder output)\n",
    "    latent_variables_samples = sample_latent_variables_from_posterior(encoder_output)\n",
    "\n",
    "    # 3 - use the sampled latent variables to reconstruct the image and to compute the log_prob of the actual data\n",
    "    #     (use neural_net_predict for that)\n",
    "    decoder_output = neural_net_predict(gen_params, latent_variables_samples)\n",
    "    log_prob = bernoulli_log_prob(data, decoder_output)\n",
    "\n",
    "    # 4 - compute the KL divergence between q(z|x) and the prior (use compute_KL for that)\n",
    "    KL_divergence = compute_KL(encoder_output)\n",
    "\n",
    "    # 5 - return an average estimate (per batch point) of the lower bound by substracting the KL to the data dependent term\n",
    "    estimated_lower_bound = np.mean(\n",
    "                                log_prob - KL_divergence,\n",
    "                                axis = -1\n",
    "                            )\n",
    "\n",
    "    return estimated_lower_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96c7ee9",
   "metadata": {},
   "source": [
    "## Task 2: Complete ADAM algorithm\n",
    "\n",
    "In this task, we complete first the initialization of the ADAM parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8dee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO write here the initial values for the ADAM parameters (including the m and v vectors)\n",
    "# you can use np.zeros_like(flattened_current_params) to initialize m and v\n",
    "\n",
    "alpha = 0.001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "epsilon = 10**-8\n",
    "m = np.zeros_like(flattened_current_params)\n",
    "v = np.zeros_like(flattened_current_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945a5cc4",
   "metadata": {},
   "source": [
    "Second, we write the ADAM updates in the main training loop of the code provided in vae.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7b1f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Use the estimated noisy gradient in grad to update the paramters using the ADAM updates\n",
    "\n",
    "m = beta1*m + (1-beta1)*grad\n",
    "v = beta2*v + (1-beta2)*grad**2\n",
    "m_unbiased = m/(1-beta1**t)\n",
    "v_unbiased = v/(1-beta2**t)\n",
    "\n",
    "flattened_current_params += alpha*m_unbiased/(np.sqrt(v_unbiased)+epsilon)\n",
    "elbo_est += objective(flattened_current_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9f6da7",
   "metadata": {},
   "source": [
    "## Task 3: \n",
    "\n",
    "### Subtask 3.1:\n",
    "\n",
    "We generate 25 images from the generative model, drawing $\\pmb{z}$ from the prior, to then generate $\\pmb{x}$ using the conditional distribution $p_\\theta(\\pmb{x}|\\pmb{z})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d698bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Generate 25 images from prior (use neural_net_predict) and save them using save_images\n",
    "\n",
    "z_prior_samples = npr.randn(25, latent_dim)\n",
    "x_samples = neural_net_predict(gen_params, z_prior_samples)\n",
    "save_images(sigmoid(x_samples), \"task_3_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a02148c",
   "metadata": {},
   "source": [
    "The images obtained are shown below:\n",
    "<img src=\"task_3_1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6c44a7",
   "metadata": {},
   "source": [
    "### Subtask 3.2:\n",
    "\n",
    "We generate 10 image reconstructions using the recognition model and then the generative model. First, we choose the first 10 images from the test set and then, the reconstructions are obtained by generating $\\pmb{z}$ using $q_\\phi(\\pmb{z}|\\pmb{x}_i)$. Finally, we generates $\\pmb{x}$ again using $p_\\theta(\\pmb{x}|\\pmb{z})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bb51f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Generate image reconstructions for the first 10 test images (use neural_net_predict for each model) \n",
    "# and save them alongside with the original image using save_images\n",
    "\n",
    "test_first_10_images = test_images[:10]\n",
    "encoder_output = neural_net_predict(rec_params, test_first_10_images)\n",
    "latent_variables_samples = sample_latent_variables_from_posterior(encoder_output)\n",
    "decoder_output = neural_net_predict(gen_params, latent_variables_samples)\n",
    "\n",
    "reconstruction_images = np.append(\n",
    "                    test_first_10_images, \n",
    "                    sigmoid(decoder_output), \n",
    "                    axis = 0\n",
    "                )\n",
    "\n",
    "save_images(reconstruction_images, \"task_3_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62725b2",
   "metadata": {},
   "source": [
    "The images obtained are shown below:\n",
    "<img src=\"task_3_2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df63fd69",
   "metadata": {},
   "source": [
    "### Subtask 3.3:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bbe8d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
