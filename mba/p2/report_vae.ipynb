{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea104c94",
   "metadata": {},
   "source": [
    "<div style=\"font-weight: bold; color:#5D8AA8\" align=\"center\">\n",
    "    <div style=\"font-size: xx-large\">Applied Bayesian Methods</div><br>\n",
    "    <div style=\"font-size: x-large; color:gray\">Auto-encoding Variational Bayes</div><br>\n",
    "    <div style=\"font-size: large\">Mar√≠a Barroso - Gloria del Valle</div><br></div><hr>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dca8515",
   "metadata": {},
   "source": [
    "In this practice we implement a VAE to be trained on the MNIST dataset. The code is found in the file *vae.py*.\n",
    "\n",
    "\n",
    "## Task 1: Complete the missing parts of the code *vae.py*\n",
    "\n",
    "\n",
    "\n",
    "In this task we implement the following functions: \n",
    "\n",
    "- **sample_latent_variables_from_posterior()**\n",
    "\n",
    "Let $q_\\phi(\\pmb{z}|\\pmb{x})$ be a distribution factoring Gaussian with mean $\\mu_j^{\\phi}(\\pmb{x})$ and variance $\\nu_j^\\phi(\\pmb{x})$, this function samples $\\pmb{z}^i$ generated from $q_\\phi(\\pmb{z}|\\pmb{x}_i)$:\n",
    "\n",
    "\n",
    "$$z_j^i = \\mu_j^{\\phi}(\\pmb{x}_i) + \\sqrt{\\nu_j^\\phi(\\pmb{x}_i)\\epsilon_i^j} \\quad \\mbox{where} \\quad \\epsilon_i^j\\sim \\mathcal{N}(0,1)$$\n",
    "\n",
    "```\n",
    "def sample_latent_variables_from_posterior(encoder_output):\n",
    "\n",
    "    # Params of a diagonal Gaussian.\n",
    "\n",
    "    D = np.shape(encoder_output)[-1] // 2\n",
    "    mean, log_std = encoder_output[:, :D], encoder_output[:, D:]\n",
    "\n",
    "    Z = mean + np.exp(log_std)*npr.randn(*mean.shape)\n",
    "\n",
    "    return Z\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9472f1b3",
   "metadata": {},
   "source": [
    "- **bernoulli_log_prob()**\n",
    "\n",
    "This function implements the log probability of the targets $\\pmb{x}$ given the generator output $f_j^\\theta(\\pmb{z})$ specified in logits:\n",
    "$$\\log p_\\theta(\\pmb{x}|\\pmb{z}) = \\sum_{j=1}^D \\log (x_i\\sigma(f_j^\\theta(\\pmb{z})) + (1-x_j)(1-\\sigma(f_j^\\theta(\\pmb{z})))) $$\n",
    "\n",
    "where $\\sigma(\\cdot)$ is the sigmoid activation function.\n",
    "\n",
    "```\n",
    "def bernoulli_log_prob(targets, logits):\n",
    "\n",
    "    # logits are in R\n",
    "    # Targets must be between 0 and 1\n",
    "\n",
    "    probs = sigmoid(logits)\n",
    "\n",
    "    log_prob = np.sum(\n",
    "                    np.log(\n",
    "                        targets*probs + (1-targets)*(1-probs)\n",
    "                    ),\n",
    "                    axis = -1\n",
    "                )\n",
    "\n",
    "    return log_prob\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f71e67d",
   "metadata": {},
   "source": [
    "- **compute_KL()**\n",
    "\n",
    "This functions compute the Kullback-Leibler divergence between the posterior approximation $q_\\phi(\\pmb{z}|\\pmb{x}_i)$ and the prior $p(\\pmb{z})$:\n",
    "\n",
    "$$\\mbox{KL}(q_\\phi(\\pmb{z}|\\pmb{x}_i) | p(\\pmb{z}))= \\sum_{j=1}^L \\frac{1}{2} (\\nu_j^\\phi(\\pmb{x}_i) + \\mu_j^\\phi(\\pmb{x}_i)^2 -1 - \\log\\nu_j^\\phi(\\pmb{x}_i))$$\n",
    "\n",
    "```\n",
    "def compute_KL(q_means_and_log_stds):\n",
    "    \n",
    "    D = np.shape(q_means_and_log_stds)[-1] // 2\n",
    "    mean, log_std = q_means_and_log_stds[:, :D], q_means_and_log_stds[:, D:]\n",
    "\n",
    "\n",
    "    KL_divergence = np.sum(\n",
    "                        0.5*(np.exp(2*log_std) + mean**2 - 1 - 2*log_std), \n",
    "                        axis = -1\n",
    "                    )\n",
    "\n",
    "    return KL_divergence\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122db642",
   "metadata": {},
   "source": [
    "- **vae_lower_bound()**\n",
    "\n",
    "This function computes a noisy estimate of the lower bound of the objetive $$O(\\phi,\\theta)=\\sum_{i=1}^N \\mathcal{E}_{q_\\phi(\\pmb{z}|\\pmb{x}_i)} [\\log p_\\theta(\\pmb{x}|\\pmb{z})] - \\mbox{KL}(q_\\phi(\\pmb{z}|\\pmb{x}_i) |\\pmb{x}_i)$$ by using a single Monte Carlo sample with mini-batchs of data points $\\mathcal{B}$:\n",
    "\n",
    "$$\\hat{O}(\\phi, \\theta) = \\frac{N}{|\\mathcal{B}|}\\sum_{i\\in\\mathcal{B}} \\mathcal{E}_{q_\\phi(\\pmb{z}|\\pmb{x}_i)} [\\log p_\\theta(\\pmb{x}|\\pmb{z})] - \\mbox{KL}(q_\\phi(\\pmb{z}|\\pmb{x}_i) |\\pmb{x}_i)$$\n",
    "\n",
    "```\n",
    "def vae_lower_bound(gen_params, rec_params, data):\n",
    "\n",
    "    # 1 - compute the encoder output using neural_net_predict given the data and rec_params\n",
    "    encoder_output = neural_net_predict(rec_params, data)\n",
    "\n",
    "    # 2 - sample the latent variables associated to the batch in data \n",
    "    #     (use sample_latent_variables_from_posterior and the encoder output)\n",
    "    latent_variables_samples = sample_latent_variables_from_posterior(encoder_output)\n",
    "\n",
    "    # 3 - use the sampled latent variables to reconstruct the image and to compute the log_prob of the actual data\n",
    "    #     (use neural_net_predict for that)\n",
    "    decoder_output = neural_net_predict(gen_params, latent_variables_samples)\n",
    "    log_prob = bernoulli_log_prob(data, decoder_output)\n",
    "\n",
    "    # 4 - compute the KL divergence between q(z|x) and the prior (use compute_KL for that)\n",
    "    KL_divergence = compute_KL(encoder_output)\n",
    "\n",
    "    # 5 - return an average estimate (per batch point) of the lower bound by substracting the KL to the data dependent term\n",
    "    estimated_lower_bound = np.mean(\n",
    "                                log_prob - KL_divergence,\n",
    "                                axis = -1\n",
    "                            )\n",
    "\n",
    "    return estimated_lower_bound\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8710ac84",
   "metadata": {},
   "source": [
    "## Task 2: Complete ADAM algorithm\n",
    "\n",
    "In this task, we complete first the initialization of the ADAM parameters:\n",
    "\n",
    "```\n",
    "alpha = 0.001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "epsilon = 10**-8\n",
    "m = np.zeros_like(flattened_current_params)\n",
    "v = np.zeros_like(flattened_current_params)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9608862f",
   "metadata": {},
   "source": [
    "Second, we write the ADAM updates in the main training loop of the code provided in vae.py:\n",
    "\n",
    "```\n",
    "m = beta1*m + (1-beta1)*grad\n",
    "v = beta2*v + (1-beta2)*grad**2\n",
    "m_unbiased = m/(1-beta1**t)\n",
    "v_unbiased = v/(1-beta2**t)\n",
    "\n",
    "flattened_current_params += alpha*m_unbiased/(np.sqrt(v_unbiased)+epsilon)\n",
    "elbo_est += objective(flattened_current_params)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fdbeb6",
   "metadata": {},
   "source": [
    "## Task 3: \n",
    "\n",
    "### Subtask 3.1:\n",
    "\n",
    "We generate 25 images from the generative model, drawing $\\pmb{z}$ from the prior, to then generate $\\pmb{x}$ using the conditional distribution $p_\\theta(\\pmb{x}|\\pmb{z})$:\n",
    "\n",
    "```\n",
    "z_prior_samples = npr.randn(25, latent_dim)\n",
    "x_samples = neural_net_predict(gen_params, z_prior_samples)\n",
    "save_images(sigmoid(x_samples), \"task_3_1\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9af215",
   "metadata": {},
   "source": [
    "The images obtained are shown below:\n",
    "<img src=\"task_3_1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd31e3a0",
   "metadata": {},
   "source": [
    "### Subtask 3.2:\n",
    "\n",
    "We generate 10 image reconstructions using the recognition model and then the generative model. First, we choose the first 10 images from the test set and then, the reconstructions are obtained by generating $\\pmb{z}$ using $q_\\phi(\\pmb{z}|\\pmb{x}_i)$. Finally, we generates $\\pmb{x}$ again using $p_\\theta(\\pmb{x}|\\pmb{z})$.\n",
    "\n",
    "\n",
    "```\n",
    "test_first_10_images = test_images[:10]\n",
    "encoder_output = neural_net_predict(rec_params, test_first_10_images)\n",
    "latent_variables_samples = sample_latent_variables_from_posterior(encoder_output)\n",
    "decoder_output = neural_net_predict(gen_params, latent_variables_samples)\n",
    "\n",
    "reconstruction_images = np.append(\n",
    "                    test_first_10_images, \n",
    "                    sigmoid(decoder_output), \n",
    "                    axis = 0\n",
    "                )\n",
    "\n",
    "save_images(reconstruction_images, \"task_3_2\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3252f8f2",
   "metadata": {},
   "source": [
    "The images obtained are shown below:\n",
    "<img src=\"task_3_2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e23c7a0",
   "metadata": {},
   "source": [
    "### Subtask 3.3:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88ac940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
