{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "P2CUDAbarrosovalle_tutorial+ejercicios.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H83eQ91UUo4c"
      },
      "source": [
        "# Práctica 2: Programación GPU y Computación Cuántica\n",
        "## Tutorial de CUDA\n",
        "\n",
        "María Barroso Honrubia\n",
        "\n",
        "Gloria del Valle Cano"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbHv-FMrvZI8"
      },
      "source": [
        "## Parte 1: Programación GPU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y44sGKPivjB6"
      },
      "source": [
        "### Introducción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP1qHupkVOHt"
      },
      "source": [
        "Primero comprobamos las características que nos ofrece Google Colab con $\\texttt{lscpu}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPsAm64NUPjz",
        "outputId": "8e8f50e4-4891-4bd4-b7c8-de8680d0e9ea"
      },
      "source": [
        "!lscpu"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture:        x86_64\n",
            "CPU op-mode(s):      32-bit, 64-bit\n",
            "Byte Order:          Little Endian\n",
            "CPU(s):              2\n",
            "On-line CPU(s) list: 0,1\n",
            "Thread(s) per core:  2\n",
            "Core(s) per socket:  1\n",
            "Socket(s):           1\n",
            "NUMA node(s):        1\n",
            "Vendor ID:           GenuineIntel\n",
            "CPU family:          6\n",
            "Model:               63\n",
            "Model name:          Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "Stepping:            0\n",
            "CPU MHz:             2299.998\n",
            "BogoMIPS:            4599.99\n",
            "Hypervisor vendor:   KVM\n",
            "Virtualization type: full\n",
            "L1d cache:           32K\n",
            "L1i cache:           32K\n",
            "L2 cache:            256K\n",
            "L3 cache:            46080K\n",
            "NUMA node0 CPU(s):   0,1\n",
            "Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat md_clear arch_capabilities\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQCR4SR1VSj-"
      },
      "source": [
        "Comprobamos la memoria física disponible y swap del sistema con $\\texttt{free}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jipoTT7VXPv",
        "outputId": "afe1909a-8a85-4d70-976a-e2e084a31a27"
      },
      "source": [
        "!free -kh"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:            12G        572M        9.9G        1.2M        2.2G         11G\n",
            "Swap:            0B          0B          0B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsMVLr4R39eD"
      },
      "source": [
        "Observamos con más profundidad información del sistema como memoria total RAM, memoria usada para la caché o número total de swaps:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3ZEIoaAVe8o",
        "outputId": "c5c01ee6-8882-4906-ec7b-f841c44b0493"
      },
      "source": [
        "!cat /proc/meminfo"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MemTotal:       13302924 kB\n",
            "MemFree:        10361308 kB\n",
            "MemAvailable:   12449604 kB\n",
            "Buffers:          131244 kB\n",
            "Cached:          2080512 kB\n",
            "SwapCached:            0 kB\n",
            "Active:          1059708 kB\n",
            "Inactive:        1599484 kB\n",
            "Active(anon):     400672 kB\n",
            "Inactive(anon):      440 kB\n",
            "Active(file):     659036 kB\n",
            "Inactive(file):  1599044 kB\n",
            "Unevictable:           0 kB\n",
            "Mlocked:               0 kB\n",
            "SwapTotal:             0 kB\n",
            "SwapFree:              0 kB\n",
            "Dirty:               940 kB\n",
            "Writeback:             0 kB\n",
            "AnonPages:        447444 kB\n",
            "Mapped:           239832 kB\n",
            "Shmem:              1188 kB\n",
            "KReclaimable:     143384 kB\n",
            "Slab:             193976 kB\n",
            "SReclaimable:     143384 kB\n",
            "SUnreclaim:        50592 kB\n",
            "KernelStack:        4796 kB\n",
            "PageTables:         6180 kB\n",
            "NFS_Unstable:          0 kB\n",
            "Bounce:                0 kB\n",
            "WritebackTmp:          0 kB\n",
            "CommitLimit:     6651460 kB\n",
            "Committed_AS:    3114684 kB\n",
            "VmallocTotal:   34359738367 kB\n",
            "VmallocUsed:       44772 kB\n",
            "VmallocChunk:          0 kB\n",
            "Percpu:             1568 kB\n",
            "AnonHugePages:         0 kB\n",
            "ShmemHugePages:        0 kB\n",
            "ShmemPmdMapped:        0 kB\n",
            "FileHugePages:         0 kB\n",
            "FilePmdMapped:         0 kB\n",
            "CmaTotal:              0 kB\n",
            "CmaFree:               0 kB\n",
            "HugePages_Total:       0\n",
            "HugePages_Free:        0\n",
            "HugePages_Rsvd:        0\n",
            "HugePages_Surp:        0\n",
            "Hugepagesize:       2048 kB\n",
            "Hugetlb:               0 kB\n",
            "DirectMap4k:      150336 kB\n",
            "DirectMap2M:     5089280 kB\n",
            "DirectMap1G:    10485760 kB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4wJQ_pYXXRZ"
      },
      "source": [
        "Observamos el funcionamiento de diferentes comandos útiles:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkZaPypuXaSs"
      },
      "source": [
        "Listar el contenido:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suXeGTH4Xh5n",
        "outputId": "11da79ab-0703-4cb2-8a13-3f5cd7b49c30"
      },
      "source": [
        "!ls -la"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 16\n",
            "drwxr-xr-x 1 root root 4096 Nov  1 13:35 .\n",
            "drwxr-xr-x 1 root root 4096 Nov  7 13:42 ..\n",
            "drwxr-xr-x 4 root root 4096 Nov  1 13:34 .config\n",
            "drwxr-xr-x 1 root root 4096 Nov  1 13:35 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tqmQjJRXq0N"
      },
      "source": [
        "Vemos el contenido del directorio /usr/local  donde está instalado CUDA:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pSvEcb0X6m2",
        "outputId": "956d5cbc-254c-448c-ded0-f764f95975d9"
      },
      "source": [
        "!ls -la /usr/local"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 84\n",
            "drwxr-xr-x  1 root root 4096 Nov  1 13:57 .\n",
            "drwxr-xr-x  1 root root 4096 Nov  7 13:42 ..\n",
            "drwxr-xr-x  1 root root 4096 Nov  4 13:12 bin\n",
            "lrwxrwxrwx  1 root root   22 Nov  1 13:27 cuda -> /etc/alternatives/cuda\n",
            "drwxr-xr-x 16 root root 4096 Nov  1 13:19 cuda-10.0\n",
            "drwxr-xr-x 15 root root 4096 Nov  1 13:21 cuda-10.1\n",
            "lrwxrwxrwx  1 root root   25 Nov  1 13:27 cuda-11 -> /etc/alternatives/cuda-11\n",
            "drwxr-xr-x 15 root root 4096 Nov  1 13:24 cuda-11.0\n",
            "drwxr-xr-x  1 root root 4096 Nov  1 13:26 cuda-11.1\n",
            "drwxr-xr-x  1 root root 4096 Nov  1 13:36 etc\n",
            "drwxr-xr-x  2 root root 4096 Nov 19  2020 games\n",
            "drwxr-xr-x  2 root root 4096 Nov  1 13:47 _gcs_config_ops.so\n",
            "drwxr-xr-x  1 root root 4096 Nov  1 13:57 include\n",
            "drwxr-xr-x  1 root root 4096 Nov  1 13:58 lib\n",
            "-rw-r--r--  1 root root 1636 Nov  1 13:52 LICENSE.txt\n",
            "drwxr-xr-x  3 root root 4096 Nov  1 13:47 licensing\n",
            "lrwxrwxrwx  1 root root    9 Nov 19  2020 man -> share/man\n",
            "drwxr-xr-x  2 root root 4096 Nov 19  2020 sbin\n",
            "-rw-r--r--  1 root root 7291 Nov  1 13:52 setup.cfg\n",
            "drwxr-xr-x  1 root root 4096 Nov  1 13:45 share\n",
            "drwxr-xr-x  2 root root 4096 Nov 19  2020 src\n",
            "drwxr-xr-x  2 root root 4096 Nov  1 13:59 xgboost\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsU2aQ3NZks5",
        "outputId": "fae9b10c-73e4-4f95-f22f-98f1825fbdb2"
      },
      "source": [
        "!ls -la /usr/local/cuda-11.1/bin"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 80220\n",
            "drwxr-xr-x 1 root root     4096 Nov  1 13:26 .\n",
            "drwxr-xr-x 1 root root     4096 Nov  1 13:26 ..\n",
            "-rwxr-xr-x 1 root root    80608 Oct 13  2020 bin2c\n",
            "lrwxrwxrwx 1 root root        4 Oct 13  2020 computeprof -> nvvp\n",
            "-rwxr-xr-x 1 root root       97 Oct 13  2020 compute-sanitizer\n",
            "drwxr-xr-x 2 root root     4096 Dec 14  2020 crt\n",
            "-rwxr-xr-x 1 root root  5190640 Oct 13  2020 cudafe++\n",
            "-rwxr-xr-x 1 root root 12035840 Oct 13  2020 cuda-gdb\n",
            "-rwxr-xr-x 1 root root   753776 Oct 13  2020 cuda-gdbserver\n",
            "-rwxr-xr-x 1 root root      800 Oct 13  2020 cuda-install-samples-11.1.sh\n",
            "-rwxr-xr-x 1 root root   353752 Oct 13  2020 cuda-memcheck\n",
            "-rwxr-xr-x 1 root root   232752 Sep 16  2020 cuobjdump\n",
            "-rwxr-xr-x 1 root root   269392 Oct 13  2020 fatbinary\n",
            "-rwxr-xr-x 1 root root     2974 Oct 16  2020 ncu\n",
            "-rwxr-xr-x 1 root root     2577 Oct 16  2020 ncu-ui\n",
            "-rwxr-xr-x 1 root root     1580 Oct 13  2020 nsight_ee_plugins_manage.sh\n",
            "-rwxr-xr-x 1 root root      745 Oct 16  2020 nsight-sys\n",
            "-rwxr-xr-x 1 root root      746 Oct 16  2020 nsys\n",
            "-rwxr-xr-x 1 root root      104 Oct 16  2020 nsys-exporter\n",
            "-rwxr-xr-x 1 root root       85 Oct 16  2020 nsys-ui\n",
            "-rwxr-xr-x 1 root root  5120848 Oct 13  2020 nvcc\n",
            "-rw-r--r-- 1 root root      417 Oct 13  2020 nvcc.profile\n",
            "-rwxr-xr-x 1 root root 33704440 Sep 16  2020 nvdisasm\n",
            "-rwxr-xr-x 1 root root  9391256 Oct 13  2020 nvlink\n",
            "lrwxrwxrwx 1 root root        6 Oct 16  2020 nv-nsight-cu -> ncu-ui\n",
            "lrwxrwxrwx 1 root root        3 Oct 16  2020 nv-nsight-cu-cli -> ncu\n",
            "-rwxr-xr-x 1 root root  5592904 Oct 13  2020 nvprof\n",
            "-rwxr-xr-x 1 root root   101296 Sep 16  2020 nvprune\n",
            "-rwxr-xr-x 1 root root      285 Oct 13  2020 nvvp\n",
            "-rwxr-xr-x 1 root root  9234464 Oct 13  2020 ptxas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvgsLG0saJ9l",
        "outputId": "7ec87474-a8a9-40c0-9179-0da7e8d1145b"
      },
      "source": [
        "!ls -la /usr/local/cuda-11.1/samples\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 108\n",
            "drwxr-xr-x 11 root root  4096 Nov  1 13:26 .\n",
            "drwxr-xr-x  1 root root  4096 Nov  1 13:26 ..\n",
            "drwxr-xr-x 63 root root  4096 Nov  1 13:26 0_Simple\n",
            "drwxr-xr-x  8 root root  4096 Nov  1 13:26 1_Utilities\n",
            "drwxr-xr-x 14 root root  4096 Nov  1 13:26 2_Graphics\n",
            "drwxr-xr-x 24 root root  4096 Nov  1 13:26 3_Imaging\n",
            "drwxr-xr-x 10 root root  4096 Nov  1 13:26 4_Finance\n",
            "drwxr-xr-x 10 root root  4096 Nov  1 13:26 5_Simulations\n",
            "drwxr-xr-x 36 root root  4096 Nov  1 13:26 6_Advanced\n",
            "drwxr-xr-x 37 root root  4096 Nov  1 13:26 7_CUDALibraries\n",
            "drwxr-xr-x  6 root root  4096 Nov  1 13:26 common\n",
            "-rw-r--r--  1 root root 60537 Oct 13  2020 EULA.txt\n",
            "-rw-r--r--  1 root root  2606 Oct 13  2020 Makefile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv4vB-OuYP_k"
      },
      "source": [
        "Observamos la versión de CUDA instalada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOWMprbqYylZ",
        "outputId": "d321d307-7549-4ec0-b194-99298d8167dc"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-IeY9zi7uFc"
      },
      "source": [
        "Con la interfaz de configuración de NVIDIA observamos el estado de la GPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIE_XcCoZF0J",
        "outputId": "98e5e6fc-0d0a-4622-fc3b-590d8f533a0b"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Nov  7 13:51:58 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqSdqOYDZR9f"
      },
      "source": [
        "Listamos los ejemplos proporcionados para el tutorial:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8amHLuTAZUoX",
        "outputId": "b3884267-fd74-47ac-8d42-4cd81fc81cc9"
      },
      "source": [
        "!ls -la /usr/local/cuda/samples/"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 108\n",
            "drwxr-xr-x 11 root root  4096 Nov  1 13:26 .\n",
            "drwxr-xr-x  1 root root  4096 Nov  1 13:26 ..\n",
            "drwxr-xr-x 63 root root  4096 Nov  1 13:26 0_Simple\n",
            "drwxr-xr-x  8 root root  4096 Nov  1 13:26 1_Utilities\n",
            "drwxr-xr-x 14 root root  4096 Nov  1 13:26 2_Graphics\n",
            "drwxr-xr-x 24 root root  4096 Nov  1 13:26 3_Imaging\n",
            "drwxr-xr-x 10 root root  4096 Nov  1 13:26 4_Finance\n",
            "drwxr-xr-x 10 root root  4096 Nov  1 13:26 5_Simulations\n",
            "drwxr-xr-x 36 root root  4096 Nov  1 13:26 6_Advanced\n",
            "drwxr-xr-x 37 root root  4096 Nov  1 13:26 7_CUDALibraries\n",
            "drwxr-xr-x  6 root root  4096 Nov  1 13:26 common\n",
            "-rw-r--r--  1 root root 60537 Oct 13  2020 EULA.txt\n",
            "-rw-r--r--  1 root root  2606 Oct 13  2020 Makefile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iwlJSZqbxEw"
      },
      "source": [
        "Compilación de uno de los ejemplos proporcionados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECcKq7X4bwun",
        "outputId": "b916e3e3-f635-406e-c272-76866a5b76b4"
      },
      "source": [
        "%cd /usr/local/cuda/samples/1_Utilities/deviceQuery/\n",
        "%ls"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/cuda-11.1/samples/1_Utilities/deviceQuery\n",
            "deviceQuery.cpp  Makefile  NsightEclipse.xml  readme.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZpOrjkgb8Mf",
        "outputId": "5e6acd05-8ed7-4a7e-bdd2-47f9e1201621"
      },
      "source": [
        "!make"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/cuda-11.1/bin/nvcc -ccbin g++ -I../../common/inc  -m64    -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_86,code=compute_86 -o deviceQuery.o -c deviceQuery.cpp\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "/usr/local/cuda-11.1/bin/nvcc -ccbin g++   -m64      -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_86,code=compute_86 -o deviceQuery deviceQuery.o \n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "mkdir -p ../../bin/x86_64/linux/release\n",
            "cp deviceQuery ../../bin/x86_64/linux/release\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w79U1y11cDst",
        "outputId": "9aa23cc1-108c-486f-90d2-618078e02ab0"
      },
      "source": [
        "!ls -la"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 776\n",
            "drwxr-xr-x 1 root root   4096 Nov  7 13:52 .\n",
            "drwxr-xr-x 1 root root   4096 Nov  1 13:26 ..\n",
            "-rwxr-xr-x 1 root root 722704 Nov  7 13:52 deviceQuery\n",
            "-rw-r--r-- 1 root root  12720 Oct 13  2020 deviceQuery.cpp\n",
            "-rw-r--r-- 1 root root  16752 Nov  7 13:52 deviceQuery.o\n",
            "-rw-r--r-- 1 root root  12165 Oct 13  2020 Makefile\n",
            "-rw-r--r-- 1 root root   1815 Oct 13  2020 NsightEclipse.xml\n",
            "-rw-r--r-- 1 root root    168 Oct 13  2020 readme.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUg5wJQIcPUM"
      },
      "source": [
        "Corremos el ejecutable que nos ofrece información relevante acerca de CUDA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGu6lqTicOs0",
        "outputId": "a74f74d2-b74a-48d6-c41d-5b861c2f3275"
      },
      "source": [
        "!./deviceQuery"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./deviceQuery Starting...\n",
            "\n",
            " CUDA Device Query (Runtime API) version (CUDART static linking)\n",
            "\n",
            "Detected 1 CUDA Capable device(s)\n",
            "\n",
            "Device 0: \"Tesla K80\"\n",
            "  CUDA Driver Version / Runtime Version          11.2 / 11.1\n",
            "  CUDA Capability Major/Minor version number:    3.7\n",
            "  Total amount of global memory:                 11441 MBytes (11996954624 bytes)\n",
            "  (13) Multiprocessors, (192) CUDA Cores/MP:     2496 CUDA Cores\n",
            "  GPU Max Clock rate:                            824 MHz (0.82 GHz)\n",
            "  Memory Clock rate:                             2505 Mhz\n",
            "  Memory Bus Width:                              384-bit\n",
            "  L2 Cache Size:                                 1572864 bytes\n",
            "  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\n",
            "  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\n",
            "  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\n",
            "  Total amount of constant memory:               65536 bytes\n",
            "  Total amount of shared memory per block:       49152 bytes\n",
            "  Total shared memory per multiprocessor:        114688 bytes\n",
            "  Total number of registers available per block: 65536\n",
            "  Warp size:                                     32\n",
            "  Maximum number of threads per multiprocessor:  2048\n",
            "  Maximum number of threads per block:           1024\n",
            "  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n",
            "  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n",
            "  Maximum memory pitch:                          2147483647 bytes\n",
            "  Texture alignment:                             512 bytes\n",
            "  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\n",
            "  Run time limit on kernels:                     No\n",
            "  Integrated GPU sharing Host Memory:            No\n",
            "  Support host page-locked memory mapping:       Yes\n",
            "  Alignment requirement for Surfaces:            Yes\n",
            "  Device has ECC support:                        Enabled\n",
            "  Device supports Unified Addressing (UVA):      Yes\n",
            "  Device supports Managed Memory:                Yes\n",
            "  Device supports Compute Preemption:            No\n",
            "  Supports Cooperative Kernel Launch:            No\n",
            "  Supports MultiDevice Co-op Kernel Launch:      No\n",
            "  Device PCI Domain ID / Bus ID / location ID:   0 / 0 / 4\n",
            "  Compute Mode:\n",
            "     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n",
            "\n",
            "deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.2, CUDA Runtime Version = 11.1, NumDevs = 1\n",
            "Result = PASS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXva9zmVvp7B"
      },
      "source": [
        "Aprovechamos los resultados para responder a las preguntas que se nos ofrecen más abajo:\n",
        "* **Pruebe a lanzar diferente número de threads (con un solo 1 bloque)\n",
        "¿Cuáles son los valores máximos y mínimos de número de threads por bloque en esta GPU?**\n",
        "\n",
        "    Además de verlo en las posteriores ejecuciones, podemos ver ya que por definición el mínimo es 1 thread y como máximo se pueden tener 1024 threads por cada bloque.\n",
        "\n",
        "* **Pruebe a lanzar diferente número de bloques (con un solo thread) ¿Cuáles son los valores máximos y mínimos de número de bloques en esta GPU?**\n",
        "\n",
        "    En este caso el máximo número de bloques es 65535, siendo 1 el mínimo. Si se desea ver más información ver [especificaciones técnicas de CUDA](http://blog.cuvilib.com/2010/06/09/nvidia-cuda-difference-between-fermi-and-previous-architectures/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzeXoGFjdFpb"
      },
      "source": [
        "Compilar en CUDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tiuxb8ZcMg1",
        "outputId": "67b01811-1ffb-46c7-d484-77bf1f931cec"
      },
      "source": [
        "%cd /content/"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmIeBtHB3kKf",
        "outputId": "b36e98f3-a6a8-476d-c56d-f6ab15a8d8de"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq2Zkdb3dRbg"
      },
      "source": [
        "Hacer un directorio de trabajo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFIrKoxRdVKU"
      },
      "source": [
        "!mkdir workcuda"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJnzSXYDdah0",
        "outputId": "4afa9942-c1a6-4814-e833-6145b308b956"
      },
      "source": [
        "cd workcuda/\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/workcuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fFsAUW9dgVJ",
        "outputId": "876f6147-550a-40c8-8b67-7569d2b02f20"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/workcuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH1Xtcend3Bf"
      },
      "source": [
        "## Ejercicio 1: suma de vectores\n",
        "A continuación se discuten diferentes versiones de un programa que suma los elementos de un vector en CUDA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89b1I4R29vaA"
      },
      "source": [
        "### Ejercicio base en CUDA: 1 bloque/1 thread\n",
        "Nota: este ejercicio aportado no aprovecha los recursos de la GPU, es decir, se ejecuta en GPU pero en serie."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwAyqNfAd95Z",
        "outputId": "31ab8fd8-5113-47d4-d627-fad3908a4f31"
      },
      "source": [
        "%%writefile suma1d.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "__global__ void add(int *a, int *b, int *c) {\n",
        "*c = *a + *b;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "int a, b, c;\n",
        "\n",
        "// host copies of variables a, b & c\n",
        "int *d_a, *d_b, *d_c;\n",
        "// device copies of variables a, b & c\n",
        "int size = sizeof(int);\n",
        "\n",
        "// Allocate space for device copies of a, b, c\n",
        "cudaMalloc((void **)&d_a, size);\n",
        "cudaMalloc((void **)&d_b, size);\n",
        "cudaMalloc((void **)&d_c, size);\n",
        "\n",
        "// Setup input values  \n",
        "c = 0;\n",
        "a = 3;\n",
        "b = 5;\n",
        "\n",
        "// Copy inputs to device\n",
        "cudaMemcpy(d_a, &a, size, cudaMemcpyHostToDevice);\n",
        "cudaMemcpy(d_b, &b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "// Launch add() kernel on GPU\n",
        "add<<<1,1>>>(d_a, d_b, d_c);\n",
        "\n",
        "// Copy result back to host\n",
        "cudaError err = cudaMemcpy(&c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "  if(err!=cudaSuccess) {\n",
        "      printf(\"CUDA error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "  }\n",
        "printf(\"result is %d\\n\",c);\n",
        "\n",
        "// Cleanup\n",
        "cudaFree(d_a);\n",
        "cudaFree(d_b);\n",
        "cudaFree(d_c);\n",
        "\n",
        "return 0;\n",
        "}"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing suma1d.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e5lNkDHfZGl"
      },
      "source": [
        "Compilamos y ejecutamos el ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-Wx8K-ffJOS",
        "outputId": "d1f3ff72-5fcd-4f2d-eb1f-e4ffe6c6fb0f"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true suma1d.cu -o suma1d -lcudadevrt\n",
        "!./suma1d"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "result is 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BjD-FD6fjHM"
      },
      "source": [
        "Comprobamos el perfil de ejecución"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq6V0drHBzsH"
      },
      "source": [
        "Vemos que el resultado es correcto, pero estamos haciendo un mal uso de la GPU al no estar haciendo ninguna división. Por consiguiente esto puede afectar al tiempo de ejecución, que se puede ver fácilmente con el $\\texttt{profiler}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpSV8mdMfomL",
        "outputId": "092a95c0-8647-4d74-9b97-f0cb1a78777c"
      },
      "source": [
        "!nvprof ./suma1d"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==351== NVPROF is profiling process 351, command: ./suma1d\n",
            "==351== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "result is 8\n",
            "==351== Profiling application: ./suma1d\n",
            "==351== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   38.56%  3.9360us         2  1.9680us  1.5680us  2.3680us  [CUDA memcpy HtoD]\n",
            "                   36.36%  3.7120us         1  3.7120us  3.7120us  3.7120us  add(int*, int*, int*)\n",
            "                   25.08%  2.5600us         1  2.5600us  2.5600us  2.5600us  [CUDA memcpy DtoH]\n",
            "      API calls:   99.59%  259.42ms         3  86.473ms  2.7710us  259.41ms  cudaMalloc\n",
            "                    0.21%  550.84us         1  550.84us  550.84us  550.84us  cuDeviceTotalMem\n",
            "                    0.09%  222.48us       101  2.2020us     152ns  78.051us  cuDeviceGetAttribute\n",
            "                    0.06%  150.04us         3  50.014us  6.0570us  128.40us  cudaFree\n",
            "                    0.02%  62.940us         3  20.980us  12.622us  31.024us  cudaMemcpy\n",
            "                    0.02%  40.660us         1  40.660us  40.660us  40.660us  cuDeviceGetName\n",
            "                    0.01%  25.738us         1  25.738us  25.738us  25.738us  cudaLaunchKernel\n",
            "                    0.00%  9.5190us         1  9.5190us  9.5190us  9.5190us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.5360us         3     845ns     229ns  1.2810us  cuDeviceGetCount\n",
            "                    0.00%  1.6360us         2     818ns     389ns  1.2470us  cuDeviceGet\n",
            "                    0.00%     302ns         1     302ns     302ns     302ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FqMcRGNHCqu"
      },
      "source": [
        "### Ejercicio N bloques/1 thread\n",
        "Probamos haciendo paralelismo de 512 bloques de 1 thread."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDULMwPfgxuU",
        "outputId": "6b6fbe00-e885-4773-8de2-2402e9e9ae98"
      },
      "source": [
        "%%writefile suma2dvector.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "__global__ void add(int *a, int *b, int *c) {\n",
        "//*c = *a + *b;\n",
        "c[blockIdx.x] = a[blockIdx.x] + b[blockIdx.x]; \n",
        "}\n",
        "\n",
        "#define N 512\n",
        "\n",
        "int main(void) {\n",
        "int *a, *b, *c;        // host copies of variables a, b & c\n",
        "int *d_a, *d_b, *d_c;  // device copies of variables a, b & c\n",
        "int size = N * sizeof(int);\n",
        "\n",
        "// Allocate space for host copies of a, b, c   Setup input values  \n",
        "a =  (int *) malloc(size); \n",
        "b =  (int *) malloc(size); \n",
        "c =  (int *) malloc(size); \n",
        "\n",
        "\n",
        "// Setup input values  \n",
        "\n",
        "for( int i = 0; i < N; i++ )\n",
        "\t{\n",
        "\t\ta[i] = i;\n",
        "    b[i] = N-i;\n",
        "\t\tc[i] = 0;\n",
        "\t}\n",
        "\n",
        "\n",
        "// Allocate space for device copies of a, b, c\n",
        "cudaMalloc((void **)&d_a, size);\n",
        "cudaMalloc((void **)&d_b, size);\n",
        "cudaMalloc((void **)&d_c, size);\n",
        "\n",
        "\n",
        "// Copy inputs to device\n",
        "cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n",
        "cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "// Launch add() kernel on GPU  Se lanzan N bloques de 1 Thread.\n",
        "add<<<N,1>>>(d_a, d_b, d_c);\n",
        "\n",
        "// Copy result back to host\n",
        "cudaError err = cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "  if(err!=cudaSuccess) {\n",
        "      printf(\"CUDA error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "  }\n",
        "printf(\" valor a[0] es %d\\n\",a[0]);\n",
        "printf(\" valor b[0] es %d\\n\",b[0]);\n",
        "printf(\"resultado c[0] es %d\\n\",c[0]);\n",
        "printf(\" valor a[2] es %d\\n\",a[2]);\n",
        "printf(\" valor b[2] es %d\\n\",b[2]);\n",
        "printf(\"resultado c[2] es %d\\n\",c[2]);\n",
        "printf(\" valor a[15] es %d\\n\",a[15]);\n",
        "printf(\" valor b[15] es %d\\n\",b[15]);\n",
        "printf(\"resultado c[15] es %d\\n\",c[15]);\n",
        "\n",
        "// Cleanup\n",
        "\n",
        "free(a); free(b);free(c); \n",
        "\n",
        "cudaFree(d_a);\n",
        "cudaFree(d_b);\n",
        "cudaFree(d_c);\n",
        "\n",
        "return 0;\n",
        "}"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing suma2dvector.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEcMF9ZODP3r"
      },
      "source": [
        "Compilamos y ejecutamos y vemos que el resultado es correcto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-w1xk4RFDZ9t",
        "outputId": "976210bd-a1ca-4a76-c696-f4bead8276ea"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true suma2dvector.cu -o suma2dvector -lcudadevrt\n",
        "!./suma2dvector"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            " valor a[0] es 0\n",
            " valor b[0] es 512\n",
            "resultado c[0] es 512\n",
            " valor a[2] es 2\n",
            " valor b[2] es 510\n",
            "resultado c[2] es 512\n",
            " valor a[15] es 15\n",
            " valor b[15] es 497\n",
            "resultado c[15] es 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lUTdjJuFTnR"
      },
      "source": [
        "Comprobamos el perfil de ejecución y vemos que tarda más que en el caso anterior. Si bien al haber solo un thread por bloque, quiere decir que solo se ejecuta un warp por bloque y no maximiza el aprovechamiento de la GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPBo6tWUFWyb",
        "outputId": "a75b80ef-5258-4924-d1ef-19aca3c3c2b5"
      },
      "source": [
        "!nvprof ./suma2dvector"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==398== NVPROF is profiling process 398, command: ./suma2dvector\n",
            "==398== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            " valor a[0] es 0\n",
            " valor b[0] es 512\n",
            "resultado c[0] es 512\n",
            " valor a[2] es 2\n",
            " valor b[2] es 510\n",
            "resultado c[2] es 512\n",
            " valor a[15] es 15\n",
            " valor b[15] es 497\n",
            "resultado c[15] es 512\n",
            "==398== Profiling application: ./suma2dvector\n",
            "==398== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   41.19%  5.0880us         1  5.0880us  5.0880us  5.0880us  add(int*, int*, int*)\n",
            "                   37.31%  4.6080us         2  2.3040us  1.9840us  2.6240us  [CUDA memcpy HtoD]\n",
            "                   21.50%  2.6560us         1  2.6560us  2.6560us  2.6560us  [CUDA memcpy DtoH]\n",
            "      API calls:   99.55%  201.47ms         3  67.156ms  2.5850us  201.46ms  cudaMalloc\n",
            "                    0.22%  450.62us         1  450.62us  450.62us  450.62us  cuDeviceTotalMem\n",
            "                    0.08%  163.84us       101  1.6220us     145ns  67.927us  cuDeviceGetAttribute\n",
            "                    0.07%  144.92us         3  48.307us  4.3430us  125.53us  cudaFree\n",
            "                    0.03%  66.646us         3  22.215us  13.612us  30.114us  cudaMemcpy\n",
            "                    0.02%  36.182us         1  36.182us  36.182us  36.182us  cudaLaunchKernel\n",
            "                    0.02%  31.116us         1  31.116us  31.116us  31.116us  cuDeviceGetName\n",
            "                    0.00%  7.8750us         1  7.8750us  7.8750us  7.8750us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.2540us         3     751ns     222ns  1.0640us  cuDeviceGetCount\n",
            "                    0.00%  1.6930us         2     846ns     489ns  1.2040us  cuDeviceGet\n",
            "                    0.00%     298ns         1     298ns     298ns     298ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0FbrAaUHUQI"
      },
      "source": [
        "### Ejercicio N bloques/1 thread\n",
        "Probamos haciendo paralelismo de 1 bloque de 1024 threads.\n",
        "\n",
        "Nota: en el tutorial se determinaba N como 1028, sin embargo esto supera el máximo número de threads permitidos por bloque, por lo que el resultado es incorrecto, con el fin de comparar los resultados con diferentes opciones, determinamos N como 1024 threads para este caso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7ujd3TwHcE0",
        "outputId": "04c73fd2-8958-4eec-db10-449f7da85184"
      },
      "source": [
        "%%writefile suma2dvectorNthreads.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "__global__ void add(int *a, int *b, int *c) {\n",
        "c[threadIdx.x] = a[threadIdx.x] + b[threadIdx.x]; \n",
        "}\n",
        "\n",
        "#define N 1024\n",
        "\n",
        "int main(void) {\n",
        "int *a, *b, *c;        // host copies of variables a, b & c\n",
        "int *d_a, *d_b, *d_c;  // device copies of variables a, b & c\n",
        "int size = N * sizeof(int);\n",
        "\n",
        "// Allocate space for host copies of a, b, c   Setup input values  \n",
        "a =  (int *) malloc(size); \n",
        "b =  (int *) malloc(size); \n",
        "c =  (int *) malloc(size); \n",
        "\n",
        "\n",
        "// Setup input values  \n",
        "\n",
        "for( int i = 0; i < N; i++ )\n",
        "\t{\n",
        "\t\ta[i] = i;\n",
        "    b[i] = N-i;\n",
        "\t\tc[i] = 0;\n",
        "\t}\n",
        "\n",
        "\n",
        "// Allocate space for device copies of a, b, c\n",
        "cudaMalloc((void **)&d_a, size);\n",
        "cudaMalloc((void **)&d_b, size);\n",
        "cudaMalloc((void **)&d_c, size);\n",
        "\n",
        "\n",
        "// Copy inputs to device\n",
        "cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n",
        "cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "// Launch add() kernel on GPU  Se lanzan 1 bloques de N Threads.\n",
        "add<<<1,N>>>(d_a, d_b, d_c);\n",
        "\n",
        "// Copy result back to host\n",
        "cudaError err = cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "  if(err!=cudaSuccess) {\n",
        "      printf(\"CUDA error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "  }\n",
        "printf(\" valor a[10] es %d\\n\",a[10]);\n",
        "printf(\" valor b[10] es %d\\n\",b[10]);\n",
        "printf(\"resultado c[10] es %d\\n\",c[10]);\n",
        "printf(\" valor a[0] es %d\\n\",a[0]);\n",
        "printf(\" valor b[0] es %d\\n\",b[0]);\n",
        "printf(\"resultado c[0] es %d\\n\",c[0]);\n",
        "printf(\" valor a[15] es %d\\n\",a[15]);\n",
        "printf(\" valor b[15] es %d\\n\",b[15]);\n",
        "printf(\"resultado c[15] es %d\\n\",c[15]);\n",
        "\n",
        "// Cleanup\n",
        "\n",
        "free(a); free(b);free(c); \n",
        "\n",
        "cudaFree(d_a);\n",
        "cudaFree(d_b);\n",
        "cudaFree(d_c);\n",
        "\n",
        "return 0;\n",
        "}"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing suma2dvectorNthreads.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E0HiMy6IU1L"
      },
      "source": [
        "Ejecutamos este ejemplo y vemos que nuevamente el resultado es correcto.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW4YFVPkIYHr",
        "outputId": "8206f78d-5bba-408e-9f79-4f0f3dfc9905"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true suma2dvectorNthreads.cu -o suma2dvectorNthreads -lcudadevrt\n",
        "!./suma2dvectorNthreads"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            " valor a[10] es 10\n",
            " valor b[10] es 1014\n",
            "resultado c[10] es 1024\n",
            " valor a[0] es 0\n",
            " valor b[0] es 1024\n",
            "resultado c[0] es 1024\n",
            " valor a[15] es 15\n",
            " valor b[15] es 1009\n",
            "resultado c[15] es 1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2uXYiF_sDAu"
      },
      "source": [
        "En este caso vemos que la suma tarda bastante y tampoco se aprovechan los recursos de la GPU de manera eficiente. Esto es porque estamos realizando 1024 operaciones que acceden a memoria global en un único bloque."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhGdz2EMw5W3",
        "outputId": "ced55768-ddbd-46ff-89a9-69b9fe35ce07"
      },
      "source": [
        "!nvprof ./suma2dvectorNthreads"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==445== NVPROF is profiling process 445, command: ./suma2dvectorNthreads\n",
            "==445== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            " valor a[10] es 10\n",
            " valor b[10] es 1014\n",
            "resultado c[10] es 1024\n",
            " valor a[0] es 0\n",
            " valor b[0] es 1024\n",
            "resultado c[0] es 1024\n",
            " valor a[15] es 15\n",
            " valor b[15] es 1009\n",
            "resultado c[15] es 1024\n",
            "==445== Profiling application: ./suma2dvectorNthreads\n",
            "==445== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   45.48%  5.3120us         2  2.6560us  2.4960us  2.8160us  [CUDA memcpy HtoD]\n",
            "                   29.59%  3.4560us         1  3.4560us  3.4560us  3.4560us  add(int*, int*, int*)\n",
            "                   24.93%  2.9120us         1  2.9120us  2.9120us  2.9120us  [CUDA memcpy DtoH]\n",
            "      API calls:   99.49%  181.44ms         3  60.481ms  2.7960us  181.44ms  cudaMalloc\n",
            "                    0.25%  460.50us         1  460.50us  460.50us  460.50us  cuDeviceTotalMem\n",
            "                    0.10%  174.09us         3  58.031us  5.5570us  153.61us  cudaFree\n",
            "                    0.09%  173.03us       101  1.7130us     145ns  70.368us  cuDeviceGetAttribute\n",
            "                    0.04%  66.011us         3  22.003us  14.488us  27.966us  cudaMemcpy\n",
            "                    0.01%  26.933us         1  26.933us  26.933us  26.933us  cudaLaunchKernel\n",
            "                    0.01%  23.622us         1  23.622us  23.622us  23.622us  cuDeviceGetName\n",
            "                    0.00%  6.9050us         1  6.9050us  6.9050us  6.9050us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.0210us         3     673ns     195ns  1.0740us  cuDeviceGetCount\n",
            "                    0.00%  1.4380us         2     719ns     263ns  1.1750us  cuDeviceGet\n",
            "                    0.00%     294ns         1     294ns     294ns     294ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX_QYtIPxjBb"
      },
      "source": [
        "### Ejercicio N bloques/N threads\n",
        "A continuación se aprovechan los recursos que nos ofrece CUDA combinando bloques y threads para procesar el programa de forma paralela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Spb5jhxKyGz1",
        "outputId": "06895f54-a011-48da-92dd-d4324f0136d4"
      },
      "source": [
        "%%writefile suma2dvectorNBloqxNthreads.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "__global__ void add(int *a, int *b, int *c) {\n",
        "int index = threadIdx.x + blockIdx.x * blockDim.x;\n",
        " c[index] = a[index] + b[index]; \n",
        "}\n",
        "\n",
        "#define N (1024*1024)\n",
        "#define THREADS_PER_BLOCK 512 \n",
        "\n",
        "int main(void) {\n",
        "int *a, *b, *c;        // host copies of variables a, b & c\n",
        "int *d_a, *d_b, *d_c;  // device copies of variables a, b & c\n",
        "int size = N*sizeof(int);\n",
        "\n",
        "// Allocate space for host copies of a, b, c   Setup input values  \n",
        "a =  (int *) malloc(size); \n",
        "b =  (int *) malloc(size); \n",
        "c =  (int *) malloc(size); \n",
        "\n",
        "\n",
        "// Setup input values  \n",
        "\n",
        "for( int i = 0; i < N; i++ )\n",
        "\t{\n",
        "\t\ta[i] = i;\n",
        "    b[i] = N-i;\n",
        "\t\tc[i] = 0;\n",
        "\t}\n",
        "\n",
        "\n",
        "// Allocate space for device copies of a, b, c\n",
        "cudaMalloc((void **)&d_a, size);\n",
        "cudaMalloc((void **)&d_b, size);\n",
        "cudaMalloc((void **)&d_c, size);\n",
        "\n",
        "\n",
        "// Copy inputs to device\n",
        "cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n",
        "cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "// Launch add() kernel on GPU  Se lanzan NuBloq=N/THREADS_PER_BLOCK bloques de \n",
        "// THREADS_PER_BLOCK Threads.\n",
        "add<<<N/THREADS_PER_BLOCK,THREADS_PER_BLOCK>>>(d_a, d_b, d_c);\n",
        "\n",
        "// Copy result back to host\n",
        "cudaError err = cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "  if(err!=cudaSuccess) {\n",
        "      printf(\"CUDA error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "  }\n",
        "printf(\" valor a[10] es %d\\n\",a[10]);\n",
        "printf(\" valor b[10] es %d\\n\",b[10]);\n",
        "printf(\"resultado c[10] es %d\\n\",c[10]);\n",
        "printf(\" valor a[0] es %d\\n\",a[0]);\n",
        "printf(\" valor b[0] es %d\\n\",b[0]);\n",
        "printf(\"resultado c[0] es %d\\n\",c[0]);\n",
        "printf(\" valor a[15] es %d\\n\",a[15]);\n",
        "printf(\" valor b[15] es %d\\n\",b[15]);\n",
        "printf(\"resultado c[15] es %d\\n\",c[15]);\n",
        "\n",
        "// Cleanup\n",
        "\n",
        "free(a); free(b);free(c); \n",
        "\n",
        "cudaFree(d_a);\n",
        "cudaFree(d_b);\n",
        "cudaFree(d_c);\n",
        "\n",
        "return 0;\n",
        "}"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing suma2dvectorNBloqxNthreads.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTeZiHnfxi1E"
      },
      "source": [
        "Comprobamos que el resultado es el mismo y es correcto:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxIvXWKCzBxw",
        "outputId": "a8fb18cb-c7bd-44d0-ead2-34dbb3d60b9c"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true suma2dvectorNBloqxNthreads.cu -o suma2dvectorNBloqxNthreads -lcudadevrt\n",
        "!./suma2dvectorNBloqxNthreads"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            " valor a[10] es 10\n",
            " valor b[10] es 1048566\n",
            "resultado c[10] es 1048576\n",
            " valor a[0] es 0\n",
            " valor b[0] es 1048576\n",
            "resultado c[0] es 1048576\n",
            " valor a[15] es 15\n",
            " valor b[15] es 1048561\n",
            "resultado c[15] es 1048576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXqdKI-X2Tcr"
      },
      "source": [
        "Comprobamos con el perfil de ejecución que se realiza el mejor uso de la GPU hasta ahora."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H08bppmr2UKm",
        "outputId": "0698c7bc-be68-48d0-8e1a-21461d779851"
      },
      "source": [
        "!nvprof ./suma2dvectorNBloqxNthreads"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==494== NVPROF is profiling process 494, command: ./suma2dvectorNBloqxNthreads\n",
            "==494== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            " valor a[10] es 10\n",
            " valor b[10] es 1048566\n",
            "resultado c[10] es 1048576\n",
            " valor a[0] es 0\n",
            " valor b[0] es 1048576\n",
            "resultado c[0] es 1048576\n",
            " valor a[15] es 15\n",
            " valor b[15] es 1048561\n",
            "resultado c[15] es 1048576\n",
            "==494== Profiling application: ./suma2dvectorNBloqxNthreads\n",
            "==494== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   62.56%  1.0927ms         2  546.35us  543.90us  548.80us  [CUDA memcpy HtoD]\n",
            "                   31.37%  547.97us         1  547.97us  547.97us  547.97us  [CUDA memcpy DtoH]\n",
            "                    6.06%  105.86us         1  105.86us  105.86us  105.86us  add(int*, int*, int*)\n",
            "      API calls:   97.80%  194.36ms         3  64.786ms  116.97us  194.12ms  cudaMalloc\n",
            "                    1.11%  2.2100ms         3  736.68us  612.10us  915.57us  cudaMemcpy\n",
            "                    0.72%  1.4310ms         3  477.00us  249.43us  592.37us  cudaFree\n",
            "                    0.25%  501.16us         1  501.16us  501.16us  501.16us  cuDeviceTotalMem\n",
            "                    0.09%  174.91us       101  1.7310us     143ns  69.759us  cuDeviceGetAttribute\n",
            "                    0.01%  28.014us         1  28.014us  28.014us  28.014us  cudaLaunchKernel\n",
            "                    0.01%  25.142us         1  25.142us  25.142us  25.142us  cuDeviceGetName\n",
            "                    0.00%  5.6510us         1  5.6510us  5.6510us  5.6510us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.0740us         3     691ns     297ns  1.1750us  cuDeviceGetCount\n",
            "                    0.00%  1.5960us         2     798ns     286ns  1.3100us  cuDeviceGet\n",
            "                    0.00%     288ns         1     288ns     288ns     288ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Plee2mF3HSI"
      },
      "source": [
        "**Pregunta ¿Qué sucede si incrementa o disminuye el valor de N?**\n",
        "Para responder a esta pregunta, probamos a ejecutar con diferentes tamaños de N para threads."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rrQ_VSzyyWe",
        "outputId": "ccdaeb6c-d5b3-4e4f-d2a6-ce371ed4b80a"
      },
      "source": [
        "!sed -i '/#define N/c\\#define N (1024*1024)' suma2dvectorNBloqxNthreads.cu\n",
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true suma2dvectorNBloqxNthreads.cu -o suma2dvectorNBloqxNthreads -lcudadevrt\n",
        "!nvprof ./suma2dvectorNBloqxNthreads"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==539== NVPROF is profiling process 539, command: ./suma2dvectorNBloqxNthreads\n",
            "==539== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            " valor a[10] es 10\n",
            " valor b[10] es 1048566\n",
            "resultado c[10] es 1048576\n",
            " valor a[0] es 0\n",
            " valor b[0] es 1048576\n",
            "resultado c[0] es 1048576\n",
            " valor a[15] es 15\n",
            " valor b[15] es 1048561\n",
            "resultado c[15] es 1048576\n",
            "==539== Profiling application: ./suma2dvectorNBloqxNthreads\n",
            "==539== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   63.60%  1.1262ms         2  563.10us  547.29us  578.91us  [CUDA memcpy HtoD]\n",
            "                   30.49%  539.84us         1  539.84us  539.84us  539.84us  [CUDA memcpy DtoH]\n",
            "                    5.91%  104.61us         1  104.61us  104.61us  104.61us  add(int*, int*, int*)\n",
            "      API calls:   97.85%  196.54ms         3  65.512ms  134.58us  196.26ms  cudaMalloc\n",
            "                    1.09%  2.1818ms         3  727.28us  605.78us  898.92us  cudaMemcpy\n",
            "                    0.67%  1.3436ms         3  447.86us  139.49us  627.39us  cudaFree\n",
            "                    0.25%  495.09us         1  495.09us  495.09us  495.09us  cuDeviceTotalMem\n",
            "                    0.10%  209.02us       101  2.0690us     153ns  110.93us  cuDeviceGetAttribute\n",
            "                    0.02%  37.142us         1  37.142us  37.142us  37.142us  cudaLaunchKernel\n",
            "                    0.01%  29.941us         1  29.941us  29.941us  29.941us  cuDeviceGetName\n",
            "                    0.00%  7.4920us         1  7.4920us  7.4920us  7.4920us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.3610us         3     787ns     183ns  1.1600us  cuDeviceGetCount\n",
            "                    0.00%  1.9100us         2     955ns     513ns  1.3970us  cuDeviceGet\n",
            "                    0.00%     283ns         1     283ns     283ns     283ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUIhc1ZLzN_F"
      },
      "source": [
        "En este caso hemos probado para un número menor de threads y vemos que el rendimiento es menor. Si lo incrementamos vemos que el resultado de la aplicación sigue siendo correcto pero porque ocupa el máximo número de threads permitidos, si bien el tamaño produce errores de memoria."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgS0a4mYzY8w",
        "outputId": "af5a56d4-6868-419f-a576-5a35b206154b"
      },
      "source": [
        "!sed -i '/#define N/c\\#define N (1024*1024*500)' suma2dvectorNBloqxNthreads.cu\n",
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true suma2dvectorNBloqxNthreads.cu -o suma2dvectorNBloqxNthreads -lcudadevrt\n",
        "!nvprof ./suma2dvectorNBloqxNthreads"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "tcmalloc: large alloc 2097152000 bytes == 0x563ef8f86000 @  0x7f1dc8ad71e7 0x563ef8254272 0x7f1dc7b08bf7 0x563ef8253fda\n",
            "tcmalloc: large alloc 2097152000 bytes == 0x563f75f86000 @  0x7f1dc8ad71e7 0x563ef8254283 0x7f1dc7b08bf7 0x563ef8253fda\n",
            "tcmalloc: large alloc 2097152000 bytes == 0x563ff2f86000 @  0x7f1dc8ad71e7 0x563ef8254294 0x7f1dc7b08bf7 0x563ef8253fda\n",
            "==584== NVPROF is profiling process 584, command: ./suma2dvectorNBloqxNthreads\n",
            "==584== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            " valor a[10] es 10\n",
            " valor b[10] es 524287990\n",
            "resultado c[10] es 524288000\n",
            " valor a[0] es 0\n",
            " valor b[0] es 524288000\n",
            "resultado c[0] es 524288000\n",
            " valor a[15] es 15\n",
            " valor b[15] es 524287985\n",
            "resultado c[15] es 524288000\n",
            "==584== Profiling application: ./suma2dvectorNBloqxNthreads\n",
            "==584== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   65.25%  568.40ms         2  284.20ms  276.02ms  292.37ms  [CUDA memcpy HtoD]\n",
            "                   29.90%  260.47ms         1  260.47ms  260.47ms  260.47ms  [CUDA memcpy DtoH]\n",
            "                    4.84%  42.188ms         1  42.188ms  42.188ms  42.188ms  add(int*, int*, int*)\n",
            "      API calls:   60.79%  871.53ms         3  290.51ms  276.19ms  302.88ms  cudaMemcpy\n",
            "                   24.53%  351.66ms         3  117.22ms  2.1975ms  177.89ms  cudaFree\n",
            "                   14.63%  209.69ms         3  69.896ms  2.7958ms  203.80ms  cudaMalloc\n",
            "                    0.04%  543.47us         1  543.47us  543.47us  543.47us  cuDeviceTotalMem\n",
            "                    0.02%  245.55us       101  2.4310us     145ns  114.94us  cuDeviceGetAttribute\n",
            "                    0.00%  44.677us         1  44.677us  44.677us  44.677us  cudaLaunchKernel\n",
            "                    0.00%  44.484us         1  44.484us  44.484us  44.484us  cuDeviceGetName\n",
            "                    0.00%  5.0450us         1  5.0450us  5.0450us  5.0450us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.1330us         3     711ns     270ns  1.0750us  cuDeviceGetCount\n",
            "                    0.00%  1.5640us         2     782ns     288ns  1.2760us  cuDeviceGet\n",
            "                    0.00%     305ns         1     305ns     305ns     305ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhpaNyib0t-b"
      },
      "source": [
        "## Ejercicio: suma de matrices cuadradas de tamaño N\n",
        "En este caso se realiza la suma de matrices para cualquier *N*, sin tener que definir números fijos y precalcular el número de bloques dado un número de threads. Para ello se ha definido el número de threads por bloque *MAX* como $\\frac{N + MAX - 1}{MAX}$. Asimismo, fijando $MAX=8$ se obtiene un tamaño de grid de $8*8 = 64$, lo que en otras palabras significa que podemos llegar al máximo número de threads sin violar ninguna restricción de CUDA. Además, se ha debido cambiar el *kernel* de la función *add* a una estructura bidimensional para que realice la suma $a_{i,j}+b_{i,j}$ y se guarde el resultado en $c_{i,j}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIb86yWWqN8T",
        "outputId": "49d7752f-0d90-4a58-8b70-4fc4c5e47549"
      },
      "source": [
        "%%writefile sumamatrix_NxN.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define N 1024\n",
        "#define MAX 8\n",
        "\n",
        "__global__ void add(float *a, float *b, float *c, int n) {\n",
        "  int j = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  int i = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "  int index = i*n + j;\n",
        "  \n",
        "  if (i < n && j < n)\n",
        "    c[index] = a[index] + b[index];\n",
        "}\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "  float *a, *b, *c;\n",
        "  float *d_a, *d_b, *d_c;\n",
        "  int size = N*N*sizeof(float);\n",
        "\n",
        "  a = (float *) malloc(size);\n",
        "  b = (float *) malloc(size);\n",
        "  c = (float *) malloc(size);\n",
        "\n",
        "  // Inicializamos los datos\n",
        "  for (int i = 0; i < N; i++ ){\n",
        "      for (int j = 0; j < N; j++){\n",
        "          a[i*N + j] = 1.0f;\n",
        "          b[i*N + j] = 2.0f;\n",
        "      }\n",
        "  }\n",
        "\n",
        "  cudaMalloc((void **)&d_a, size);\n",
        "  cudaMalloc((void **)&d_b, size);\n",
        "  cudaMalloc((void **)&d_c, size);\n",
        "\n",
        "  cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "  // grid size\n",
        "  dim3 dimGrid((N + MAX - 1)/MAX , (N + MAX - 1)/MAX, 1);\n",
        "  \n",
        "  // block size\n",
        "  dim3 dimBlock(MAX, MAX, 1);\n",
        "\n",
        "  // Launch add() kernel on GPU\n",
        "\n",
        "  add<<<dimGrid, dimBlock>>>(d_a, d_b, d_c, N);\n",
        "\n",
        "    // Copy result back to host\n",
        "  cudaError err = cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "    if(err!=cudaSuccess) {\n",
        "        printf(\"CUDA error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "    }\n",
        "  printf(\" valor a[10] es %f\\n\",a[10]);\n",
        "  printf(\" valor b[10] es %f\\n\",b[10]);\n",
        "  printf(\"resultado c[10] es %f\\n\",c[10]);\n",
        "  printf(\" valor a[0] es %f\\n\",a[0]);\n",
        "  printf(\" valor b[0] es %f\\n\",b[0]);\n",
        "  printf(\"resultado c[0] es %f\\n\",c[0]);\n",
        "  printf(\" valor a[15] es %f\\n\",a[0]);\n",
        "  printf(\" valor b[15] es %f\\n\",b[0]);\n",
        "  printf(\"resultado c[15] es %f\\n\",c[0]);\n",
        "\n",
        "  // Cleanup\n",
        "\n",
        "  free(a); free(b); free(c); \n",
        "\n",
        "  cudaFree(d_a);\n",
        "  cudaFree(d_b);\n",
        "  cudaFree(d_c);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing sumamatrix_NxN.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow545eeb41Ke"
      },
      "source": [
        "Vemos que la ejecución es correcta:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5IXknL8u5m6",
        "outputId": "13b9340a-0b87-43d9-be8a-aff34e6e9eb0"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true sumamatrix_NxN.cu -o sumamatrix_NxN -lcudadevrt\n",
        "!./sumamatrix_NxN"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            " valor a[10] es 1.000000\n",
            " valor b[10] es 2.000000\n",
            "resultado c[10] es 3.000000\n",
            " valor a[0] es 1.000000\n",
            " valor b[0] es 2.000000\n",
            "resultado c[0] es 3.000000\n",
            " valor a[15] es 1.000000\n",
            " valor b[15] es 2.000000\n",
            "resultado c[15] es 3.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYFb5Wxt45RY"
      },
      "source": [
        "Además vemos que se ejecuta en un tiempo razonable (1.7 ms)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j38xyYx5qPar",
        "outputId": "46fb4ea3-0013-4660-9795-8ad3258014f9"
      },
      "source": [
        "!nvprof ./sumamatrix_NxN"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==634== NVPROF is profiling process 634, command: ./sumamatrix_NxN\n",
            "==634== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            " valor a[10] es 1.000000\n",
            " valor b[10] es 2.000000\n",
            "resultado c[10] es 3.000000\n",
            " valor a[0] es 1.000000\n",
            " valor b[0] es 2.000000\n",
            "resultado c[0] es 3.000000\n",
            " valor a[15] es 1.000000\n",
            " valor b[15] es 2.000000\n",
            "resultado c[15] es 3.000000\n",
            "==634== Profiling application: ./sumamatrix_NxN\n",
            "==634== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   56.47%  1.4002ms         1  1.4002ms  1.4002ms  1.4002ms  [CUDA memcpy DtoH]\n",
            "                   39.05%  968.32us         2  484.16us  451.36us  516.96us  [CUDA memcpy HtoD]\n",
            "                    4.48%  110.98us         1  110.98us  110.98us  110.98us  add(float*, float*, float*, int)\n",
            "      API calls:   97.06%  193.37ms         3  64.455ms  131.88us  193.08ms  cudaMalloc\n",
            "                    1.97%  3.9274ms         3  1.3091ms  506.09us  2.7748ms  cudaMemcpy\n",
            "                    0.47%  942.70us         3  314.23us  158.01us  393.22us  cudaFree\n",
            "                    0.32%  644.16us         1  644.16us  644.16us  644.16us  cuDeviceTotalMem\n",
            "                    0.13%  260.20us       101  2.5760us     141ns  102.19us  cuDeviceGetAttribute\n",
            "                    0.02%  34.873us         1  34.873us  34.873us  34.873us  cudaLaunchKernel\n",
            "                    0.02%  30.491us         1  30.491us  30.491us  30.491us  cuDeviceGetName\n",
            "                    0.00%  5.1820us         1  5.1820us  5.1820us  5.1820us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.8040us         3     601ns     220ns     928ns  cuDeviceGetCount\n",
            "                    0.00%  1.4440us         2     722ns     338ns  1.1060us  cuDeviceGet\n",
            "                    0.00%     294ns         1     294ns     294ns     294ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZJjBuKx3VYo"
      },
      "source": [
        "#Ejercicio: ejecución de Stencil1D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uKzTtEeISQw"
      },
      "source": [
        "En este ejercicio vamos a estudiar el efecto de la memoria compartida en una GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13R0IYfr599H"
      },
      "source": [
        "### Sin memoria compartida\n",
        "Primero realizamos una versión sin memoria compartida.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85-QjdtR5sDK",
        "outputId": "f6680988-35d1-48cd-859c-e17821c64702"
      },
      "source": [
        "%%writefile stencil1d_base.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "#define RADIUS        3\n",
        "#define BLOCK_SIZE    256\n",
        "#define NUM_ELEMENTS  (4096*2)\n",
        "\n",
        "// CUDA API error checking macro\n",
        "#define cudaCheck(error) \\\n",
        "  if (error != cudaSuccess) { \\\n",
        "    printf(\"Fatal error: %s at %s:%d\\n\", \\\n",
        "      cudaGetErrorString(error), \\\n",
        "      __FILE__, __LINE__); \\\n",
        "    exit(1); \\\n",
        "  }\n",
        "\n",
        "__global__ void stencil_1d(int *in, int *out) \n",
        "{\n",
        "    // Just one global index \n",
        "    int index = threadIdx.x + (blockIdx.x * blockDim.x) + RADIUS;\n",
        "\n",
        "    // Apply the stencil\n",
        "    int result = 0;\n",
        "    for (int offset = -RADIUS ; offset <= RADIUS ; offset++)\n",
        "        result += in[index + offset];\n",
        "\n",
        "    // Store the result\n",
        "    out[index-RADIUS] = result;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  unsigned int i;\n",
        "  int h_in[NUM_ELEMENTS + 2 * RADIUS], h_out[NUM_ELEMENTS];\n",
        "  int *d_in, *d_out;\n",
        "\n",
        "  // Initialize host data\n",
        "  for( i = 0; i < (NUM_ELEMENTS + 2*RADIUS); ++i )\n",
        "    h_in[i] = 1; // With a value of 1 and RADIUS of 3, all output values should be 7\n",
        "\n",
        "  // Allocate space on the device\n",
        "  cudaCheck( cudaMalloc( &d_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int)) );\n",
        "  cudaCheck( cudaMalloc( &d_out, NUM_ELEMENTS * sizeof(int)) );\n",
        "\n",
        "  // Copy input data to device\n",
        "  cudaCheck( cudaMemcpy( d_in, h_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int), cudaMemcpyHostToDevice) );\n",
        "\n",
        "  stencil_1d<<< (NUM_ELEMENTS + BLOCK_SIZE - 1)/BLOCK_SIZE, BLOCK_SIZE >>> (d_in, d_out);\n",
        "\n",
        "  cudaCheck( cudaMemcpy( h_out, d_out, NUM_ELEMENTS * sizeof(int), cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // Verify every out value is 7\n",
        "  for( i = 0; i < NUM_ELEMENTS; ++i )\n",
        "    if (h_out[i] != 7)\n",
        "    {\n",
        "      printf(\"Element h_out[%d] == %d != 7\\n\", i, h_out[i]);\n",
        "      break;\n",
        "    }\n",
        "\n",
        "  if (i == NUM_ELEMENTS)\n",
        "    printf(\"SUCCESS!\\n\");\n",
        "\n",
        "  // Free out memory\n",
        "  cudaFree(d_in);\n",
        "  cudaFree(d_out);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing stencil1d_base.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fC5zcttL7KO_",
        "outputId": "0c11c5c9-a015-47c7-82d7-906da8c2f872"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true stencil1d_base.cu -o stencil1d_base -lcudadevrt"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0QKT55-7J-L",
        "outputId": "5207430d-fe6f-4246-e78f-e80aacf78c52"
      },
      "source": [
        "for i in range(100):\n",
        "  !./stencil1d_base"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDyxs9Tg8BHy"
      },
      "source": [
        "Ejecutamos el código 100 veces y vemos que no ha ocurrido problema. Comprobamos la ejecución con $\\texttt{nvprof}$ y vemos que se dedica un tiempo considerable a la copia de elementos, por lo que podemos mejorar en problema con un uso eficiente de memoria compartida."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9olPv5qi8NVi",
        "outputId": "33047c99-398f-4d74-8108-9a8040d2eaab"
      },
      "source": [
        "!nvprof ./stencil1d_base "
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==982== NVPROF is profiling process 982, command: ./stencil1d_base\n",
            "==982== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "SUCCESS!\n",
            "==982== Profiling application: ./stencil1d_base\n",
            "==982== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   44.68%  6.1760us         1  6.1760us  6.1760us  6.1760us  [CUDA memcpy HtoD]\n",
            "                   34.26%  4.7360us         1  4.7360us  4.7360us  4.7360us  [CUDA memcpy DtoH]\n",
            "                   21.06%  2.9120us         1  2.9120us  2.9120us  2.9120us  stencil_1d(int*, int*)\n",
            "      API calls:   99.47%  195.58ms         2  97.788ms  6.5570us  195.57ms  cudaMalloc\n",
            "                    0.27%  530.07us         1  530.07us  530.07us  530.07us  cuDeviceTotalMem\n",
            "                    0.11%  219.43us       101  2.1720us     160ns  98.401us  cuDeviceGetAttribute\n",
            "                    0.07%  142.49us         2  71.245us  13.312us  129.18us  cudaFree\n",
            "                    0.04%  76.836us         2  38.418us  28.971us  47.865us  cudaMemcpy\n",
            "                    0.02%  29.603us         1  29.603us  29.603us  29.603us  cuDeviceGetName\n",
            "                    0.01%  25.142us         1  25.142us  25.142us  25.142us  cudaLaunchKernel\n",
            "                    0.00%  7.2780us         1  7.2780us  7.2780us  7.2780us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.2350us         3     745ns     286ns  1.1210us  cuDeviceGetCount\n",
            "                    0.00%  1.5390us         2     769ns     328ns  1.2110us  cuDeviceGet\n",
            "                    0.00%     296ns         1     296ns     296ns     296ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6tDS3q28yAc"
      },
      "source": [
        "### Versión con memoria compartida, pero sin sincronización de threads\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGaG1nrF3hKZ",
        "outputId": "7292da44-ae9c-465d-86dc-25683a35cfb8"
      },
      "source": [
        "%%writefile stenciltest_sm_1.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "#define RADIUS        3\n",
        "#define BLOCK_SIZE    256\n",
        "#define NUM_ELEMENTS  (4096*2)\n",
        "\n",
        "// CUDA API error checking macro\n",
        "#define cudaCheck(error) \\\n",
        "  if (error != cudaSuccess) { \\\n",
        "    printf(\"Fatal error: %s at %s:%d\\n\", \\\n",
        "      cudaGetErrorString(error), \\\n",
        "      __FILE__, __LINE__); \\\n",
        "    exit(1); \\\n",
        "  }\n",
        "\n",
        "__global__ void stencil_1d(int *in, int *out) \n",
        "{\n",
        "    __shared__ int temp[BLOCK_SIZE + 2 * RADIUS];\n",
        "    int gindex = threadIdx.x + (blockIdx.x * blockDim.x) + RADIUS;\n",
        "    int lindex = threadIdx.x + RADIUS;\n",
        "\n",
        "    // Read input elements into shared memory\n",
        "    temp[lindex] = in[gindex];\n",
        "    if (threadIdx.x < RADIUS) \n",
        "    {\n",
        "        temp[lindex - RADIUS] = in[gindex - RADIUS];\n",
        "        temp[lindex + BLOCK_SIZE] = in[gindex + BLOCK_SIZE];\n",
        "    }\n",
        "\n",
        "    // Make sure all threads get to this point before proceeding!\n",
        "    //__syncthreads();\n",
        "\n",
        "    // Apply the stencil\n",
        "    int result = 0;\n",
        "    for (int offset = -RADIUS ; offset <= RADIUS ; offset++)\n",
        "        result += temp[lindex + offset];\n",
        "\n",
        "    // Store the result\n",
        "    out[gindex-RADIUS] = result;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  unsigned int i;\n",
        "  int h_in[NUM_ELEMENTS + 2 * RADIUS], h_out[NUM_ELEMENTS];\n",
        "  int *d_in, *d_out;\n",
        "\n",
        "  // Initialize host data\n",
        "  for( i = 0; i < (NUM_ELEMENTS + 2*RADIUS); ++i )\n",
        "    h_in[i] = 1; // With a value of 1 and RADIUS of 3, all output values should be 7\n",
        "\n",
        "  // Allocate space on the device\n",
        "  cudaCheck( cudaMalloc( &d_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int)) );\n",
        "  cudaCheck( cudaMalloc( &d_out, NUM_ELEMENTS * sizeof(int)) );\n",
        "\n",
        "  // Copy input data to device\n",
        "  cudaCheck( cudaMemcpy( d_in, h_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int), cudaMemcpyHostToDevice) );\n",
        "\n",
        "  stencil_1d<<< (NUM_ELEMENTS + BLOCK_SIZE - 1)/BLOCK_SIZE, BLOCK_SIZE >>> (d_in, d_out);\n",
        "\n",
        "  cudaCheck( cudaMemcpy( h_out, d_out, NUM_ELEMENTS * sizeof(int), cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // Verify every out value is 7\n",
        "  for( i = 0; i < NUM_ELEMENTS; ++i )\n",
        "    if (h_out[i] != 7)\n",
        "    {\n",
        "      printf(\"Element h_out[%d] == %d != 7\\n\", i, h_out[i]);\n",
        "      break;\n",
        "    }\n",
        "\n",
        "  if (i == NUM_ELEMENTS)\n",
        "    printf(\"SUCCESS!\\n\");\n",
        "\n",
        "  // Free out memory\n",
        "  cudaFree(d_in);\n",
        "  cudaFree(d_out);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing stenciltest_sm_1.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXahjwTKG4Xy",
        "outputId": "b34f560f-f13e-4ab3-db88-0c09661315fd"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true stenciltest_sm_1.cu -o ./stenciltest_sm_1 -lcudadevrt"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4t7fl5s2IIMw",
        "outputId": "9e20e5d6-4ea3-44cf-f02e-cb2cba3b5fd7"
      },
      "source": [
        "for i in range(100):\n",
        "  !./stenciltest_sm_1"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Element h_out[0] == 2 != 7\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_VaWf_3EF-C",
        "outputId": "31942ea8-f70c-446c-fb3a-0f01bb064659"
      },
      "source": [
        "!nvprof ./stenciltest_sm_1"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==1330== NVPROF is profiling process 1330, command: ./stenciltest_sm_1\n",
            "==1330== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "SUCCESS!\n",
            "==1330== Profiling application: ./stenciltest_sm_1\n",
            "==1330== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   59.65%  15.136us         1  15.136us  15.136us  15.136us  [CUDA memcpy HtoD]\n",
            "                   27.11%  6.8800us         1  6.8800us  6.8800us  6.8800us  [CUDA memcpy DtoH]\n",
            "                   13.24%  3.3600us         1  3.3600us  3.3600us  3.3600us  stencil_1d(int*, int*)\n",
            "      API calls:   99.47%  195.52ms         2  97.759ms  5.0560us  195.51ms  cudaMalloc\n",
            "                    0.25%  494.18us         1  494.18us  494.18us  494.18us  cuDeviceTotalMem\n",
            "                    0.11%  225.18us       101  2.2290us     153ns  87.800us  cuDeviceGetAttribute\n",
            "                    0.08%  149.45us         2  74.726us  12.626us  136.83us  cudaFree\n",
            "                    0.05%  98.131us         2  49.065us  37.031us  61.100us  cudaMemcpy\n",
            "                    0.02%  36.221us         1  36.221us  36.221us  36.221us  cudaLaunchKernel\n",
            "                    0.02%  29.732us         1  29.732us  29.732us  29.732us  cuDeviceGetName\n",
            "                    0.00%  8.9110us         1  8.9110us  8.9110us  8.9110us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.4320us         3     810ns     250ns  1.2670us  cuDeviceGetCount\n",
            "                    0.00%  1.8570us         2     928ns     508ns  1.3490us  cuDeviceGet\n",
            "                    0.00%     320ns         1     320ns     320ns     320ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piM0l12_9z8f"
      },
      "source": [
        "En este caso vemos que existen errores ya que no se utiliza la función $\\texttt{__syncthreads()}$ para hacer que no exista la condición de carrera con los threads. Por ello procedemos a utilizar la función y conseguir que el efecto de la memoria compartida sea seguro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SEy0nUw-Rie"
      },
      "source": [
        "### Versión con memoria compartida y sincronía de threads."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76ur4eykFTkP",
        "outputId": "c21c6270-4a30-4b8a-de42-058bb26af618"
      },
      "source": [
        "%%writefile stenciltest_sm_2.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "#define RADIUS        3\n",
        "#define BLOCK_SIZE    256\n",
        "#define NUM_ELEMENTS  (4096*2)\n",
        "\n",
        "// CUDA API error checking macro\n",
        "#define cudaCheck(error) \\\n",
        "  if (error != cudaSuccess) { \\\n",
        "    printf(\"Fatal error: %s at %s:%d\\n\", \\\n",
        "      cudaGetErrorString(error), \\\n",
        "      __FILE__, __LINE__); \\\n",
        "    exit(1); \\\n",
        "  }\n",
        "\n",
        "__global__ void stencil_1d(int *in, int *out) \n",
        "{\n",
        "    __shared__ int temp[BLOCK_SIZE + 2 * RADIUS];\n",
        "    int gindex = threadIdx.x + (blockIdx.x * blockDim.x) + RADIUS;\n",
        "    int lindex = threadIdx.x + RADIUS;\n",
        "\n",
        "    // Read input elements into shared memory\n",
        "    temp[lindex] = in[gindex];\n",
        "    if (threadIdx.x < RADIUS) \n",
        "    {\n",
        "        temp[lindex - RADIUS] = in[gindex - RADIUS];\n",
        "        temp[lindex + BLOCK_SIZE] = in[gindex + BLOCK_SIZE];\n",
        "    }\n",
        "\n",
        "    // Make sure all threads get to this point before proceeding!\n",
        "    __syncthreads();\n",
        "\n",
        "    // Apply the stencil\n",
        "    int result = 0;\n",
        "    for (int offset = -RADIUS ; offset <= RADIUS ; offset++)\n",
        "        result += temp[lindex + offset];\n",
        "\n",
        "    // Store the result\n",
        "    out[gindex-RADIUS] = result;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  unsigned int i;\n",
        "  int h_in[NUM_ELEMENTS + 2 * RADIUS], h_out[NUM_ELEMENTS];\n",
        "  int *d_in, *d_out;\n",
        "\n",
        "  // Initialize host data\n",
        "  for( i = 0; i < (NUM_ELEMENTS + 2*RADIUS); ++i )\n",
        "    h_in[i] = 1; // With a value of 1 and RADIUS of 3, all output values should be 7\n",
        "\n",
        "  // Allocate space on the device\n",
        "  cudaCheck( cudaMalloc( &d_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int)) );\n",
        "  cudaCheck( cudaMalloc( &d_out, NUM_ELEMENTS * sizeof(int)) );\n",
        "\n",
        "  // Copy input data to device\n",
        "  cudaCheck( cudaMemcpy( d_in, h_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int), cudaMemcpyHostToDevice) );\n",
        "\n",
        "  stencil_1d<<< (NUM_ELEMENTS + BLOCK_SIZE - 1)/BLOCK_SIZE, BLOCK_SIZE >>> (d_in, d_out);\n",
        "\n",
        "  cudaCheck( cudaMemcpy( h_out, d_out, NUM_ELEMENTS * sizeof(int), cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // Verify every out value is 7\n",
        "  for( i = 0; i < NUM_ELEMENTS; ++i )\n",
        "    if (h_out[i] != 7)\n",
        "    {\n",
        "      printf(\"Element h_out[%d] == %d != 7\\n\", i, h_out[i]);\n",
        "      break;\n",
        "    }\n",
        "\n",
        "  if (i == NUM_ELEMENTS)\n",
        "    printf(\"SUCCESS!\\n\");\n",
        "\n",
        "  // Free out memory\n",
        "  cudaFree(d_in);\n",
        "  cudaFree(d_out);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing stenciltest_sm_2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CamqRgGsHxU-",
        "outputId": "fe43b2d7-b02e-4fae-cad3-179ed762b875"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true stenciltest_sm_2.cu -o ./stenciltest_sm_2 -lcudadevrt"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9tmF1F4H1jY",
        "outputId": "72f7d782-378d-49b0-91dd-bb5c54e7a37c"
      },
      "source": [
        "for i in range(100):\n",
        "  !./stenciltest_sm_2"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bodklbXLH-LF",
        "outputId": "c3fcc792-8b81-47a4-b9c9-508c78e46e49"
      },
      "source": [
        "!nvprof ./stenciltest_sm_2"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==1678== NVPROF is profiling process 1678, command: ./stenciltest_sm_2\n",
            "==1678== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "==1678== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
            "SUCCESS!\n",
            "==1678== Profiling application: ./stenciltest_sm_2\n",
            "==1678== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   45.89%  9.1200us         1  9.1200us  9.1200us  9.1200us  [CUDA memcpy HtoD]\n",
            "                   34.14%  6.7840us         1  6.7840us  6.7840us  6.7840us  [CUDA memcpy DtoH]\n",
            "                   19.97%  3.9680us         1  3.9680us  3.9680us  3.9680us  stencil_1d(int*, int*)\n",
            "      API calls:   99.44%  180.20ms         2  90.100ms  5.0490us  180.20ms  cudaMalloc\n",
            "                    0.26%  465.07us         1  465.07us  465.07us  465.07us  cuDeviceTotalMem\n",
            "                    0.12%  215.93us       101  2.1370us     153ns  95.719us  cuDeviceGetAttribute\n",
            "                    0.08%  141.33us         2  70.666us  16.060us  125.27us  cudaFree\n",
            "                    0.05%  98.870us         2  49.435us  25.767us  73.103us  cudaMemcpy\n",
            "                    0.03%  59.289us         1  59.289us  59.289us  59.289us  cudaLaunchKernel\n",
            "                    0.02%  28.669us         1  28.669us  28.669us  28.669us  cuDeviceGetName\n",
            "                    0.00%  8.3000us         1  8.3000us  8.3000us  8.3000us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.8640us         3     621ns     246ns     986ns  cuDeviceGetCount\n",
            "                    0.00%  1.5890us         2     794ns     312ns  1.2770us  cuDeviceGet\n",
            "                    0.00%     259ns         1     259ns     259ns     259ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9VI8UK6-pl9"
      },
      "source": [
        "Gracias a la utilización de la barrera se garantiza que todos los threads lleguen a tiempo y se realice una correcta ejecución. \n",
        "\n",
        "Vemos que en este caso no hay una diferencia notable en tiempos de ejecución, lo que puede deberse a un tamaño bajo de radio y vector. Cambiamos los valores y comparamos con la versión base y con la de memoria compartida con sincronía de threads."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyZfMttJ_5RL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1969a3c-cdc9-478b-9368-74a445aaaaeb"
      },
      "source": [
        "!sed -i '/#define RADIUS/c\\#define RADIUS 5' stencil1d_base.cu\n",
        "!sed -i '/#define NUM_ELEMENTS/c\\#define NUM_ELEMENTS (4096*16)' stencil1d_base.cu\n",
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true stencil1d_base.cu -o stencil1d_base -lcudadevrt\n",
        "!nvprof ./stencil1d_base"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==1726== NVPROF is profiling process 1726, command: ./stencil1d_base\n",
            "==1726== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "Element h_out[0] == 11 != 7\n",
            "==1726== Profiling application: ./stencil1d_base\n",
            "==1726== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   41.92%  38.912us         1  38.912us  38.912us  38.912us  [CUDA memcpy HtoD]\n",
            "                   38.30%  35.552us         1  35.552us  35.552us  35.552us  [CUDA memcpy DtoH]\n",
            "                   19.79%  18.367us         1  18.367us  18.367us  18.367us  stencil_1d(int*, int*)\n",
            "      API calls:   99.26%  193.92ms         2  96.962ms  5.1930us  193.92ms  cudaMalloc\n",
            "                    0.31%  601.76us         1  601.76us  601.76us  601.76us  cuDeviceTotalMem\n",
            "                    0.18%  358.76us         2  179.38us  82.407us  276.35us  cudaMemcpy\n",
            "                    0.11%  209.98us         2  104.99us  16.115us  193.86us  cudaFree\n",
            "                    0.11%  205.69us       101  2.0360us     142ns  84.221us  cuDeviceGetAttribute\n",
            "                    0.02%  31.215us         1  31.215us  31.215us  31.215us  cuDeviceGetName\n",
            "                    0.01%  25.326us         1  25.326us  25.326us  25.326us  cudaLaunchKernel\n",
            "                    0.00%  4.8560us         1  4.8560us  4.8560us  4.8560us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.7670us         3     922ns     339ns  1.3510us  cuDeviceGetCount\n",
            "                    0.00%  1.9930us         2     996ns     462ns  1.5310us  cuDeviceGet\n",
            "                    0.00%     276ns         1     276ns     276ns     276ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrlPuv4M_5LB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b792723e-e076-457d-c80e-5cc7ddba3b2a"
      },
      "source": [
        "!sed -i '/#define RADIUS/c\\#define RADIUS 5' stenciltest_sm_2.cu\n",
        "!sed -i '/#define NUM_ELEMENTS/c\\#define NUM_ELEMENTS (4096*16)' stenciltest_sm_2.cu\n",
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true stenciltest_sm_2.cu -o stenciltest_sm_2 -lcudadevrt\n",
        "!nvprof ./stenciltest_sm_2"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==1772== NVPROF is profiling process 1772, command: ./stenciltest_sm_2\n",
            "==1772== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "==1772== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
            "Element h_out[0] == 11 != 7\n",
            "==1772== Profiling application: ./stenciltest_sm_2\n",
            "==1772== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   50.74%  46.912us         1  46.912us  46.912us  46.912us  [CUDA memcpy HtoD]\n",
            "                   38.63%  35.712us         1  35.712us  35.712us  35.712us  [CUDA memcpy DtoH]\n",
            "                   10.63%  9.8240us         1  9.8240us  9.8240us  9.8240us  stencil_1d(int*, int*)\n",
            "      API calls:   99.37%  199.97ms         2  99.985ms  5.0200us  199.96ms  cudaMalloc\n",
            "                    0.24%  492.70us         1  492.70us  492.70us  492.70us  cuDeviceTotalMem\n",
            "                    0.15%  297.45us         2  148.72us  82.558us  214.89us  cudaMemcpy\n",
            "                    0.10%  208.71us       101  2.0660us     144ns  95.397us  cuDeviceGetAttribute\n",
            "                    0.08%  163.28us         2  81.641us  13.128us  150.15us  cudaFree\n",
            "                    0.03%  69.300us         1  69.300us  69.300us  69.300us  cudaLaunchKernel\n",
            "                    0.01%  28.251us         1  28.251us  28.251us  28.251us  cuDeviceGetName\n",
            "                    0.00%  7.5830us         1  7.5830us  7.5830us  7.5830us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.3530us         3     784ns     242ns  1.1130us  cuDeviceGetCount\n",
            "                    0.00%  1.7210us         2     860ns     571ns  1.1500us  cuDeviceGet\n",
            "                    0.00%     409ns         1     409ns     409ns     409ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU0BHCiQJIEd"
      },
      "source": [
        "#EJERCICIO OPCIONAL: Producto de matrices en GPU\n",
        "\n",
        "En este código, solo se utiliza un bloque con 16x16 threads y el reparto de trabajo a los threads se ha definido de manera bidimensional.\n",
        "En la ejecución serie de la CPU las operaciones realizadas son $O(N^3)$ mientras que para la GPU se ha reducido a $O(N^2)$ las operaciones que realiza cada thread.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOQ4Bjol1DRq"
      },
      "source": [
        "Primero ejecutamos el algoritmo proporcionado en CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HL_ZsWh6_Dy",
        "outputId": "759afafb-60b7-4fa4-97c7-6c8a21550553"
      },
      "source": [
        "%%writefile matrixMul_cpu.cu\n",
        "#include <stdio.h>\n",
        "#define N 16\n",
        "#define M 1\n",
        "\n",
        "void matrixMultCPU(int a[N][N], int b[N][N], int c[N][N]){\n",
        "    int n, m;\n",
        "    for (int i = 0; i < N; i++){\n",
        "        for (int j = 0; j < N; j++){\n",
        "            int sum = 0;\n",
        "            for (int k = 0; k < N; k++){\n",
        "                m = a[i][k];\n",
        "                n = b[k][j];\n",
        "                sum += m*n;\n",
        "            }\n",
        "            c[i][j] = sum;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    int a[N][N], b[N][N], c[N][N];\n",
        "    int cont, i , j;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    for (i = 0; i < N; i++){\n",
        "        cont = 0;\n",
        "        for (j = 0; j < N; j++){\n",
        "            a[i][j] = cont;\n",
        "            b[i][j] = cont;\n",
        "            cont++;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    int nIter = 1000;\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    for (int m = 0; m < nIter; m++){\n",
        "        matrixMultCPU(a, b, c);\n",
        "    }\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float msecTotal = 0.0;\n",
        "    cudaEventElapsedTime(&msecTotal, start, stop);\n",
        "\n",
        "    for (int y = 0; y < N; y++){\n",
        "        for (int x = 0; x < N; x++){\n",
        "            printf(\"[%d][%d]=%d \", y, x, c[x][y]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    float msecPerKernelExecution = msecTotal / nIter;\n",
        "    double flopsPerMMull = 2.0 * N * N * N;\n",
        "    double gigaFlops = (flopsPerMMull * 1.0e-9f) / (msecPerKernelExecution / 1000.0f);\n",
        "\n",
        "    printf(\"GigaFlops: %f\", gigaFlops);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing matrixMul_cpu.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucU3ablX7fvi",
        "outputId": "fa6f46c5-53b6-4b67-dcc5-22b35b570411"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true matrixMul_cpu.cu -o matrixMul_cpu -lcudadevrt\n",
        "!nvprof ./matrixMul_cpu"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==1816== NVPROF is profiling process 1816, command: ./matrixMul_cpu\n",
            "==1816== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "[0][0]=0 [0][1]=0 [0][2]=0 [0][3]=0 [0][4]=0 [0][5]=0 [0][6]=0 [0][7]=0 [0][8]=0 [0][9]=0 [0][10]=0 [0][11]=0 [0][12]=0 [0][13]=0 [0][14]=0 [0][15]=0 \n",
            "[1][0]=120 [1][1]=120 [1][2]=120 [1][3]=120 [1][4]=120 [1][5]=120 [1][6]=120 [1][7]=120 [1][8]=120 [1][9]=120 [1][10]=120 [1][11]=120 [1][12]=120 [1][13]=120 [1][14]=120 [1][15]=120 \n",
            "[2][0]=240 [2][1]=240 [2][2]=240 [2][3]=240 [2][4]=240 [2][5]=240 [2][6]=240 [2][7]=240 [2][8]=240 [2][9]=240 [2][10]=240 [2][11]=240 [2][12]=240 [2][13]=240 [2][14]=240 [2][15]=240 \n",
            "[3][0]=360 [3][1]=360 [3][2]=360 [3][3]=360 [3][4]=360 [3][5]=360 [3][6]=360 [3][7]=360 [3][8]=360 [3][9]=360 [3][10]=360 [3][11]=360 [3][12]=360 [3][13]=360 [3][14]=360 [3][15]=360 \n",
            "[4][0]=480 [4][1]=480 [4][2]=480 [4][3]=480 [4][4]=480 [4][5]=480 [4][6]=480 [4][7]=480 [4][8]=480 [4][9]=480 [4][10]=480 [4][11]=480 [4][12]=480 [4][13]=480 [4][14]=480 [4][15]=480 \n",
            "[5][0]=600 [5][1]=600 [5][2]=600 [5][3]=600 [5][4]=600 [5][5]=600 [5][6]=600 [5][7]=600 [5][8]=600 [5][9]=600 [5][10]=600 [5][11]=600 [5][12]=600 [5][13]=600 [5][14]=600 [5][15]=600 \n",
            "[6][0]=720 [6][1]=720 [6][2]=720 [6][3]=720 [6][4]=720 [6][5]=720 [6][6]=720 [6][7]=720 [6][8]=720 [6][9]=720 [6][10]=720 [6][11]=720 [6][12]=720 [6][13]=720 [6][14]=720 [6][15]=720 \n",
            "[7][0]=840 [7][1]=840 [7][2]=840 [7][3]=840 [7][4]=840 [7][5]=840 [7][6]=840 [7][7]=840 [7][8]=840 [7][9]=840 [7][10]=840 [7][11]=840 [7][12]=840 [7][13]=840 [7][14]=840 [7][15]=840 \n",
            "[8][0]=960 [8][1]=960 [8][2]=960 [8][3]=960 [8][4]=960 [8][5]=960 [8][6]=960 [8][7]=960 [8][8]=960 [8][9]=960 [8][10]=960 [8][11]=960 [8][12]=960 [8][13]=960 [8][14]=960 [8][15]=960 \n",
            "[9][0]=1080 [9][1]=1080 [9][2]=1080 [9][3]=1080 [9][4]=1080 [9][5]=1080 [9][6]=1080 [9][7]=1080 [9][8]=1080 [9][9]=1080 [9][10]=1080 [9][11]=1080 [9][12]=1080 [9][13]=1080 [9][14]=1080 [9][15]=1080 \n",
            "[10][0]=1200 [10][1]=1200 [10][2]=1200 [10][3]=1200 [10][4]=1200 [10][5]=1200 [10][6]=1200 [10][7]=1200 [10][8]=1200 [10][9]=1200 [10][10]=1200 [10][11]=1200 [10][12]=1200 [10][13]=1200 [10][14]=1200 [10][15]=1200 \n",
            "[11][0]=1320 [11][1]=1320 [11][2]=1320 [11][3]=1320 [11][4]=1320 [11][5]=1320 [11][6]=1320 [11][7]=1320 [11][8]=1320 [11][9]=1320 [11][10]=1320 [11][11]=1320 [11][12]=1320 [11][13]=1320 [11][14]=1320 [11][15]=1320 \n",
            "[12][0]=1440 [12][1]=1440 [12][2]=1440 [12][3]=1440 [12][4]=1440 [12][5]=1440 [12][6]=1440 [12][7]=1440 [12][8]=1440 [12][9]=1440 [12][10]=1440 [12][11]=1440 [12][12]=1440 [12][13]=1440 [12][14]=1440 [12][15]=1440 \n",
            "[13][0]=1560 [13][1]=1560 [13][2]=1560 [13][3]=1560 [13][4]=1560 [13][5]=1560 [13][6]=1560 [13][7]=1560 [13][8]=1560 [13][9]=1560 [13][10]=1560 [13][11]=1560 [13][12]=1560 [13][13]=1560 [13][14]=1560 [13][15]=1560 \n",
            "[14][0]=1680 [14][1]=1680 [14][2]=1680 [14][3]=1680 [14][4]=1680 [14][5]=1680 [14][6]=1680 [14][7]=1680 [14][8]=1680 [14][9]=1680 [14][10]=1680 [14][11]=1680 [14][12]=1680 [14][13]=1680 [14][14]=1680 [14][15]=1680 \n",
            "[15][0]=1800 [15][1]=1800 [15][2]=1800 [15][3]=1800 [15][4]=1800 [15][5]=1800 [15][6]=1800 [15][7]=1800 [15][8]=1800 [15][9]=1800 [15][10]=1800 [15][11]=1800 [15][12]=1800 [15][13]=1800 [15][14]=1800 [15][15]=1800 \n",
            "GigaFlops: 0.507847==1816== Profiling application: ./matrixMul_cpu\n",
            "==1816== Profiling result:\n",
            "No kernels were profiled.\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            "      API calls:   99.60%  186.31ms         2  93.155ms  1.1980us  186.31ms  cudaEventCreate\n",
            "                    0.27%  502.27us         1  502.27us  502.27us  502.27us  cuDeviceTotalMem\n",
            "                    0.09%  170.14us       101  1.6840us     156ns  70.451us  cuDeviceGetAttribute\n",
            "                    0.02%  34.315us         1  34.315us  34.315us  34.315us  cuDeviceGetName\n",
            "                    0.01%  18.083us         2  9.0410us  8.3880us  9.6950us  cudaEventRecord\n",
            "                    0.00%  9.0790us         1  9.0790us  9.0790us  9.0790us  cudaEventSynchronize\n",
            "                    0.00%  5.5660us         1  5.5660us  5.5660us  5.5660us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.4270us         1  2.4270us  2.4270us  2.4270us  cudaEventElapsedTime\n",
            "                    0.00%  2.0270us         3     675ns     170ns  1.0440us  cuDeviceGetCount\n",
            "                    0.00%  1.8310us         2     915ns     313ns  1.5180us  cuDeviceGet\n",
            "                    0.00%     318ns         1     318ns     318ns     318ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlgsgPbl7oso"
      },
      "source": [
        "Ahora ejecutamos el ejemplo para GPU usando memoria global."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjbJNj7jvEcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "164e673c-f0e2-4c35-9443-442bb7699a2c"
      },
      "source": [
        "%%writefile matrixMul_gpu.cu\n",
        "#include <stdio.h>\n",
        "#define N 16\n",
        "#define M 1\n",
        "\n",
        "__global__ void matrixMultGPU(int *a, int *b, int *c){\n",
        "    int k, sum = 0;\n",
        "    int col = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "    int fil = threadIdx.y + blockDim.y * blockIdx.y;\n",
        "    if (col < N && fil < N){\n",
        "        for (k = 0; k < N; k++){\n",
        "            sum += a[fil*N+k] * b[k*N+col];\n",
        "        }\n",
        "        c[fil*N+col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    int a[N][N], b[N][N], c[N][N];\n",
        "    int *dev_a, *dev_b, *dev_c;\n",
        "    int cont, i , j;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    for (i = 0; i < N; i++){\n",
        "        cont = 0;\n",
        "        for (j = 0; j < N; j++){\n",
        "            a[i][j] = cont;\n",
        "            b[i][j] = cont;\n",
        "            cont++;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    int size = N*N*sizeof(int);\n",
        "    cudaMalloc((void **) &dev_a, size);\n",
        "    cudaMalloc((void **) &dev_b, size);\n",
        "    cudaMalloc((void **) &dev_c, size);\n",
        "\n",
        "    cudaMemcpy(dev_a, a, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 dimGrid(M, M);\n",
        "    dim3 dimBlock(N, N);\n",
        "\n",
        "    int nIter = 1000;\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    for (int m = 0; m < nIter; m++){\n",
        "        matrixMultGPU<<<dimGrid, dimBlock>>>(dev_a, dev_b, dev_c);\n",
        "    }\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    cudaMemcpy(c, dev_c, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    float msecTotal = 0.0;\n",
        "    cudaEventElapsedTime(&msecTotal, start, stop);\n",
        "\n",
        "    cudaFree(dev_a);\n",
        "    cudaFree(dev_b);\n",
        "    cudaFree(dev_c);\n",
        "\n",
        "    for (int y = 0; y < N; y++){\n",
        "        for (int x = 0; x < N; x++){\n",
        "            printf(\"[%d][%d]=%d \", y, x, c[x][y]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    float msecPerKernelExecution = msecTotal / nIter;\n",
        "    double flopsPerMMull = 2.0 * N * N * N;\n",
        "    double gigaFlops = (flopsPerMMull * 1.0e-9f) / (msecPerKernelExecution / 1000.0f);\n",
        "\n",
        "    printf(\"GigaFlops: %f\", gigaFlops);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing matrixMul_gpu.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdBDoJo6weQ1",
        "outputId": "be9e658a-5735-49f8-e5d8-b8b761a3e37d"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true matrixMul_gpu.cu -o matrixMul_gpu -lcudadevrt\n",
        "!./matrixMul_gpu"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "[0][0]=0 [0][1]=0 [0][2]=0 [0][3]=0 [0][4]=0 [0][5]=0 [0][6]=0 [0][7]=0 [0][8]=0 [0][9]=0 [0][10]=0 [0][11]=0 [0][12]=0 [0][13]=0 [0][14]=0 [0][15]=0 \n",
            "[1][0]=120 [1][1]=120 [1][2]=120 [1][3]=120 [1][4]=120 [1][5]=120 [1][6]=120 [1][7]=120 [1][8]=120 [1][9]=120 [1][10]=120 [1][11]=120 [1][12]=120 [1][13]=120 [1][14]=120 [1][15]=120 \n",
            "[2][0]=240 [2][1]=240 [2][2]=240 [2][3]=240 [2][4]=240 [2][5]=240 [2][6]=240 [2][7]=240 [2][8]=240 [2][9]=240 [2][10]=240 [2][11]=240 [2][12]=240 [2][13]=240 [2][14]=240 [2][15]=240 \n",
            "[3][0]=360 [3][1]=360 [3][2]=360 [3][3]=360 [3][4]=360 [3][5]=360 [3][6]=360 [3][7]=360 [3][8]=360 [3][9]=360 [3][10]=360 [3][11]=360 [3][12]=360 [3][13]=360 [3][14]=360 [3][15]=360 \n",
            "[4][0]=480 [4][1]=480 [4][2]=480 [4][3]=480 [4][4]=480 [4][5]=480 [4][6]=480 [4][7]=480 [4][8]=480 [4][9]=480 [4][10]=480 [4][11]=480 [4][12]=480 [4][13]=480 [4][14]=480 [4][15]=480 \n",
            "[5][0]=600 [5][1]=600 [5][2]=600 [5][3]=600 [5][4]=600 [5][5]=600 [5][6]=600 [5][7]=600 [5][8]=600 [5][9]=600 [5][10]=600 [5][11]=600 [5][12]=600 [5][13]=600 [5][14]=600 [5][15]=600 \n",
            "[6][0]=720 [6][1]=720 [6][2]=720 [6][3]=720 [6][4]=720 [6][5]=720 [6][6]=720 [6][7]=720 [6][8]=720 [6][9]=720 [6][10]=720 [6][11]=720 [6][12]=720 [6][13]=720 [6][14]=720 [6][15]=720 \n",
            "[7][0]=840 [7][1]=840 [7][2]=840 [7][3]=840 [7][4]=840 [7][5]=840 [7][6]=840 [7][7]=840 [7][8]=840 [7][9]=840 [7][10]=840 [7][11]=840 [7][12]=840 [7][13]=840 [7][14]=840 [7][15]=840 \n",
            "[8][0]=960 [8][1]=960 [8][2]=960 [8][3]=960 [8][4]=960 [8][5]=960 [8][6]=960 [8][7]=960 [8][8]=960 [8][9]=960 [8][10]=960 [8][11]=960 [8][12]=960 [8][13]=960 [8][14]=960 [8][15]=960 \n",
            "[9][0]=1080 [9][1]=1080 [9][2]=1080 [9][3]=1080 [9][4]=1080 [9][5]=1080 [9][6]=1080 [9][7]=1080 [9][8]=1080 [9][9]=1080 [9][10]=1080 [9][11]=1080 [9][12]=1080 [9][13]=1080 [9][14]=1080 [9][15]=1080 \n",
            "[10][0]=1200 [10][1]=1200 [10][2]=1200 [10][3]=1200 [10][4]=1200 [10][5]=1200 [10][6]=1200 [10][7]=1200 [10][8]=1200 [10][9]=1200 [10][10]=1200 [10][11]=1200 [10][12]=1200 [10][13]=1200 [10][14]=1200 [10][15]=1200 \n",
            "[11][0]=1320 [11][1]=1320 [11][2]=1320 [11][3]=1320 [11][4]=1320 [11][5]=1320 [11][6]=1320 [11][7]=1320 [11][8]=1320 [11][9]=1320 [11][10]=1320 [11][11]=1320 [11][12]=1320 [11][13]=1320 [11][14]=1320 [11][15]=1320 \n",
            "[12][0]=1440 [12][1]=1440 [12][2]=1440 [12][3]=1440 [12][4]=1440 [12][5]=1440 [12][6]=1440 [12][7]=1440 [12][8]=1440 [12][9]=1440 [12][10]=1440 [12][11]=1440 [12][12]=1440 [12][13]=1440 [12][14]=1440 [12][15]=1440 \n",
            "[13][0]=1560 [13][1]=1560 [13][2]=1560 [13][3]=1560 [13][4]=1560 [13][5]=1560 [13][6]=1560 [13][7]=1560 [13][8]=1560 [13][9]=1560 [13][10]=1560 [13][11]=1560 [13][12]=1560 [13][13]=1560 [13][14]=1560 [13][15]=1560 \n",
            "[14][0]=1680 [14][1]=1680 [14][2]=1680 [14][3]=1680 [14][4]=1680 [14][5]=1680 [14][6]=1680 [14][7]=1680 [14][8]=1680 [14][9]=1680 [14][10]=1680 [14][11]=1680 [14][12]=1680 [14][13]=1680 [14][14]=1680 [14][15]=1680 \n",
            "[15][0]=1800 [15][1]=1800 [15][2]=1800 [15][3]=1800 [15][4]=1800 [15][5]=1800 [15][6]=1800 [15][7]=1800 [15][8]=1800 [15][9]=1800 [15][10]=1800 [15][11]=1800 [15][12]=1800 [15][13]=1800 [15][14]=1800 [15][15]=1800 \n",
            "GigaFlops: 0.832718"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmENwJd7x-Ai",
        "outputId": "556c8a50-6324-4459-e6cd-0a929746139e"
      },
      "source": [
        "!nvprof ./matrixMul_gpu"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==1863== NVPROF is profiling process 1863, command: ./matrixMul_gpu\n",
            "==1863== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "[0][0]=0 [0][1]=0 [0][2]=0 [0][3]=0 [0][4]=0 [0][5]=0 [0][6]=0 [0][7]=0 [0][8]=0 [0][9]=0 [0][10]=0 [0][11]=0 [0][12]=0 [0][13]=0 [0][14]=0 [0][15]=0 \n",
            "[1][0]=120 [1][1]=120 [1][2]=120 [1][3]=120 [1][4]=120 [1][5]=120 [1][6]=120 [1][7]=120 [1][8]=120 [1][9]=120 [1][10]=120 [1][11]=120 [1][12]=120 [1][13]=120 [1][14]=120 [1][15]=120 \n",
            "[2][0]=240 [2][1]=240 [2][2]=240 [2][3]=240 [2][4]=240 [2][5]=240 [2][6]=240 [2][7]=240 [2][8]=240 [2][9]=240 [2][10]=240 [2][11]=240 [2][12]=240 [2][13]=240 [2][14]=240 [2][15]=240 \n",
            "[3][0]=360 [3][1]=360 [3][2]=360 [3][3]=360 [3][4]=360 [3][5]=360 [3][6]=360 [3][7]=360 [3][8]=360 [3][9]=360 [3][10]=360 [3][11]=360 [3][12]=360 [3][13]=360 [3][14]=360 [3][15]=360 \n",
            "[4][0]=480 [4][1]=480 [4][2]=480 [4][3]=480 [4][4]=480 [4][5]=480 [4][6]=480 [4][7]=480 [4][8]=480 [4][9]=480 [4][10]=480 [4][11]=480 [4][12]=480 [4][13]=480 [4][14]=480 [4][15]=480 \n",
            "[5][0]=600 [5][1]=600 [5][2]=600 [5][3]=600 [5][4]=600 [5][5]=600 [5][6]=600 [5][7]=600 [5][8]=600 [5][9]=600 [5][10]=600 [5][11]=600 [5][12]=600 [5][13]=600 [5][14]=600 [5][15]=600 \n",
            "[6][0]=720 [6][1]=720 [6][2]=720 [6][3]=720 [6][4]=720 [6][5]=720 [6][6]=720 [6][7]=720 [6][8]=720 [6][9]=720 [6][10]=720 [6][11]=720 [6][12]=720 [6][13]=720 [6][14]=720 [6][15]=720 \n",
            "[7][0]=840 [7][1]=840 [7][2]=840 [7][3]=840 [7][4]=840 [7][5]=840 [7][6]=840 [7][7]=840 [7][8]=840 [7][9]=840 [7][10]=840 [7][11]=840 [7][12]=840 [7][13]=840 [7][14]=840 [7][15]=840 \n",
            "[8][0]=960 [8][1]=960 [8][2]=960 [8][3]=960 [8][4]=960 [8][5]=960 [8][6]=960 [8][7]=960 [8][8]=960 [8][9]=960 [8][10]=960 [8][11]=960 [8][12]=960 [8][13]=960 [8][14]=960 [8][15]=960 \n",
            "[9][0]=1080 [9][1]=1080 [9][2]=1080 [9][3]=1080 [9][4]=1080 [9][5]=1080 [9][6]=1080 [9][7]=1080 [9][8]=1080 [9][9]=1080 [9][10]=1080 [9][11]=1080 [9][12]=1080 [9][13]=1080 [9][14]=1080 [9][15]=1080 \n",
            "[10][0]=1200 [10][1]=1200 [10][2]=1200 [10][3]=1200 [10][4]=1200 [10][5]=1200 [10][6]=1200 [10][7]=1200 [10][8]=1200 [10][9]=1200 [10][10]=1200 [10][11]=1200 [10][12]=1200 [10][13]=1200 [10][14]=1200 [10][15]=1200 \n",
            "[11][0]=1320 [11][1]=1320 [11][2]=1320 [11][3]=1320 [11][4]=1320 [11][5]=1320 [11][6]=1320 [11][7]=1320 [11][8]=1320 [11][9]=1320 [11][10]=1320 [11][11]=1320 [11][12]=1320 [11][13]=1320 [11][14]=1320 [11][15]=1320 \n",
            "[12][0]=1440 [12][1]=1440 [12][2]=1440 [12][3]=1440 [12][4]=1440 [12][5]=1440 [12][6]=1440 [12][7]=1440 [12][8]=1440 [12][9]=1440 [12][10]=1440 [12][11]=1440 [12][12]=1440 [12][13]=1440 [12][14]=1440 [12][15]=1440 \n",
            "[13][0]=1560 [13][1]=1560 [13][2]=1560 [13][3]=1560 [13][4]=1560 [13][5]=1560 [13][6]=1560 [13][7]=1560 [13][8]=1560 [13][9]=1560 [13][10]=1560 [13][11]=1560 [13][12]=1560 [13][13]=1560 [13][14]=1560 [13][15]=1560 \n",
            "[14][0]=1680 [14][1]=1680 [14][2]=1680 [14][3]=1680 [14][4]=1680 [14][5]=1680 [14][6]=1680 [14][7]=1680 [14][8]=1680 [14][9]=1680 [14][10]=1680 [14][11]=1680 [14][12]=1680 [14][13]=1680 [14][14]=1680 [14][15]=1680 \n",
            "[15][0]=1800 [15][1]=1800 [15][2]=1800 [15][3]=1800 [15][4]=1800 [15][5]=1800 [15][6]=1800 [15][7]=1800 [15][8]=1800 [15][9]=1800 [15][10]=1800 [15][11]=1800 [15][12]=1800 [15][13]=1800 [15][14]=1800 [15][15]=1800 \n",
            "GigaFlops: 0.831215==1863== Profiling application: ./matrixMul_gpu\n",
            "==1863== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.87%  4.9981ms      1000  4.9980us  4.8640us  8.6400us  matrixMultGPU(int*, int*, int*)\n",
            "                    0.08%  4.1920us         2  2.0960us  1.7920us  2.4000us  [CUDA memcpy HtoD]\n",
            "                    0.05%  2.5280us         1  2.5280us  2.5280us  2.5280us  [CUDA memcpy DtoH]\n",
            "      API calls:   94.78%  190.62ms         2  95.312ms  1.5170us  190.62ms  cudaEventCreate\n",
            "                    2.75%  5.5231ms      1000  5.5230us  4.3250us  25.876us  cudaLaunchKernel\n",
            "                    1.86%  3.7320ms         1  3.7320ms  3.7320ms  3.7320ms  cudaEventSynchronize\n",
            "                    0.27%  540.07us         1  540.07us  540.07us  540.07us  cuDeviceTotalMem\n",
            "                    0.12%  240.81us       101  2.3840us     150ns  98.082us  cuDeviceGetAttribute\n",
            "                    0.10%  205.26us         3  68.419us  2.1120us  198.60us  cudaMalloc\n",
            "                    0.07%  136.61us         3  45.537us  2.6610us  126.71us  cudaFree\n",
            "                    0.03%  60.234us         3  20.078us  13.536us  26.929us  cudaMemcpy\n",
            "                    0.01%  26.781us         1  26.781us  26.781us  26.781us  cuDeviceGetName\n",
            "                    0.00%  9.7810us         1  9.7810us  9.7810us  9.7810us  cuDeviceGetPCIBusId\n",
            "                    0.00%  7.4850us         2  3.7420us  3.6240us  3.8610us  cudaEventRecord\n",
            "                    0.00%  2.5070us         1  2.5070us  2.5070us  2.5070us  cudaEventElapsedTime\n",
            "                    0.00%  2.0420us         3     680ns     173ns     982ns  cuDeviceGetCount\n",
            "                    0.00%  1.7550us         2     877ns     443ns  1.3120us  cuDeviceGet\n",
            "                    0.00%     422ns         1     422ns     422ns     422ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcRkQU69C9pN"
      },
      "source": [
        "Vemos que el tiempo de ejecución es mucho mejor en GPU. Ahora variamos el número de hilos por bloque y ajustamos el tamaño de particionado, al igual que hicimos en el ejemplo de la suma de vectores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcmLBwho75SE",
        "outputId": "e3d6ff31-db43-4b71-f825-40fb87b78476"
      },
      "source": [
        "%%writefile matrixMul_gpu2.cu\n",
        "#include <stdio.h>\n",
        "#define N 64\n",
        "#define MAX 8\n",
        "\n",
        "__global__ void matrixMultGPU(int *a, int *b, int *c){\n",
        "    int k, sum = 0;\n",
        "    int col = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "    int fil = threadIdx.y + blockDim.y * blockIdx.y;\n",
        "    if (col < N && fil < N){\n",
        "        for (k = 0; k < N; k++){\n",
        "            sum += a[fil*N+k] * b[k*N+col];\n",
        "        }\n",
        "        c[fil*N+col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    int a[N][N], b[N][N], c[N][N];\n",
        "    int *dev_a, *dev_b, *dev_c;\n",
        "    int cont, i , j;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    for (i = 0; i < N; i++){\n",
        "        cont = 0;\n",
        "        for (j = 0; j < N; j++){\n",
        "            a[i][j] = cont;\n",
        "            b[i][j] = cont;\n",
        "            cont++;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    int size = N*N*sizeof(int);\n",
        "    cudaMalloc((void **) &dev_a, size);\n",
        "    cudaMalloc((void **) &dev_b, size);\n",
        "    cudaMalloc((void **) &dev_c, size);\n",
        "\n",
        "    cudaMemcpy(dev_a, a, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 dimGrid((N + MAX - 1)/MAX, (N + MAX - 1)/MAX);\n",
        "    dim3 dimBlock(MAX, MAX);\n",
        "\n",
        "    int nIter = 1000;\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    for (int m = 0; m < nIter; m++){\n",
        "        matrixMultGPU<<<dimGrid, dimBlock>>>(dev_a, dev_b, dev_c);\n",
        "    }\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    cudaMemcpy(c, dev_c, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    float msecTotal = 0.0;\n",
        "    cudaEventElapsedTime(&msecTotal, start, stop);\n",
        "\n",
        "    cudaFree(dev_a);\n",
        "    cudaFree(dev_b);\n",
        "    cudaFree(dev_c);\n",
        "\n",
        "    //for (int y = 0; y < N; y++){\n",
        "    //    for (int x = 0; x < N; x++){\n",
        "    //        printf(\"[%d][%d]=%d \", y, x, c[x][y]);\n",
        "    //    }\n",
        "    //    printf(\"\\n\");\n",
        "    //}\n",
        "\n",
        "    float msecPerKernelExecution = msecTotal / nIter;\n",
        "    double flopsPerMMull = 2.0 * N * N * N;\n",
        "    double gigaFlops = (flopsPerMMull * 1.0e-9f) / (msecPerKernelExecution / 1000.0f);\n",
        "\n",
        "    printf(\"GigaFlops: %f\", gigaFlops);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing matrixMul_gpu2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxEbuaBF81Hl",
        "outputId": "aafe6fb9-d990-474c-c1e3-155d57a33f73"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true matrixMul_gpu2.cu -o matrixMul_gpu2 -lcudadevrt\n",
        "!nvprof ./matrixMul_gpu2"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==1907== NVPROF is profiling process 1907, command: ./matrixMul_gpu2\n",
            "==1907== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "GigaFlops: 21.263810==1907== Profiling application: ./matrixMul_gpu2\n",
            "==1907== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.92%  19.566ms      1000  19.565us  19.007us  20.256us  matrixMultGPU(int*, int*, int*)\n",
            "                    0.05%  10.559us         2  5.2790us  5.2150us  5.3440us  [CUDA memcpy HtoD]\n",
            "                    0.02%  4.7040us         1  4.7040us  4.7040us  4.7040us  [CUDA memcpy DtoH]\n",
            "      API calls:   87.85%  184.19ms         2  92.097ms  1.1540us  184.19ms  cudaEventCreate\n",
            "                    9.49%  19.905ms         1  19.905ms  19.905ms  19.905ms  cudaEventSynchronize\n",
            "                    2.10%  4.4107ms      1000  4.4100us  3.5620us  39.313us  cudaLaunchKernel\n",
            "                    0.28%  577.99us         1  577.99us  577.99us  577.99us  cuDeviceTotalMem\n",
            "                    0.08%  166.58us       101  1.6490us     140ns  68.437us  cuDeviceGetAttribute\n",
            "                    0.07%  151.18us         3  50.391us  2.1700us  145.13us  cudaMalloc\n",
            "                    0.07%  140.92us         3  46.974us  2.5910us  130.16us  cudaFree\n",
            "                    0.04%  76.474us         3  25.491us  17.709us  38.890us  cudaMemcpy\n",
            "                    0.01%  23.442us         1  23.442us  23.442us  23.442us  cuDeviceGetName\n",
            "                    0.01%  13.028us         1  13.028us  13.028us  13.028us  cuDeviceGetPCIBusId\n",
            "                    0.00%  7.8180us         2  3.9090us  3.2980us  4.5200us  cudaEventRecord\n",
            "                    0.00%  2.5550us         1  2.5550us  2.5550us  2.5550us  cudaEventElapsedTime\n",
            "                    0.00%  2.2490us         3     749ns     348ns  1.2600us  cuDeviceGetCount\n",
            "                    0.00%  1.4290us         2     714ns     300ns  1.1290us  cuDeviceGet\n",
            "                    0.00%     403ns         1     403ns     403ns     403ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Xb4PI9f9jZA"
      },
      "source": [
        "Comparamos el producto de NxN con el uso de memoria compartida y con optimizaciones:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXBalU6W9iFZ",
        "outputId": "c01b30b3-dd61-4b3c-8850-a0bc9a0e36bd"
      },
      "source": [
        "%%writefile matrixMul_gpu3.cu\n",
        "#include <stdio.h>\n",
        "#define N 64\n",
        "#define MAX 8\n",
        "\n",
        "__global__ void matrixMultGPU(int *a, int *b, int *c, int n){\n",
        "    int k, sum = 0;\n",
        "    int thx = threadIdx.x;\n",
        "    int thy = threadIdx.y;\n",
        "    int col = thx + blockDim.x * blockIdx.x;\n",
        "    int fil = thy + blockDim.y * blockIdx.y;\n",
        "\n",
        "    __shared__ float Aij[MAX][MAX];\n",
        "    __shared__ float Bij[MAX][MAX];\n",
        "\n",
        "    for (int lim = 0; lim < (n + MAX - 1)/MAX; lim++){\n",
        "        if ((thy + (lim*MAX)) < n  && col < n){\n",
        "            Aij[thy][thx] = a[(col*n) + (thy + (lim*MAX))];\n",
        "        }\n",
        "        else {\n",
        "            Aij[thy][thx] = 0.0;\n",
        "        }\n",
        "        if ((thx + (lim*MAX)) < n  && fil < n){\n",
        "            Bij[thy][thx] = b[((thx + (lim*MAX))*n) + fil];\n",
        "        }\n",
        "        else{\n",
        "            Bij[thy][thx] = 0.0;\n",
        "        }\n",
        "        __syncthreads();\n",
        "    #pragma unroll\n",
        "        for (k = 0; k < MAX; k++){\n",
        "            sum += Aij[k][thx] * Bij[thy][k];\n",
        "        }\n",
        "\n",
        "        __syncthreads();\n",
        "    }\n",
        "    if (col < n && fil < n){\n",
        "        c[col * n + fil] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    int a[N][N], b[N][N], c[N][N];\n",
        "    int *dev_a, *dev_b, *dev_c;\n",
        "    int cont, i , j;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    for (i = 0; i < N; i++){\n",
        "        cont = 0;\n",
        "        for (j = 0; j < N; j++){\n",
        "            a[i][j] = cont;\n",
        "            b[i][j] = cont;\n",
        "            cont++;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    int size = N*N*sizeof(int);\n",
        "    cudaMalloc((void **) &dev_a, size);\n",
        "    cudaMalloc((void **) &dev_b, size);\n",
        "    cudaMalloc((void **) &dev_c, size);\n",
        "\n",
        "    cudaMemcpy(dev_a, a, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 dimGrid((N + MAX - 1)/MAX, (N + MAX - 1)/MAX);\n",
        "    dim3 dimBlock(MAX, MAX);\n",
        "\n",
        "    int nIter = 1000;\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    for (int m = 0; m < nIter; m++){\n",
        "        matrixMultGPU<<<dimGrid, dimBlock>>>(dev_a, dev_b, dev_c, N);\n",
        "    }\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    cudaMemcpy(c, dev_c, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    float msecTotal = 0.0;\n",
        "    cudaEventElapsedTime(&msecTotal, start, stop);\n",
        "\n",
        "    cudaFree(dev_a);\n",
        "    cudaFree(dev_b);\n",
        "    cudaFree(dev_c);\n",
        "\n",
        "    //for (int y = 0; y < N; y++){\n",
        "    //    for (int x = 0; x < N; x++){\n",
        "    //        printf(\"[%d][%d]=%d \", y, x, c[x][y]);\n",
        "    //    }\n",
        "    //    printf(\"\\n\");\n",
        "    //}\n",
        "\n",
        "    float msecPerKernelExecution = msecTotal / nIter;\n",
        "    double flopsPerMMull = 2.0 * N * N * N;\n",
        "    double gigaFlops = (flopsPerMMull * 1.0e-9f) / (msecPerKernelExecution / 1000.0f);\n",
        "\n",
        "    printf(\"GigaFlops: %f\", gigaFlops);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing matrixMul_gpu3.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9WNnL2GA1RK",
        "outputId": "672ac4a2-8498-4c24-dc11-467d1288c450"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true matrixMul_gpu3.cu -o matrixMul_gpu3 -lcudadevrt\n",
        "!nvprof ./matrixMul_gpu3"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==1951== NVPROF is profiling process 1951, command: ./matrixMul_gpu3\n",
            "==1951== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "==1951== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
            "GigaFlops: 25.579895==1951== Profiling application: ./matrixMul_gpu3\n",
            "==1951== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.90%  15.670ms      1000  15.670us  15.360us  17.824us  matrixMultGPU(int*, int*, int*, int)\n",
            "                    0.07%  10.560us         2  5.2800us  5.1840us  5.3760us  [CUDA memcpy HtoD]\n",
            "                    0.03%  4.7040us         1  4.7040us  4.7040us  4.7040us  [CUDA memcpy DtoH]\n",
            "      API calls:   89.61%  182.64ms         2  91.322ms  1.3280us  182.64ms  cudaEventCreate\n",
            "                    6.25%  12.734ms         1  12.734ms  12.734ms  12.734ms  cudaEventSynchronize\n",
            "                    3.53%  7.1966ms      1000  7.1960us  3.6000us  86.878us  cudaLaunchKernel\n",
            "                    0.26%  537.83us         1  537.83us  537.83us  537.83us  cuDeviceTotalMem\n",
            "                    0.11%  229.22us         3  76.407us  2.9180us  220.64us  cudaMalloc\n",
            "                    0.09%  178.84us       101  1.7700us     144ns  70.679us  cuDeviceGetAttribute\n",
            "                    0.08%  156.85us         3  52.284us  2.8890us  145.91us  cudaFree\n",
            "                    0.05%  103.65us         3  34.549us  24.226us  52.664us  cudaMemcpy\n",
            "                    0.01%  24.954us         1  24.954us  24.954us  24.954us  cuDeviceGetName\n",
            "                    0.01%  10.472us         2  5.2360us  4.6760us  5.7960us  cudaEventRecord\n",
            "                    0.00%  7.5060us         1  7.5060us  7.5060us  7.5060us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.6790us         1  2.6790us  2.6790us  2.6790us  cudaEventElapsedTime\n",
            "                    0.00%  1.9540us         3     651ns     221ns  1.0910us  cuDeviceGetCount\n",
            "                    0.00%  1.4830us         2     741ns     274ns  1.2090us  cuDeviceGet\n",
            "                    0.00%     381ns         1     381ns     381ns     381ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agd10FrQB2vQ"
      },
      "source": [
        "### Resultados de la ejecución de multiplicación de matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5Kk6Y4wB--U"
      },
      "source": [
        "Sin memoria compartida:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPZSCOh2B2H7",
        "outputId": "64a58c4b-c075-43a5-8b5f-a54fc08a84e6"
      },
      "source": [
        "!for i in 16 32 64 128 512; do sed -i \"/#define N/c\\#define N $i\" matrixMul_gpu2.cu && /usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true matrixMul_gpu2.cu -o matrixMul_gpu2 -lcudadevrt && nvprof ./matrixMul_gpu2; done"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==1997== NVPROF is profiling process 1997, command: ./matrixMul_gpu2\n",
            "==1997== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "GigaFlops: 29.257145==1997== Profiling application: ./matrixMul_gpu2\n",
            "==1997== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.95%  61.601ms      1000  61.601us  59.903us  63.296us  matrixMultGPU(int*, int*, int*)\n",
            "                    0.03%  20.768us         2  10.384us  10.304us  10.464us  [CUDA memcpy HtoD]\n",
            "                    0.01%  7.6800us         1  7.6800us  7.6800us  7.6800us  [CUDA memcpy DtoH]\n",
            "      API calls:   75.47%  207.02ms         2  103.51ms  1.1970us  207.01ms  cudaEventCreate\n",
            "                   21.77%  59.709ms         1  59.709ms  59.709ms  59.709ms  cudaEventSynchronize\n",
            "                    2.24%  6.1417ms      1000  6.1410us  4.6840us  45.859us  cudaLaunchKernel\n",
            "                    0.19%  524.39us         1  524.39us  524.39us  524.39us  cuDeviceTotalMem\n",
            "                    0.09%  239.10us         3  79.698us  6.9490us  210.71us  cudaFree\n",
            "                    0.08%  225.84us         3  75.279us  4.5850us  212.58us  cudaMalloc\n",
            "                    0.08%  218.26us       101  2.1600us     166ns  108.90us  cuDeviceGetAttribute\n",
            "                    0.06%  175.64us         3  58.546us  32.083us  97.069us  cudaMemcpy\n",
            "                    0.01%  26.611us         1  26.611us  26.611us  26.611us  cuDeviceGetName\n",
            "                    0.00%  10.360us         2  5.1800us  3.9880us  6.3720us  cudaEventRecord\n",
            "                    0.00%  7.8480us         1  7.8480us  7.8480us  7.8480us  cuDeviceGetPCIBusId\n",
            "                    0.00%  5.3140us         1  5.3140us  5.3140us  5.3140us  cudaEventElapsedTime\n",
            "                    0.00%  2.7450us         3     915ns     218ns  1.3930us  cuDeviceGetCount\n",
            "                    0.00%  1.9750us         2     987ns     556ns  1.4190us  cuDeviceGet\n",
            "                    0.00%     477ns         1     477ns     477ns     477ns  cuDeviceGetUuid\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==2044== NVPROF is profiling process 2044, command: ./matrixMul_gpu2\n",
            "==2044== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "GigaFlops: 29.257697==2044== Profiling application: ./matrixMul_gpu2\n",
            "==2044== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.95%  61.628ms      1000  61.627us  60.160us  63.615us  matrixMultGPU(int*, int*, int*)\n",
            "                    0.03%  20.704us         2  10.352us  10.272us  10.432us  [CUDA memcpy HtoD]\n",
            "                    0.01%  7.6480us         1  7.6480us  7.6480us  7.6480us  [CUDA memcpy DtoH]\n",
            "      API calls:   73.23%  183.72ms         2  91.860ms  1.2340us  183.72ms  cudaEventCreate\n",
            "                   24.05%  60.333ms         1  60.333ms  60.333ms  60.333ms  cudaEventSynchronize\n",
            "                    2.20%  5.5182ms      1000  5.5180us  4.4690us  45.457us  cudaLaunchKernel\n",
            "                    0.21%  515.72us         1  515.72us  515.72us  515.72us  cuDeviceTotalMem\n",
            "                    0.09%  213.52us       101  2.1140us     141ns  96.578us  cuDeviceGetAttribute\n",
            "                    0.08%  190.59us         3  63.530us  3.8350us  172.27us  cudaFree\n",
            "                    0.08%  190.09us         3  63.361us  3.0840us  180.94us  cudaMalloc\n",
            "                    0.05%  132.66us         3  44.218us  28.673us  72.649us  cudaMemcpy\n",
            "                    0.01%  32.309us         1  32.309us  32.309us  32.309us  cuDeviceGetName\n",
            "                    0.01%  12.772us         2  6.3860us  4.2430us  8.5290us  cudaEventRecord\n",
            "                    0.00%  5.0010us         1  5.0010us  5.0010us  5.0010us  cuDeviceGetPCIBusId\n",
            "                    0.00%  3.2320us         1  3.2320us  3.2320us  3.2320us  cudaEventElapsedTime\n",
            "                    0.00%  2.3030us         3     767ns     200ns  1.0830us  cuDeviceGetCount\n",
            "                    0.00%  1.9160us         2     958ns     470ns  1.4460us  cuDeviceGet\n",
            "                    0.00%     285ns         1     285ns     285ns     285ns  cuDeviceGetUuid\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==2089== NVPROF is profiling process 2089, command: ./matrixMul_gpu2\n",
            "==2089== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "==2089== Profiling application: ./matrixMul_gpu2\n",
            "GigaFlops: 29.195214==2089== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.95%  61.682ms      1000  61.681us  59.967us  63.360us  matrixMultGPU(int*, int*, int*)\n",
            "                    0.03%  20.800us         2  10.400us  10.272us  10.528us  [CUDA memcpy HtoD]\n",
            "                    0.01%  7.6160us         1  7.6160us  7.6160us  7.6160us  [CUDA memcpy DtoH]\n",
            "      API calls:   74.54%  196.82ms         2  98.412ms  1.6500us  196.82ms  cudaEventCreate\n",
            "                   23.18%  61.214ms         1  61.214ms  61.214ms  61.214ms  cudaEventSynchronize\n",
            "                    1.84%  4.8508ms      1000  4.8500us  3.5230us  32.036us  cudaLaunchKernel\n",
            "                    0.16%  430.77us         1  430.77us  430.77us  430.77us  cuDeviceTotalMem\n",
            "                    0.07%  192.49us         3  64.164us  2.5030us  184.62us  cudaMalloc\n",
            "                    0.07%  184.09us         3  61.362us  3.5520us  165.77us  cudaFree\n",
            "                    0.06%  165.78us       101  1.6410us     142ns  66.552us  cuDeviceGetAttribute\n",
            "                    0.05%  136.72us         3  45.572us  27.617us  80.820us  cudaMemcpy\n",
            "                    0.01%  27.986us         1  27.986us  27.986us  27.986us  cuDeviceGetName\n",
            "                    0.00%  7.7040us         2  3.8520us  3.6920us  4.0120us  cudaEventRecord\n",
            "                    0.00%  5.1800us         1  5.1800us  5.1800us  5.1800us  cuDeviceGetPCIBusId\n",
            "                    0.00%  4.0930us         1  4.0930us  4.0930us  4.0930us  cudaEventElapsedTime\n",
            "                    0.00%  2.3060us         3     768ns     256ns  1.1340us  cuDeviceGetCount\n",
            "                    0.00%  1.8150us         2     907ns     482ns  1.3330us  cuDeviceGet\n",
            "                    0.00%     282ns         1     282ns     282ns     282ns  cuDeviceGetUuid\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==2134== NVPROF is profiling process 2134, command: ./matrixMul_gpu2\n",
            "==2134== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "GigaFlops: 29.183851==2134== Profiling application: ./matrixMul_gpu2\n",
            "==2134== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.95%  61.681ms      1000  61.681us  60.032us  63.392us  matrixMultGPU(int*, int*, int*)\n",
            "                    0.03%  20.704us         2  10.352us  10.304us  10.400us  [CUDA memcpy HtoD]\n",
            "                    0.01%  7.7760us         1  7.7760us  7.7760us  7.7760us  [CUDA memcpy DtoH]\n",
            "      API calls:   73.74%  189.64ms         2  94.818ms  1.8720us  189.63ms  cudaEventCreate\n",
            "                   23.75%  61.084ms         1  61.084ms  61.084ms  61.084ms  cudaEventSynchronize\n",
            "                    1.94%  4.9935ms      1000  4.9930us  3.9750us  37.161us  cudaLaunchKernel\n",
            "                    0.24%  623.94us         1  623.94us  623.94us  623.94us  cuDeviceTotalMem\n",
            "                    0.10%  252.45us         3  84.150us  16.052us  218.89us  cudaFree\n",
            "                    0.09%  223.99us       101  2.2170us     141ns  86.730us  cuDeviceGetAttribute\n",
            "                    0.06%  154.51us         3  51.503us  2.4870us  147.16us  cudaMalloc\n",
            "                    0.05%  136.30us         3  45.431us  25.218us  82.933us  cudaMemcpy\n",
            "                    0.01%  25.292us         1  25.292us  25.292us  25.292us  cuDeviceGetName\n",
            "                    0.00%  7.9780us         2  3.9890us  3.7000us  4.2780us  cudaEventRecord\n",
            "                    0.00%  7.3760us         1  7.3760us  7.3760us  7.3760us  cuDeviceGetPCIBusId\n",
            "                    0.00%  4.1340us         1  4.1340us  4.1340us  4.1340us  cudaEventElapsedTime\n",
            "                    0.00%  2.0020us         3     667ns     250ns     905ns  cuDeviceGetCount\n",
            "                    0.00%  1.3770us         2     688ns     261ns  1.1160us  cuDeviceGet\n",
            "                    0.00%     252ns         1     252ns     252ns     252ns  cuDeviceGetUuid\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==2179== NVPROF is profiling process 2179, command: ./matrixMul_gpu2\n",
            "==2179== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "GigaFlops: 29.241306==2179== Profiling application: ./matrixMul_gpu2\n",
            "==2179== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.95%  61.683ms      1000  61.683us  60.159us  63.904us  matrixMultGPU(int*, int*, int*)\n",
            "                    0.03%  20.928us         2  10.464us  10.272us  10.656us  [CUDA memcpy HtoD]\n",
            "                    0.01%  7.6800us         1  7.6800us  7.6800us  7.6800us  [CUDA memcpy DtoH]\n",
            "      API calls:   73.60%  187.63ms         2  93.815ms  1.0000us  187.63ms  cudaEventCreate\n",
            "                   24.21%  61.727ms         1  61.727ms  61.727ms  61.727ms  cudaEventSynchronize\n",
            "                    1.69%  4.3157ms      1000  4.3150us  3.5440us  22.727us  cudaLaunchKernel\n",
            "                    0.18%  460.53us         1  460.53us  460.53us  460.53us  cuDeviceTotalMem\n",
            "                    0.09%  217.24us         3  72.412us  4.3720us  190.51us  cudaFree\n",
            "                    0.08%  209.29us       101  2.0720us     134ns  111.91us  cuDeviceGetAttribute\n",
            "                    0.08%  192.62us         3  64.205us  2.5180us  185.08us  cudaMalloc\n",
            "                    0.05%  134.47us         3  44.823us  26.863us  79.219us  cudaMemcpy\n",
            "                    0.01%  29.446us         1  29.446us  29.446us  29.446us  cuDeviceGetName\n",
            "                    0.00%  6.9590us         2  3.4790us  3.0430us  3.9160us  cudaEventRecord\n",
            "                    0.00%  5.9450us         1  5.9450us  5.9450us  5.9450us  cuDeviceGetPCIBusId\n",
            "                    0.00%  3.8090us         1  3.8090us  3.8090us  3.8090us  cudaEventElapsedTime\n",
            "                    0.00%  2.6820us         3     894ns     185ns  1.3870us  cuDeviceGetCount\n",
            "                    0.00%  1.8890us         2     944ns     477ns  1.4120us  cuDeviceGet\n",
            "                    0.00%     294ns         1     294ns     294ns     294ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w591hNMTCROP"
      },
      "source": [
        "<style type=\"text/css\">\n",
        ".tg  {border-collapse:collapse;border-spacing:0;}\n",
        ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
        "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
        ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
        "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
        ".tg .tg-n2k3{background-color:#FFF;color:#000000;font-family:\"Palatino Linotype\", \"Book Antiqua\", Palatino, serif !important;;\n",
        "  text-align:center;vertical-align:top}\n",
        ".tg .tg-6kwm{background-color:#FFF;border-color:inherit;color:#000000;\n",
        "  font-family:\"Palatino Linotype\", \"Book Antiqua\", Palatino, serif !important;;text-align:center;vertical-align:top}\n",
        ".tg .tg-bifh{background-color:#FFF;color:#000000;font-family:\"Palatino Linotype\", \"Book Antiqua\", Palatino, serif !important;;\n",
        "  text-align:center;vertical-align:middle}\n",
        ".tg .tg-h01i{background-color:#FFF;border-color:inherit;color:#000000;\n",
        "  font-family:\"Palatino Linotype\", \"Book Antiqua\", Palatino, serif !important;;text-align:center;vertical-align:middle}\n",
        "</style>\n",
        "<table class=\"tg\">\n",
        "<thead>\n",
        "  <tr>\n",
        "    <th class=\"tg-6kwm\">Sin usar memoria compartida</th>\n",
        "    <th class=\"tg-6kwm\" colspan=\"3\">Tiempo de Ejecución (msec)</th>\n",
        "    <th class=\"tg-h01i\"></th>\n",
        "    <th class=\"tg-bifh\"></th>\n",
        "  </tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "  <tr>\n",
        "    <td class=\"tg-6kwm\">Tamaño de la matriz</td>\n",
        "    <td class=\"tg-6kwm\">CPU- &gt;GPU</td>\n",
        "    <td class=\"tg-6kwm\">GPU- &gt;CPU</td>\n",
        "    <td class=\"tg-6kwm\">Ejecución kernel</td>\n",
        "    <td class=\"tg-6kwm\">Ratio comparado con 128x128</td>\n",
        "    <td class=\"tg-n2k3\">GFLOPs</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-6kwm\">16x16</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-6kwm\">32x32</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-n2k3\">64x64</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-n2k3\">128x128</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-n2k3\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-n2k3\">512x512</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "  </tr>\n",
        "</tbody>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaB6NAsHCDhd"
      },
      "source": [
        "Con memoria compartida:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6ce9CdTBZAL",
        "outputId": "218901c4-97fd-45eb-9eec-9d567faf7a8c"
      },
      "source": [
        "!for i in 16 32 64 128 512; do sed -i \"/#define N/c\\#define N $i\" matrixMul_gpu3.cu && /usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true matrixMul_gpu3.cu -o matrixMul_gpu3 -lcudadevrt && nvprof ./matrixMul_gpu3; done"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==2225== NVPROF is profiling process 2225, command: ./matrixMul_gpu3\n",
            "==2225== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "==2225== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
            "GigaFlops: 38.511384==2225== Profiling application: ./matrixMul_gpu3\n",
            "==2225== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.94%  45.619ms      1000  45.618us  43.776us  47.040us  matrixMultGPU(int*, int*, int*, int)\n",
            "                    0.05%  20.768us         2  10.384us  10.304us  10.464us  [CUDA memcpy HtoD]\n",
            "                    0.02%  7.6480us         1  7.6480us  7.6480us  7.6480us  [CUDA memcpy DtoH]\n",
            "      API calls:   79.39%  197.76ms         2  98.881ms  1.1290us  197.76ms  cudaEventCreate\n",
            "                   18.06%  44.995ms         1  44.995ms  44.995ms  44.995ms  cudaEventSynchronize\n",
            "                    2.03%  5.0459ms      1000  5.0450us  4.0810us  57.679us  cudaLaunchKernel\n",
            "                    0.19%  476.60us         1  476.60us  476.60us  476.60us  cuDeviceTotalMem\n",
            "                    0.09%  234.11us       101  2.3170us     160ns  127.11us  cuDeviceGetAttribute\n",
            "                    0.09%  211.94us         3  70.645us  3.7840us  185.63us  cudaFree\n",
            "                    0.07%  178.32us         3  59.439us  2.5800us  170.11us  cudaMalloc\n",
            "                    0.05%  127.18us         3  42.394us  25.356us  72.987us  cudaMemcpy\n",
            "                    0.02%  41.644us         1  41.644us  41.644us  41.644us  cuDeviceGetName\n",
            "                    0.01%  17.439us         3  5.8130us     253ns  16.216us  cuDeviceGetCount\n",
            "                    0.00%  11.759us         2  5.8790us  3.8490us  7.9100us  cudaEventRecord\n",
            "                    0.00%  5.6040us         1  5.6040us  5.6040us  5.6040us  cuDeviceGetPCIBusId\n",
            "                    0.00%  3.1980us         1  3.1980us  3.1980us  3.1980us  cudaEventElapsedTime\n",
            "                    0.00%  1.9740us         2     987ns     308ns  1.6660us  cuDeviceGet\n",
            "                    0.00%     296ns         1     296ns     296ns     296ns  cuDeviceGetUuid\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==2270== NVPROF is profiling process 2270, command: ./matrixMul_gpu3\n",
            "==2270== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "==2270== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
            "GigaFlops: 38.459584==2270== Profiling application: ./matrixMul_gpu3\n",
            "==2270== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.94%  45.596ms      1000  45.596us  43.968us  47.008us  matrixMultGPU(int*, int*, int*, int)\n",
            "                    0.05%  20.704us         2  10.352us  10.272us  10.432us  [CUDA memcpy HtoD]\n",
            "                    0.02%  7.6480us         1  7.6480us  7.6480us  7.6480us  [CUDA memcpy DtoH]\n",
            "      API calls:   79.99%  205.62ms         2  102.81ms  1.2580us  205.62ms  cudaEventCreate\n",
            "                   17.51%  45.005ms         1  45.005ms  45.005ms  45.005ms  cudaEventSynchronize\n",
            "                    1.98%  5.0970ms      1000  5.0960us  4.2100us  65.620us  cudaLaunchKernel\n",
            "                    0.20%  513.22us         1  513.22us  513.22us  513.22us  cuDeviceTotalMem\n",
            "                    0.09%  219.43us       101  2.1720us     160ns  93.508us  cuDeviceGetAttribute\n",
            "                    0.08%  216.28us         3  72.091us  3.8500us  193.68us  cudaFree\n",
            "                    0.07%  192.64us         3  64.214us  2.7670us  184.28us  cudaMalloc\n",
            "                    0.06%  150.89us         3  50.298us  28.509us  93.756us  cudaMemcpy\n",
            "                    0.01%  32.094us         1  32.094us  32.094us  32.094us  cuDeviceGetName\n",
            "                    0.00%  7.9610us         2  3.9800us  3.6920us  4.2690us  cudaEventRecord\n",
            "                    0.00%  5.8560us         1  5.8560us  5.8560us  5.8560us  cuDeviceGetPCIBusId\n",
            "                    0.00%  3.5540us         1  3.5540us  3.5540us  3.5540us  cudaEventElapsedTime\n",
            "                    0.00%  2.9520us         3     984ns     253ns  1.6470us  cuDeviceGetCount\n",
            "                    0.00%  1.7780us         2     889ns     524ns  1.2540us  cuDeviceGet\n",
            "                    0.00%     350ns         1     350ns     350ns     350ns  cuDeviceGetUuid\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==2315== NVPROF is profiling process 2315, command: ./matrixMul_gpu3\n",
            "==2315== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "==2315== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
            "GigaFlops: 38.422767==2315== Profiling application: ./matrixMul_gpu3\n",
            "==2315== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.94%  45.590ms      1000  45.590us  43.903us  46.752us  matrixMultGPU(int*, int*, int*, int)\n",
            "                    0.05%  20.704us         2  10.352us  10.272us  10.432us  [CUDA memcpy HtoD]\n",
            "                    0.02%  7.6160us         1  7.6160us  7.6160us  7.6160us  [CUDA memcpy DtoH]\n",
            "      API calls:   78.46%  187.10ms         2  93.552ms  1.1500us  187.10ms  cudaEventCreate\n",
            "                   19.00%  45.307ms         1  45.307ms  45.307ms  45.307ms  cudaEventSynchronize\n",
            "                    2.02%  4.8136ms      1000  4.8130us  3.6840us  56.224us  cudaLaunchKernel\n",
            "                    0.19%  443.06us         1  443.06us  443.06us  443.06us  cuDeviceTotalMem\n",
            "                    0.11%  257.73us         3  85.908us  6.0070us  233.89us  cudaFree\n",
            "                    0.08%  190.09us         3  63.364us  4.0870us  178.35us  cudaMalloc\n",
            "                    0.07%  167.23us       101  1.6550us     139ns  67.240us  cuDeviceGetAttribute\n",
            "                    0.06%  137.10us         3  45.699us  26.651us  80.933us  cudaMemcpy\n",
            "                    0.01%  28.464us         1  28.464us  28.464us  28.464us  cuDeviceGetName\n",
            "                    0.00%  8.5280us         2  4.2640us  4.0820us  4.4460us  cudaEventRecord\n",
            "                    0.00%  6.6780us         1  6.6780us  6.6780us  6.6780us  cuDeviceGetPCIBusId\n",
            "                    0.00%  4.9440us         1  4.9440us  4.9440us  4.9440us  cudaEventElapsedTime\n",
            "                    0.00%  2.2870us         3     762ns     183ns  1.0560us  cuDeviceGetCount\n",
            "                    0.00%  1.5390us         2     769ns     451ns  1.0880us  cuDeviceGet\n",
            "                    0.00%     289ns         1     289ns     289ns     289ns  cuDeviceGetUuid\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==2362== NVPROF is profiling process 2362, command: ./matrixMul_gpu3\n",
            "==2362== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "==2362== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
            "GigaFlops: 38.375475==2362== Profiling application: ./matrixMul_gpu3\n",
            "==2362== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.94%  45.631ms      1000  45.631us  43.840us  46.848us  matrixMultGPU(int*, int*, int*, int)\n",
            "                    0.05%  20.896us         2  10.448us  10.432us  10.464us  [CUDA memcpy HtoD]\n",
            "                    0.02%  7.6480us         1  7.6480us  7.6480us  7.6480us  [CUDA memcpy DtoH]\n",
            "      API calls:   78.83%  191.46ms         2  95.729ms  1.0410us  191.46ms  cudaEventCreate\n",
            "                   18.69%  45.388ms         1  45.388ms  45.388ms  45.388ms  cudaEventSynchronize\n",
            "                    1.97%  4.7901ms      1000  4.7900us  3.7880us  53.352us  cudaLaunchKernel\n",
            "                    0.17%  409.90us         1  409.90us  409.90us  409.90us  cuDeviceTotalMem\n",
            "                    0.11%  267.66us         3  89.219us  6.2910us  239.24us  cudaFree\n",
            "                    0.07%  179.68us       101  1.7780us     137ns  76.921us  cuDeviceGetAttribute\n",
            "                    0.07%  166.32us         3  55.440us  2.5790us  158.44us  cudaMalloc\n",
            "                    0.07%  162.68us         3  54.225us  24.080us  99.871us  cudaMemcpy\n",
            "                    0.01%  27.639us         1  27.639us  27.639us  27.639us  cuDeviceGetName\n",
            "                    0.00%  7.4380us         2  3.7190us  3.5400us  3.8980us  cudaEventRecord\n",
            "                    0.00%  6.5540us         1  6.5540us  6.5540us  6.5540us  cuDeviceGetPCIBusId\n",
            "                    0.00%  5.2280us         1  5.2280us  5.2280us  5.2280us  cudaEventElapsedTime\n",
            "                    0.00%  1.9280us         3     642ns     168ns  1.0160us  cuDeviceGetCount\n",
            "                    0.00%  1.7000us         2     850ns     277ns  1.4230us  cuDeviceGet\n",
            "                    0.00%     284ns         1     284ns     284ns     284ns  cuDeviceGetUuid\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==2407== NVPROF is profiling process 2407, command: ./matrixMul_gpu3\n",
            "==2407== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "==2407== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
            "GigaFlops: 38.422739==2407== Profiling application: ./matrixMul_gpu3\n",
            "==2407== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.94%  45.633ms      1000  45.632us  43.680us  46.848us  matrixMultGPU(int*, int*, int*, int)\n",
            "                    0.05%  20.992us         2  10.496us  10.464us  10.528us  [CUDA memcpy HtoD]\n",
            "                    0.02%  7.6480us         1  7.6480us  7.6480us  7.6480us  [CUDA memcpy DtoH]\n",
            "      API calls:   79.59%  200.63ms         2  100.31ms  1.1400us  200.63ms  cudaEventCreate\n",
            "                   17.98%  45.331ms         1  45.331ms  45.331ms  45.331ms  cudaEventSynchronize\n",
            "                    1.89%  4.7632ms      1000  4.7630us  3.9390us  54.776us  cudaLaunchKernel\n",
            "                    0.20%  498.70us         1  498.70us  498.70us  498.70us  cuDeviceTotalMem\n",
            "                    0.09%  223.37us       101  2.2110us     161ns  84.838us  cuDeviceGetAttribute\n",
            "                    0.09%  221.60us         3  73.867us  4.3210us  198.09us  cudaFree\n",
            "                    0.07%  179.86us         3  59.953us  2.7350us  171.64us  cudaMalloc\n",
            "                    0.07%  177.31us         3  59.104us  27.678us  121.78us  cudaMemcpy\n",
            "                    0.01%  26.127us         1  26.127us  26.127us  26.127us  cuDeviceGetName\n",
            "                    0.00%  10.934us         2  5.4670us  3.5220us  7.4120us  cudaEventRecord\n",
            "                    0.00%  6.0230us         1  6.0230us  6.0230us  6.0230us  cudaEventElapsedTime\n",
            "                    0.00%  5.4900us         1  5.4900us  5.4900us  5.4900us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.9750us         3     658ns     200ns  1.0750us  cuDeviceGetCount\n",
            "                    0.00%  1.3840us         2     692ns     262ns  1.1220us  cuDeviceGet\n",
            "                    0.00%     329ns         1     329ns     329ns     329ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApWeMA7SGZDN"
      },
      "source": [
        "<style type=\"text/css\">\n",
        ".tg  {border-collapse:collapse;border-spacing:0;}\n",
        ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
        "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
        ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
        "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
        ".tg .tg-n2k3{background-color:#FFF;color:#000000;font-family:\"Palatino Linotype\", \"Book Antiqua\", Palatino, serif !important;;\n",
        "  text-align:center;vertical-align:top}\n",
        ".tg .tg-6kwm{background-color:#FFF;border-color:inherit;color:#000000;\n",
        "  font-family:\"Palatino Linotype\", \"Book Antiqua\", Palatino, serif !important;;text-align:center;vertical-align:top}\n",
        ".tg .tg-bifh{background-color:#FFF;color:#000000;font-family:\"Palatino Linotype\", \"Book Antiqua\", Palatino, serif !important;;\n",
        "  text-align:center;vertical-align:middle}\n",
        ".tg .tg-h01i{background-color:#FFF;border-color:inherit;color:#000000;\n",
        "  font-family:\"Palatino Linotype\", \"Book Antiqua\", Palatino, serif !important;;text-align:center;vertical-align:middle}\n",
        "</style>\n",
        "<table class=\"tg\">\n",
        "<thead>\n",
        "  <tr>\n",
        "    <th class=\"tg-6kwm\">Con memoria compartida</th>\n",
        "    <th class=\"tg-6kwm\" colspan=\"3\">Tiempo de Ejecución (msec)</th>\n",
        "    <th class=\"tg-h01i\"></th>\n",
        "    <th class=\"tg-bifh\"></th>\n",
        "  </tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "  <tr>\n",
        "    <td class=\"tg-6kwm\">Tamaño de la matriz</td>\n",
        "    <td class=\"tg-6kwm\">CPU- &gt;GPU</td>\n",
        "    <td class=\"tg-6kwm\">GPU- &gt;CPU</td>\n",
        "    <td class=\"tg-6kwm\">Ejecución kernel</td>\n",
        "    <td class=\"tg-6kwm\">Ratio comparado con 128x128</td>\n",
        "    <td class=\"tg-n2k3\">GFLOPs</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-6kwm\">16x16</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-6kwm\">32x32</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-n2k3\">64x64</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-n2k3\">128x128</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-n2k3\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-n2k3\">512x512</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "  </tr>\n",
        "</tbody>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv0tV9ofCG62"
      },
      "source": [
        "## Revisión de los ejemplos instalados con CUDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mq9bEvZ6JUpq",
        "outputId": "9f08cf0e-7fef-456e-ae8f-01acfcd72833"
      },
      "source": [
        "%cd /usr/local/cuda/samples\n",
        "%ls"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/cuda-11.1/samples\n",
            "\u001b[0m\u001b[01;34m0_Simple\u001b[0m/     \u001b[01;34m2_Graphics\u001b[0m/  \u001b[01;34m4_Finance\u001b[0m/      \u001b[01;34m6_Advanced\u001b[0m/       \u001b[01;34mbin\u001b[0m/     EULA.txt\n",
            "\u001b[01;34m1_Utilities\u001b[0m/  \u001b[01;34m3_Imaging\u001b[0m/   \u001b[01;34m5_Simulations\u001b[0m/  \u001b[01;34m7_CUDALibraries\u001b[0m/  \u001b[01;34mcommon\u001b[0m/  Makefile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tR1tfxYzJeVc",
        "outputId": "33a8484c-240d-4223-df8b-09cdece15880"
      },
      "source": [
        "%cd  0_Simple/matrixMul/\n",
        "%ls\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/cuda-11.1/samples/0_Simple/matrixMul\n",
            "Makefile  matrixMul.cu  NsightEclipse.xml  readme.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "m485XaWJJ43M",
        "outputId": "ae112cb5-7e1a-4d3f-be5b-61b12568c35a"
      },
      "source": [
        "%pwd\n",
        "\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/usr/local/cuda-11.1/samples/0_Simple/matrixMul'"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2AAxHSoKX_K",
        "outputId": "08131680-e388-44ce-ebec-05c92491622d"
      },
      "source": [
        "!cat matrixMul.cu\n",
        "\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/**\n",
            " * Copyright 1993-2015 NVIDIA Corporation.  All rights reserved.\n",
            " *\n",
            " * Please refer to the NVIDIA end user license agreement (EULA) associated\n",
            " * with this source code for terms and conditions that govern your use of\n",
            " * this software. Any use, reproduction, disclosure, or distribution of\n",
            " * this software and related documentation outside the terms of the EULA\n",
            " * is strictly prohibited.\n",
            " *\n",
            " */\n",
            "\n",
            "/**\n",
            " * Matrix multiplication: C = A * B.\n",
            " * Host code.\n",
            " *\n",
            " * This sample implements matrix multiplication which makes use of shared memory\n",
            " * to ensure data reuse, the matrix multiplication is done using tiling approach.\n",
            " * It has been written for clarity of exposition to illustrate various CUDA programming\n",
            " * principles, not with the goal of providing the most performant generic kernel for matrix multiplication.\n",
            " * See also:\n",
            " * V. Volkov and J. Demmel, \"Benchmarking GPUs to tune dense linear algebra,\"\n",
            " * in Proc. 2008 ACM/IEEE Conf. on Supercomputing (SC '08),\n",
            " * Piscataway, NJ: IEEE Press, 2008, pp. Art. 31:1-11.\n",
            " */\n",
            "\n",
            "// System includes\n",
            "#include <stdio.h>\n",
            "#include <assert.h>\n",
            "\n",
            "// CUDA runtime\n",
            "#include <cuda_runtime.h>\n",
            "\n",
            "// Helper functions and utilities to work with CUDA\n",
            "#include <helper_functions.h>\n",
            "#include <helper_cuda.h>\n",
            "\n",
            "/**\n",
            " * Matrix multiplication (CUDA Kernel) on the device: C = A * B\n",
            " * wA is A's width and wB is B's width\n",
            " */\n",
            "template <int BLOCK_SIZE> __global__ void MatrixMulCUDA(float *C, float *A,\n",
            "                                                        float *B, int wA,\n",
            "                                                        int wB) {\n",
            "    // Block index\n",
            "    int bx = blockIdx.x;\n",
            "    int by = blockIdx.y;\n",
            "\n",
            "    // Thread index\n",
            "    int tx = threadIdx.x;\n",
            "    int ty = threadIdx.y;\n",
            "\n",
            "    // Index of the first sub-matrix of A processed by the block\n",
            "    int aBegin = wA * BLOCK_SIZE * by;\n",
            "\n",
            "    // Index of the last sub-matrix of A processed by the block\n",
            "    int aEnd   = aBegin + wA - 1;\n",
            "\n",
            "    // Step size used to iterate through the sub-matrices of A\n",
            "    int aStep  = BLOCK_SIZE;\n",
            "\n",
            "    // Index of the first sub-matrix of B processed by the block\n",
            "    int bBegin = BLOCK_SIZE * bx;\n",
            "\n",
            "    // Step size used to iterate through the sub-matrices of B\n",
            "    int bStep  = BLOCK_SIZE * wB;\n",
            "\n",
            "    // Csub is used to store the element of the block sub-matrix\n",
            "    // that is computed by the thread\n",
            "    float Csub = 0;\n",
            "\n",
            "    // Loop over all the sub-matrices of A and B\n",
            "    // required to compute the block sub-matrix\n",
            "    for (int a = aBegin, b = bBegin;\n",
            "            a <= aEnd;\n",
            "            a += aStep, b += bStep) {\n",
            "        // Declaration of the shared memory array As used to\n",
            "        // store the sub-matrix of A\n",
            "        __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];\n",
            "\n",
            "        // Declaration of the shared memory array Bs used to\n",
            "        // store the sub-matrix of B\n",
            "        __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n",
            "\n",
            "        // Load the matrices from device memory\n",
            "        // to shared memory; each thread loads\n",
            "        // one element of each matrix\n",
            "        As[ty][tx] = A[a + wA * ty + tx];\n",
            "        Bs[ty][tx] = B[b + wB * ty + tx];\n",
            "\n",
            "        // Synchronize to make sure the matrices are loaded\n",
            "        __syncthreads();\n",
            "\n",
            "        // Multiply the two matrices together;\n",
            "        // each thread computes one element\n",
            "        // of the block sub-matrix\n",
            "#pragma unroll\n",
            "\n",
            "        for (int k = 0; k < BLOCK_SIZE; ++k) {\n",
            "            Csub += As[ty][k] * Bs[k][tx];\n",
            "        }\n",
            "\n",
            "        // Synchronize to make sure that the preceding\n",
            "        // computation is done before loading two new\n",
            "        // sub-matrices of A and B in the next iteration\n",
            "        __syncthreads();\n",
            "    }\n",
            "\n",
            "    // Write the block sub-matrix to device memory;\n",
            "    // each thread writes one element\n",
            "    int c = wB * BLOCK_SIZE * by + BLOCK_SIZE * bx;\n",
            "    C[c + wB * ty + tx] = Csub;\n",
            "}\n",
            "\n",
            "void ConstantInit(float *data, int size, float val) {\n",
            "    for (int i = 0; i < size; ++i) {\n",
            "        data[i] = val;\n",
            "    }\n",
            "}\n",
            "\n",
            "/**\n",
            " * Run a simple test of matrix multiplication using CUDA\n",
            " */\n",
            "int MatrixMultiply(int argc, char **argv,\n",
            "                   int block_size, const dim3 &dimsA,\n",
            "                   const dim3 &dimsB) {\n",
            "    // Allocate host memory for matrices A and B\n",
            "    unsigned int size_A = dimsA.x * dimsA.y;\n",
            "    unsigned int mem_size_A = sizeof(float) * size_A;\n",
            "    float *h_A;\n",
            "    checkCudaErrors(cudaMallocHost(&h_A, mem_size_A));\n",
            "    unsigned int size_B = dimsB.x * dimsB.y;\n",
            "    unsigned int mem_size_B = sizeof(float) * size_B;\n",
            "    float *h_B;\n",
            "    checkCudaErrors(cudaMallocHost(&h_B, mem_size_B));\n",
            "    cudaStream_t stream;\n",
            "\n",
            "    // Initialize host memory\n",
            "    const float valB = 0.01f;\n",
            "    ConstantInit(h_A, size_A, 1.0f);\n",
            "    ConstantInit(h_B, size_B, valB);\n",
            "\n",
            "    // Allocate device memory\n",
            "    float *d_A, *d_B, *d_C;\n",
            "\n",
            "    // Allocate host matrix C\n",
            "    dim3 dimsC(dimsB.x, dimsA.y, 1);\n",
            "    unsigned int mem_size_C = dimsC.x * dimsC.y * sizeof(float);\n",
            "    float *h_C;\n",
            "    checkCudaErrors(cudaMallocHost(&h_C, mem_size_C));\n",
            "\n",
            "    if (h_C == NULL) {\n",
            "        fprintf(stderr, \"Failed to allocate host matrix C!\\n\");\n",
            "        exit(EXIT_FAILURE);\n",
            "    }\n",
            "\n",
            "    checkCudaErrors(cudaMalloc(reinterpret_cast<void **>(&d_A), mem_size_A));\n",
            "    checkCudaErrors(cudaMalloc(reinterpret_cast<void **>(&d_B), mem_size_B));\n",
            "    checkCudaErrors(cudaMalloc(reinterpret_cast<void **>(&d_C), mem_size_C));\n",
            "    // Allocate CUDA events that we'll use for timing\n",
            "    cudaEvent_t start, stop;\n",
            "    checkCudaErrors(cudaEventCreate(&start));\n",
            "    checkCudaErrors(cudaEventCreate(&stop));\n",
            "\n",
            "    checkCudaErrors(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));\n",
            "\n",
            "    // copy host memory to device\n",
            "    checkCudaErrors(cudaMemcpyAsync(d_A, h_A, mem_size_A, cudaMemcpyHostToDevice, stream));\n",
            "    checkCudaErrors(cudaMemcpyAsync(d_B, h_B, mem_size_B, cudaMemcpyHostToDevice, stream));\n",
            "\n",
            "    // Setup execution parameters\n",
            "    dim3 threads(block_size, block_size);\n",
            "    dim3 grid(dimsB.x / threads.x, dimsA.y / threads.y);\n",
            "\n",
            "    // Create and start timer\n",
            "    printf(\"Computing result using CUDA Kernel...\\n\");\n",
            "\n",
            "    // Performs warmup operation using matrixMul CUDA kernel\n",
            "    if (block_size == 16) {\n",
            "        MatrixMulCUDA<16> <<< grid, threads, 0, stream>>>(d_C, d_A, d_B,\n",
            "                                                dimsA.x, dimsB.x);\n",
            "    } else {\n",
            "        MatrixMulCUDA<32> <<< grid, threads, 0, stream>>>(d_C, d_A, d_B,\n",
            "                                                dimsA.x, dimsB.x);\n",
            "    }\n",
            "\n",
            "    printf(\"done\\n\");\n",
            "    checkCudaErrors(cudaStreamSynchronize(stream));\n",
            "\n",
            "    // Record the start event\n",
            "    checkCudaErrors(cudaEventRecord(start, stream));\n",
            "\n",
            "    // Execute the kernel\n",
            "    int nIter = 300;\n",
            "\n",
            "    for (int j = 0; j < nIter; j++) {\n",
            "        if (block_size == 16) {\n",
            "            MatrixMulCUDA<16> <<<grid, threads, 0, stream>>>(d_C, d_A, d_B,\n",
            "                                                    dimsA.x, dimsB.x);\n",
            "        } else {\n",
            "            MatrixMulCUDA<32> <<<grid, threads, 0, stream>>>(d_C, d_A, d_B,\n",
            "                                                    dimsA.x, dimsB.x);\n",
            "        }\n",
            "    }\n",
            "\n",
            "    // Record the stop event\n",
            "    checkCudaErrors(cudaEventRecord(stop, stream));\n",
            "\n",
            "    // Wait for the stop event to complete\n",
            "    checkCudaErrors(cudaEventSynchronize(stop));\n",
            "\n",
            "    float msecTotal = 0.0f;\n",
            "    checkCudaErrors(cudaEventElapsedTime(&msecTotal, start, stop));\n",
            "\n",
            "    // Compute and print the performance\n",
            "    float msecPerMatrixMul = msecTotal / nIter;\n",
            "    double flopsPerMatrixMul = 2.0 * static_cast<double>(dimsA.x) *\n",
            "                               static_cast<double>(dimsA.y) *\n",
            "                               static_cast<double>(dimsB.x);\n",
            "    double gigaFlops = (flopsPerMatrixMul * 1.0e-9f) /\n",
            "                       (msecPerMatrixMul / 1000.0f);\n",
            "    printf(\n",
            "        \"Performance= %.2f GFlop/s, Time= %.3f msec, Size= %.0f Ops,\" \\\n",
            "        \" WorkgroupSize= %u threads/block\\n\",\n",
            "        gigaFlops,\n",
            "        msecPerMatrixMul,\n",
            "        flopsPerMatrixMul,\n",
            "        threads.x * threads.y);\n",
            "\n",
            "    // Copy result from device to host\n",
            "    checkCudaErrors(cudaMemcpyAsync(h_C, d_C, mem_size_C, cudaMemcpyDeviceToHost, stream));\n",
            "    checkCudaErrors(cudaStreamSynchronize(stream));\n",
            "\n",
            "    printf(\"Checking computed result for correctness: \");\n",
            "    bool correct = true;\n",
            "\n",
            "    // test relative error by the formula\n",
            "    //     |<x, y>_cpu - <x,y>_gpu|/<|x|, |y|>  < eps\n",
            "    double eps = 1.e-6;  // machine zero\n",
            "\n",
            "    for (int i = 0; i < static_cast<int>(dimsC.x * dimsC.y); i++) {\n",
            "        double abs_err = fabs(h_C[i] - (dimsA.x * valB));\n",
            "        double dot_length = dimsA.x;\n",
            "        double abs_val = fabs(h_C[i]);\n",
            "        double rel_err = abs_err / abs_val / dot_length;\n",
            "\n",
            "        if (rel_err > eps) {\n",
            "            printf(\"Error! Matrix[%05d]=%.8f, ref=%.8f error term is > %E\\n\",\n",
            "                   i, h_C[i], dimsA.x * valB, eps);\n",
            "            correct = false;\n",
            "        }\n",
            "    }\n",
            "\n",
            "    printf(\"%s\\n\", correct ? \"Result = PASS\" : \"Result = FAIL\");\n",
            "\n",
            "    // Clean up memory\n",
            "    checkCudaErrors(cudaFreeHost(h_A));\n",
            "    checkCudaErrors(cudaFreeHost(h_B));\n",
            "    checkCudaErrors(cudaFreeHost(h_C));\n",
            "    checkCudaErrors(cudaFree(d_A));\n",
            "    checkCudaErrors(cudaFree(d_B));\n",
            "    checkCudaErrors(cudaFree(d_C));\n",
            "    checkCudaErrors(cudaEventDestroy(start));\n",
            "    checkCudaErrors(cudaEventDestroy(stop));\n",
            "    printf(\"\\nNOTE: The CUDA Samples are not meant for performance\"\\\n",
            "           \"measurements. Results may vary when GPU Boost is enabled.\\n\");\n",
            "\n",
            "    if (correct) {\n",
            "        return EXIT_SUCCESS;\n",
            "    } else {\n",
            "        return EXIT_FAILURE;\n",
            "    }\n",
            "}\n",
            "\n",
            "\n",
            "/**\n",
            " * Program main\n",
            " */\n",
            "int main(int argc, char **argv) {\n",
            "    printf(\"[Matrix Multiply Using CUDA] - Starting...\\n\");\n",
            "\n",
            "    if (checkCmdLineFlag(argc, (const char **)argv, \"help\") ||\n",
            "            checkCmdLineFlag(argc, (const char **)argv, \"?\")) {\n",
            "        printf(\"Usage -device=n (n >= 0 for deviceID)\\n\");\n",
            "        printf(\"      -wA=WidthA -hA=HeightA (Width x Height of Matrix A)\\n\");\n",
            "        printf(\"      -wB=WidthB -hB=HeightB (Width x Height of Matrix B)\\n\");\n",
            "        printf(\"  Note: Outer matrix dimensions of A & B matrices\" \\\n",
            "               \" must be equal.\\n\");\n",
            "\n",
            "        exit(EXIT_SUCCESS);\n",
            "    }\n",
            "\n",
            "    // This will pick the best possible CUDA capable device, otherwise\n",
            "    // override the device ID based on input provided at the command line\n",
            "    int dev = findCudaDevice(argc, (const char **)argv);\n",
            "\n",
            "    int block_size = 32;\n",
            "\n",
            "    dim3 dimsA(5 * 2 * block_size, 5 * 2 * block_size, 1);\n",
            "    dim3 dimsB(5 * 4 * block_size, 5 * 2 * block_size, 1);\n",
            "\n",
            "    // width of Matrix A\n",
            "    if (checkCmdLineFlag(argc, (const char **)argv, \"wA\")) {\n",
            "        dimsA.x = getCmdLineArgumentInt(argc, (const char **)argv, \"wA\");\n",
            "    }\n",
            "\n",
            "    // height of Matrix A\n",
            "    if (checkCmdLineFlag(argc, (const char **)argv, \"hA\")) {\n",
            "        dimsA.y = getCmdLineArgumentInt(argc, (const char **)argv, \"hA\");\n",
            "    }\n",
            "\n",
            "    // width of Matrix B\n",
            "    if (checkCmdLineFlag(argc, (const char **)argv, \"wB\")) {\n",
            "        dimsB.x = getCmdLineArgumentInt(argc, (const char **)argv, \"wB\");\n",
            "    }\n",
            "\n",
            "    // height of Matrix B\n",
            "    if (checkCmdLineFlag(argc, (const char **)argv, \"hB\")) {\n",
            "        dimsB.y = getCmdLineArgumentInt(argc, (const char **)argv, \"hB\");\n",
            "    }\n",
            "\n",
            "    if (dimsA.x != dimsB.y) {\n",
            "        printf(\"Error: outer matrix dimensions must be equal. (%d != %d)\\n\",\n",
            "               dimsA.x, dimsB.y);\n",
            "        exit(EXIT_FAILURE);\n",
            "    }\n",
            "\n",
            "    printf(\"MatrixA(%d,%d), MatrixB(%d,%d)\\n\", dimsA.x, dimsA.y,\n",
            "                                               dimsB.x, dimsB.y);\n",
            "\n",
            "    int matrix_result = MatrixMultiply(argc, argv, block_size, dimsA, dimsB);\n",
            "\n",
            "    exit(matrix_result);\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFPG0ssAl_lY"
      },
      "source": [
        ""
      ],
      "execution_count": 65,
      "outputs": []
    }
  ]
}