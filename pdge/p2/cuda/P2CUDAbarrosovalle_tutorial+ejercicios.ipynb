{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "P2CUDAbarrosovalle_tutorial+ejercicios.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H83eQ91UUo4c"
      },
      "source": [
        "# Práctica 2: Programación GPU y Computación Cuántica\n",
        "## Tutorial de CUDA\n",
        "\n",
        "María Barroso Honrubia\n",
        "\n",
        "Gloria del Valle Cano"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbHv-FMrvZI8"
      },
      "source": [
        "## Parte 1: Programación GPU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y44sGKPivjB6"
      },
      "source": [
        "### Introducción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP1qHupkVOHt"
      },
      "source": [
        "Primero comprobamos las características que nos ofrece Google Colab con $\\texttt{lscpu}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPsAm64NUPjz",
        "outputId": "b91d3d61-2976-4388-b372-413cf33fcf36"
      },
      "source": [
        "!lscpu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Architecture:        x86_64\n",
            "CPU op-mode(s):      32-bit, 64-bit\n",
            "Byte Order:          Little Endian\n",
            "CPU(s):              2\n",
            "On-line CPU(s) list: 0,1\n",
            "Thread(s) per core:  2\n",
            "Core(s) per socket:  1\n",
            "Socket(s):           1\n",
            "NUMA node(s):        1\n",
            "Vendor ID:           GenuineIntel\n",
            "CPU family:          6\n",
            "Model:               63\n",
            "Model name:          Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "Stepping:            0\n",
            "CPU MHz:             2299.998\n",
            "BogoMIPS:            4599.99\n",
            "Hypervisor vendor:   KVM\n",
            "Virtualization type: full\n",
            "L1d cache:           32K\n",
            "L1i cache:           32K\n",
            "L2 cache:            256K\n",
            "L3 cache:            46080K\n",
            "NUMA node0 CPU(s):   0,1\n",
            "Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat md_clear arch_capabilities\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQCR4SR1VSj-"
      },
      "source": [
        "Comprobamos la memoria física disponible y swap del sistema con $\\texttt{free}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jipoTT7VXPv",
        "outputId": "d67c9b75-d2f4-4110-c6d3-a54b3057c0f3"
      },
      "source": [
        "!free -kh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:            12G        609M        9.5G        1.2M        2.6G         11G\n",
            "Swap:            0B          0B          0B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsMVLr4R39eD"
      },
      "source": [
        "Observamos con más profundidad información del sistema como memoria total RAM, memoria usada para la caché o número total de swaps:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3ZEIoaAVe8o",
        "outputId": "89a6052b-1618-446b-fcb6-0351bdad496a"
      },
      "source": [
        "!cat /proc/meminfo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MemTotal:       13302924 kB\n",
            "MemFree:         9992260 kB\n",
            "MemAvailable:   12416732 kB\n",
            "Buffers:          137680 kB\n",
            "Cached:          2398740 kB\n",
            "SwapCached:            0 kB\n",
            "Active:          1169432 kB\n",
            "Inactive:        1848260 kB\n",
            "Active(anon):     426636 kB\n",
            "Inactive(anon):      432 kB\n",
            "Active(file):     742796 kB\n",
            "Inactive(file):  1847828 kB\n",
            "Unevictable:           0 kB\n",
            "Mlocked:               0 kB\n",
            "SwapTotal:             0 kB\n",
            "SwapFree:              0 kB\n",
            "Dirty:               608 kB\n",
            "Writeback:             0 kB\n",
            "AnonPages:        481276 kB\n",
            "Mapped:           270488 kB\n",
            "Shmem:              1196 kB\n",
            "KReclaimable:     150648 kB\n",
            "Slab:             202764 kB\n",
            "SReclaimable:     150648 kB\n",
            "SUnreclaim:        52116 kB\n",
            "KernelStack:        4832 kB\n",
            "PageTables:         6208 kB\n",
            "NFS_Unstable:          0 kB\n",
            "Bounce:                0 kB\n",
            "WritebackTmp:          0 kB\n",
            "CommitLimit:     6651460 kB\n",
            "Committed_AS:    3131820 kB\n",
            "VmallocTotal:   34359738367 kB\n",
            "VmallocUsed:       44836 kB\n",
            "VmallocChunk:          0 kB\n",
            "Percpu:             1592 kB\n",
            "AnonHugePages:      2048 kB\n",
            "ShmemHugePages:        0 kB\n",
            "ShmemPmdMapped:        0 kB\n",
            "FileHugePages:         0 kB\n",
            "FilePmdMapped:         0 kB\n",
            "CmaTotal:              0 kB\n",
            "CmaFree:               0 kB\n",
            "HugePages_Total:       0\n",
            "HugePages_Free:        0\n",
            "HugePages_Rsvd:        0\n",
            "HugePages_Surp:        0\n",
            "Hugepagesize:       2048 kB\n",
            "Hugetlb:               0 kB\n",
            "DirectMap4k:      299840 kB\n",
            "DirectMap2M:     8085504 kB\n",
            "DirectMap1G:     7340032 kB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4wJQ_pYXXRZ"
      },
      "source": [
        "Observamos el funcionamiento de diferentes comandos útiles:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkZaPypuXaSs"
      },
      "source": [
        "Listar el contenido:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suXeGTH4Xh5n",
        "outputId": "6353f44b-597d-4071-e17b-111fcc14194c"
      },
      "source": [
        "!ls -la"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 6308\n",
            "drwxr-xr-x 2 root root   4096 Nov  1 19:29 .\n",
            "drwxr-xr-x 1 root root   4096 Nov  1 16:50 ..\n",
            "-rwxr-xr-x 1 root root 711200 Nov  1 19:17 stencil1d_base\n",
            "-rw-r--r-- 1 root root   1751 Nov  1 19:16 stencil1d_base.cu\n",
            "-rwxr-xr-x 1 root root 711216 Nov  1 19:25 stenciltest_sm_1\n",
            "-rw-r--r-- 1 root root   2140 Nov  1 19:24 stenciltest_sm_1.cu\n",
            "-rwxr-xr-x 1 root root 711216 Nov  1 19:29 stenciltest_sm_2\n",
            "-rw-r--r-- 1 root root   2138 Nov  1 19:29 stenciltest_sm_2.cu\n",
            "-rwxr-xr-x 1 root root 711048 Nov  1 16:50 suma1d\n",
            "-rw-r--r-- 1 root root    931 Nov  1 16:50 suma1d.cu\n",
            "-rwxr-xr-x 1 root root 711080 Nov  1 17:55 suma2dvector\n",
            "-rw-r--r-- 1 root root   1596 Nov  1 17:55 suma2dvector.cu\n",
            "-rwxr-xr-x 1 root root 711152 Nov  1 18:44 suma2dvectorNBloqxNthreads\n",
            "-rw-r--r-- 1 root root   1744 Nov  1 18:44 suma2dvectorNBloqxNthreads.cu\n",
            "-rwxr-xr-x 1 root root 711120 Nov  1 18:05 suma2dvectorNthreads\n",
            "-rw-r--r-- 1 root root   1591 Nov  1 18:05 suma2dvectorNthreads.cu\n",
            "-rwxr-xr-x 1 root root 711024 Nov  1 18:54 suma2dvector_NxN\n",
            "-rw-r--r-- 1 root root   1784 Nov  1 18:53 suma2dvector_NxN.cu\n",
            "-rwxr-xr-x 1 root root 711016 Nov  1 19:04 sumamatrix_NxN\n",
            "-rw-r--r-- 1 root root   1784 Nov  1 19:04 sumamatrix_NxN.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tqmQjJRXq0N"
      },
      "source": [
        "Vemos el contenido del directorio /usr/local  donde está instalado CUDA:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pSvEcb0X6m2",
        "outputId": "d1151bfe-037e-438e-cd86-43c03f58793e"
      },
      "source": [
        "!ls -la /usr/local"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 92\n",
            "drwxr-xr-x  1 root root 4096 Oct 26 13:56 .\n",
            "drwxr-xr-x  1 root root 4096 Nov  1 16:45 ..\n",
            "drwxr-xr-x  1 root root 4096 Oct 29 17:07 bin\n",
            "lrwxrwxrwx  1 root root   22 Oct 26 13:27 cuda -> /etc/alternatives/cuda\n",
            "drwxr-xr-x 16 root root 4096 Oct 26 13:18 cuda-10.0\n",
            "drwxr-xr-x 15 root root 4096 Oct 26 13:21 cuda-10.1\n",
            "lrwxrwxrwx  1 root root   25 Oct 26 13:27 cuda-11 -> /etc/alternatives/cuda-11\n",
            "drwxr-xr-x 15 root root 4096 Oct 26 13:23 cuda-11.0\n",
            "drwxr-xr-x  1 root root 4096 Oct 26 13:25 cuda-11.1\n",
            "drwxr-xr-x  1 root root 4096 Oct 26 13:35 etc\n",
            "drwxr-xr-x  2 root root 4096 Nov 19  2020 games\n",
            "drwxr-xr-x  2 root root 4096 Oct 26 13:46 _gcs_config_ops.so\n",
            "drwxr-xr-x  1 root root 4096 Oct 26 13:56 include\n",
            "drwxr-xr-x  1 root root 4096 Oct 26 13:57 lib\n",
            "-rw-r--r--  1 root root 1636 Oct 26 13:51 LICENSE.txt\n",
            "drwxr-xr-x  3 root root 4096 Oct 26 13:46 licensing\n",
            "lrwxrwxrwx  1 root root    9 Nov 19  2020 man -> share/man\n",
            "drwxr-xr-x  2 root root 4096 Nov 19  2020 sbin\n",
            "-rw-r--r--  1 root root 7291 Oct 26 13:51 setup.cfg\n",
            "drwxr-xr-x  1 root root 4096 Oct 26 13:44 share\n",
            "drwxr-xr-x  2 root root 4096 Nov 19  2020 src\n",
            "drwxr-xr-x  2 root root 4096 Oct 26 13:58 xgboost\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsU2aQ3NZks5",
        "outputId": "0c80ea21-20a0-49be-97ab-31e05dc77750"
      },
      "source": [
        "!ls -la /usr/local/cuda-11.1/bin"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 80224\n",
            "drwxr-xr-x 1 root root     4096 Oct 26 13:25 .\n",
            "drwxr-xr-x 1 root root     4096 Oct 26 13:25 ..\n",
            "-rwxr-xr-x 1 root root    80608 Oct 13  2020 bin2c\n",
            "lrwxrwxrwx 1 root root        4 Oct 13  2020 computeprof -> nvvp\n",
            "-rwxr-xr-x 1 root root       97 Oct 13  2020 compute-sanitizer\n",
            "drwxr-xr-x 2 root root     4096 Dec 14  2020 crt\n",
            "-rwxr-xr-x 1 root root  5190640 Oct 13  2020 cudafe++\n",
            "-rwxr-xr-x 1 root root 12035840 Oct 13  2020 cuda-gdb\n",
            "-rwxr-xr-x 1 root root   753776 Oct 13  2020 cuda-gdbserver\n",
            "-rwxr-xr-x 1 root root      800 Oct 13  2020 cuda-install-samples-11.1.sh\n",
            "-rwxr-xr-x 1 root root   353752 Oct 13  2020 cuda-memcheck\n",
            "-rwxr-xr-x 1 root root   232752 Sep 16  2020 cuobjdump\n",
            "-rwxr-xr-x 1 root root   269392 Oct 13  2020 fatbinary\n",
            "-rwxr-xr-x 1 root root     2974 Oct 16  2020 ncu\n",
            "-rwxr-xr-x 1 root root     2577 Oct 16  2020 ncu-ui\n",
            "-rwxr-xr-x 1 root root     1580 Oct 13  2020 nsight_ee_plugins_manage.sh\n",
            "-rwxr-xr-x 1 root root      745 Oct 16  2020 nsight-sys\n",
            "-rwxr-xr-x 1 root root      746 Oct 16  2020 nsys\n",
            "-rwxr-xr-x 1 root root      104 Oct 16  2020 nsys-exporter\n",
            "-rwxr-xr-x 1 root root       85 Oct 16  2020 nsys-ui\n",
            "-rwxr-xr-x 1 root root  5120848 Oct 13  2020 nvcc\n",
            "-rw-r--r-- 1 root root      417 Oct 13  2020 nvcc.profile\n",
            "-rwxr-xr-x 1 root root 33704440 Sep 16  2020 nvdisasm\n",
            "-rwxr-xr-x 1 root root  9391256 Oct 13  2020 nvlink\n",
            "lrwxrwxrwx 1 root root        6 Oct 16  2020 nv-nsight-cu -> ncu-ui\n",
            "lrwxrwxrwx 1 root root        3 Oct 16  2020 nv-nsight-cu-cli -> ncu\n",
            "-rwxr-xr-x 1 root root  5592904 Oct 13  2020 nvprof\n",
            "-rwxr-xr-x 1 root root   101296 Sep 16  2020 nvprune\n",
            "-rwxr-xr-x 1 root root      285 Oct 13  2020 nvvp\n",
            "-rwxr-xr-x 1 root root  9234464 Oct 13  2020 ptxas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvgsLG0saJ9l",
        "outputId": "6332a8cf-5851-4fe0-fc0e-8995fe717d44"
      },
      "source": [
        "!ls -la /usr/local/cuda-11.1/samples\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 124\n",
            "drwxr-xr-x  1 root root  4096 Nov  1 16:50 .\n",
            "drwxr-xr-x  1 root root  4096 Oct 26 13:25 ..\n",
            "drwxr-xr-x 63 root root  4096 Oct 26 13:25 0_Simple\n",
            "drwxr-xr-x  1 root root  4096 Oct 26 13:25 1_Utilities\n",
            "drwxr-xr-x 14 root root  4096 Oct 26 13:25 2_Graphics\n",
            "drwxr-xr-x 24 root root  4096 Oct 26 13:25 3_Imaging\n",
            "drwxr-xr-x 10 root root  4096 Oct 26 13:25 4_Finance\n",
            "drwxr-xr-x 10 root root  4096 Oct 26 13:25 5_Simulations\n",
            "drwxr-xr-x 36 root root  4096 Oct 26 13:25 6_Advanced\n",
            "drwxr-xr-x 37 root root  4096 Oct 26 13:25 7_CUDALibraries\n",
            "drwxr-xr-x  3 root root  4096 Nov  1 16:50 bin\n",
            "drwxr-xr-x  6 root root  4096 Oct 26 13:25 common\n",
            "-rw-r--r--  1 root root 60537 Oct 13  2020 EULA.txt\n",
            "-rw-r--r--  1 root root  2606 Oct 13  2020 Makefile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv4vB-OuYP_k"
      },
      "source": [
        "Observamos la versión de CUDA instalada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOWMprbqYylZ",
        "outputId": "480ab70a-f4f6-4034-8520-234f1c43f67c"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-IeY9zi7uFc"
      },
      "source": [
        "Con la interfaz de configuración de NVIDIA observamos el estado de la GPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIE_XcCoZF0J",
        "outputId": "8fb6775b-d61a-4923-c76c-c6fa49d36bb4"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Nov  1 19:35:51 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.29.05    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P8    27W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqSdqOYDZR9f"
      },
      "source": [
        "Listamos los ejemplos proporcionados para el tutorial:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8amHLuTAZUoX",
        "outputId": "17cde98b-0397-4a92-e86c-506e35c8cb70"
      },
      "source": [
        "!ls -la /usr/local/cuda/samples/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 124\n",
            "drwxr-xr-x  1 root root  4096 Nov  1 16:50 .\n",
            "drwxr-xr-x  1 root root  4096 Oct 26 13:25 ..\n",
            "drwxr-xr-x 63 root root  4096 Oct 26 13:25 0_Simple\n",
            "drwxr-xr-x  1 root root  4096 Oct 26 13:25 1_Utilities\n",
            "drwxr-xr-x 14 root root  4096 Oct 26 13:25 2_Graphics\n",
            "drwxr-xr-x 24 root root  4096 Oct 26 13:25 3_Imaging\n",
            "drwxr-xr-x 10 root root  4096 Oct 26 13:25 4_Finance\n",
            "drwxr-xr-x 10 root root  4096 Oct 26 13:25 5_Simulations\n",
            "drwxr-xr-x 36 root root  4096 Oct 26 13:25 6_Advanced\n",
            "drwxr-xr-x 37 root root  4096 Oct 26 13:25 7_CUDALibraries\n",
            "drwxr-xr-x  3 root root  4096 Nov  1 16:50 bin\n",
            "drwxr-xr-x  6 root root  4096 Oct 26 13:25 common\n",
            "-rw-r--r--  1 root root 60537 Oct 13  2020 EULA.txt\n",
            "-rw-r--r--  1 root root  2606 Oct 13  2020 Makefile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iwlJSZqbxEw"
      },
      "source": [
        "Compilación de uno de los ejemplos proporcionados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECcKq7X4bwun",
        "outputId": "84b02354-74c2-4d03-c67b-fd8b13bd90a8"
      },
      "source": [
        "%cd /usr/local/cuda/samples/1_Utilities/deviceQuery/\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/cuda-11.1/samples/1_Utilities/deviceQuery\n",
            "\u001b[0m\u001b[01;32mdeviceQuery\u001b[0m*     deviceQuery.o  NsightEclipse.xml\n",
            "deviceQuery.cpp  Makefile       readme.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZpOrjkgb8Mf",
        "outputId": "ba173c9c-bd0d-443a-fa77-42262f29f6fb"
      },
      "source": [
        "!make"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "make: Nothing to be done for 'all'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w79U1y11cDst",
        "outputId": "75c7fddf-fb1c-4d5c-b382-c47b80730e3a"
      },
      "source": [
        "!ls -la"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 776\n",
            "drwxr-xr-x 1 root root   4096 Nov  1 16:50 .\n",
            "drwxr-xr-x 1 root root   4096 Oct 26 13:25 ..\n",
            "-rwxr-xr-x 1 root root 722704 Nov  1 16:50 deviceQuery\n",
            "-rw-r--r-- 1 root root  12720 Oct 13  2020 deviceQuery.cpp\n",
            "-rw-r--r-- 1 root root  16752 Nov  1 16:50 deviceQuery.o\n",
            "-rw-r--r-- 1 root root  12165 Oct 13  2020 Makefile\n",
            "-rw-r--r-- 1 root root   1815 Oct 13  2020 NsightEclipse.xml\n",
            "-rw-r--r-- 1 root root    168 Oct 13  2020 readme.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUg5wJQIcPUM"
      },
      "source": [
        "Corremos el ejecutable que nos ofrece información relevante acerca de CUDA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGu6lqTicOs0",
        "outputId": "0b23ec66-a914-4e2c-93e6-3f4cbd8c476b"
      },
      "source": [
        "!./deviceQuery"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./deviceQuery Starting...\n",
            "\n",
            " CUDA Device Query (Runtime API) version (CUDART static linking)\n",
            "\n",
            "Detected 1 CUDA Capable device(s)\n",
            "\n",
            "Device 0: \"Tesla K80\"\n",
            "  CUDA Driver Version / Runtime Version          11.2 / 11.1\n",
            "  CUDA Capability Major/Minor version number:    3.7\n",
            "  Total amount of global memory:                 11441 MBytes (11996954624 bytes)\n",
            "  (13) Multiprocessors, (192) CUDA Cores/MP:     2496 CUDA Cores\n",
            "  GPU Max Clock rate:                            824 MHz (0.82 GHz)\n",
            "  Memory Clock rate:                             2505 Mhz\n",
            "  Memory Bus Width:                              384-bit\n",
            "  L2 Cache Size:                                 1572864 bytes\n",
            "  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\n",
            "  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\n",
            "  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\n",
            "  Total amount of constant memory:               65536 bytes\n",
            "  Total amount of shared memory per block:       49152 bytes\n",
            "  Total shared memory per multiprocessor:        114688 bytes\n",
            "  Total number of registers available per block: 65536\n",
            "  Warp size:                                     32\n",
            "  Maximum number of threads per multiprocessor:  2048\n",
            "  Maximum number of threads per block:           1024\n",
            "  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n",
            "  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n",
            "  Maximum memory pitch:                          2147483647 bytes\n",
            "  Texture alignment:                             512 bytes\n",
            "  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\n",
            "  Run time limit on kernels:                     No\n",
            "  Integrated GPU sharing Host Memory:            No\n",
            "  Support host page-locked memory mapping:       Yes\n",
            "  Alignment requirement for Surfaces:            Yes\n",
            "  Device has ECC support:                        Enabled\n",
            "  Device supports Unified Addressing (UVA):      Yes\n",
            "  Device supports Managed Memory:                Yes\n",
            "  Device supports Compute Preemption:            No\n",
            "  Supports Cooperative Kernel Launch:            No\n",
            "  Supports MultiDevice Co-op Kernel Launch:      No\n",
            "  Device PCI Domain ID / Bus ID / location ID:   0 / 0 / 4\n",
            "  Compute Mode:\n",
            "     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n",
            "\n",
            "deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.2, CUDA Runtime Version = 11.1, NumDevs = 1\n",
            "Result = PASS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXva9zmVvp7B"
      },
      "source": [
        "Aprovechamos los resultados para responder a las preguntas que se nos ofrecen más abajo:\n",
        "* **Pruebe a lanzar diferente número de threads (con un solo 1 bloque)\n",
        "¿Cuáles son los valores máximos y mínimos de número de threads por bloque en esta GPU?**\n",
        "\n",
        "    Además de verlo en las posteriores ejecuciones, podemos ver ya que por definición el mínimo es 1 thread y como máximo se pueden tener 1024 threads por cada bloque.\n",
        "\n",
        "* **Pruebe a lanzar diferente número de bloques (con un solo thread) ¿Cuáles son los valores máximos y mínimos de número de bloques en esta GPU?**\n",
        "\n",
        "    En este caso el máximo número de bloques es 65535, siendo 1 el mínimo. Si se desea ver más información ver [especificaciones técnicas de CUDA](http://blog.cuvilib.com/2010/06/09/nvidia-cuda-difference-between-fermi-and-previous-architectures/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzeXoGFjdFpb"
      },
      "source": [
        "Compilar en CUDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tiuxb8ZcMg1",
        "outputId": "2f7ae9ed-1fb5-4d6c-be38-0ed78dcd4e9a"
      },
      "source": [
        "%cd /content/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmIeBtHB3kKf",
        "outputId": "c7b88c20-4e00-4e23-8dda-20fc935368c8"
      },
      "source": [
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/  \u001b[01;34mworkcuda\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq2Zkdb3dRbg"
      },
      "source": [
        "Hacer un directorio de trabajo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFIrKoxRdVKU",
        "outputId": "2f076690-6ca3-42c0-cd3b-db67a446dccb"
      },
      "source": [
        "!mkdir workcuda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘workcuda’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJnzSXYDdah0",
        "outputId": "054d2ba5-617b-48ff-a95b-a07710794fc2"
      },
      "source": [
        "cd workcuda/\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/workcuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fFsAUW9dgVJ",
        "outputId": "9ffcbe2d-7fba-4cc8-85f3-53d850c018bc"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/workcuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH1Xtcend3Bf"
      },
      "source": [
        "## Ejercicio 1: suma de vectores\n",
        "A continuación se discuten diferentes versiones de un programa que suma los elementos de un vector en CUDA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89b1I4R29vaA"
      },
      "source": [
        "### Ejercicio base en CUDA: 1 bloque/1 thread\n",
        "Nota: este ejercicio aportado no aprovecha los recursos de la GPU, es decir, se ejecuta en GPU pero en serie."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwAyqNfAd95Z",
        "outputId": "84530952-f559-4b30-dc72-39d3e991be3f"
      },
      "source": [
        "%%writefile suma1d.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "__global__ void add(int *a, int *b, int *c) {\n",
        "*c = *a + *b;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "int a, b, c;\n",
        "\n",
        "// host copies of variables a, b & c\n",
        "int *d_a, *d_b, *d_c;\n",
        "// device copies of variables a, b & c\n",
        "int size = sizeof(int);\n",
        "\n",
        "// Allocate space for device copies of a, b, c\n",
        "cudaMalloc((void **)&d_a, size);\n",
        "cudaMalloc((void **)&d_b, size);\n",
        "cudaMalloc((void **)&d_c, size);\n",
        "\n",
        "// Setup input values  \n",
        "c = 0;\n",
        "a = 3;\n",
        "b = 5;\n",
        "\n",
        "// Copy inputs to device\n",
        "cudaMemcpy(d_a, &a, size, cudaMemcpyHostToDevice);\n",
        "cudaMemcpy(d_b, &b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "// Launch add() kernel on GPU\n",
        "add<<<1,1>>>(d_a, d_b, d_c);\n",
        "\n",
        "// Copy result back to host\n",
        "cudaError err = cudaMemcpy(&c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "  if(err!=cudaSuccess) {\n",
        "      printf(\"CUDA error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "  }\n",
        "printf(\"result is %d\\n\",c);\n",
        "\n",
        "// Cleanup\n",
        "cudaFree(d_a);\n",
        "cudaFree(d_b);\n",
        "cudaFree(d_c);\n",
        "\n",
        "return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting suma1d.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e5lNkDHfZGl"
      },
      "source": [
        "Compilamos y ejecutamos el ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-Wx8K-ffJOS",
        "outputId": "361d13c2-907c-4d3f-9ef0-f049a9ad93de"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true suma1d.cu -o suma1d -lcudadevrt\n",
        "!./suma1d"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "result is 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BjD-FD6fjHM"
      },
      "source": [
        "Comprobamos el perfil de ejecución"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq6V0drHBzsH"
      },
      "source": [
        "Vemos que el resultado es correcto, pero estamos haciendo un mal uso de la GPU al no estar haciendo ninguna división. Por consiguiente esto puede afectar al tiempo de ejecución, que se puede ver fácilmente con el $\\texttt{profiler}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpSV8mdMfomL",
        "outputId": "fae7afb0-9efd-4e32-917c-b59eed15eb0e"
      },
      "source": [
        "!nvprof ./suma1d"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==4493== NVPROF is profiling process 4493, command: ./suma1d\n",
            "==4493== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "result is 8\n",
            "==4493== Profiling application: ./suma1d\n",
            "==4493== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   38.14%  3.8080us         2  1.9040us  1.5360us  2.2720us  [CUDA memcpy HtoD]\n",
            "                   36.54%  3.6480us         1  3.6480us  3.6480us  3.6480us  add(int*, int*, int*)\n",
            "                   25.32%  2.5280us         1  2.5280us  2.5280us  2.5280us  [CUDA memcpy DtoH]\n",
            "      API calls:   99.51%  181.06ms         3  60.354ms  2.5710us  181.05ms  cudaMalloc\n",
            "                    0.26%  471.62us         1  471.62us  471.62us  471.62us  cuDeviceTotalMem\n",
            "                    0.09%  170.59us       101  1.6890us     158ns  71.211us  cuDeviceGetAttribute\n",
            "                    0.07%  134.11us         3  44.703us  5.2880us  115.83us  cudaFree\n",
            "                    0.03%  53.173us         3  17.724us  12.258us  24.704us  cudaMemcpy\n",
            "                    0.01%  26.954us         1  26.954us  26.954us  26.954us  cuDeviceGetName\n",
            "                    0.01%  22.920us         1  22.920us  22.920us  22.920us  cudaLaunchKernel\n",
            "                    0.00%  7.6280us         1  7.6280us  7.6280us  7.6280us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.0840us         3     694ns     233ns  1.1880us  cuDeviceGetCount\n",
            "                    0.00%  1.4180us         2     709ns     274ns  1.1440us  cuDeviceGet\n",
            "                    0.00%     493ns         1     493ns     493ns     493ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FqMcRGNHCqu"
      },
      "source": [
        "### Ejercicio N bloques/1 thread\n",
        "Probamos haciendo paralelismo de 512 bloques de 1 thread."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDULMwPfgxuU",
        "outputId": "753a54b3-f0bf-42e5-d373-8e7a3bb1481b"
      },
      "source": [
        "%%writefile suma2dvector.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "__global__ void add(int *a, int *b, int *c) {\n",
        "//*c = *a + *b;\n",
        "c[blockIdx.x] = a[blockIdx.x] + b[blockIdx.x]; \n",
        "}\n",
        "\n",
        "#define N 512\n",
        "\n",
        "int main(void) {\n",
        "int *a, *b, *c;        // host copies of variables a, b & c\n",
        "int *d_a, *d_b, *d_c;  // device copies of variables a, b & c\n",
        "int size = N * sizeof(int);\n",
        "\n",
        "// Allocate space for host copies of a, b, c   Setup input values  \n",
        "a =  (int *) malloc(size); \n",
        "b =  (int *) malloc(size); \n",
        "c =  (int *) malloc(size); \n",
        "\n",
        "\n",
        "// Setup input values  \n",
        "\n",
        "for( int i = 0; i < N; i++ )\n",
        "\t{\n",
        "\t\ta[i] = i;\n",
        "    b[i] = N-i;\n",
        "\t\tc[i] = 0;\n",
        "\t}\n",
        "\n",
        "\n",
        "// Allocate space for device copies of a, b, c\n",
        "cudaMalloc((void **)&d_a, size);\n",
        "cudaMalloc((void **)&d_b, size);\n",
        "cudaMalloc((void **)&d_c, size);\n",
        "\n",
        "\n",
        "// Copy inputs to device\n",
        "cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n",
        "cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "// Launch add() kernel on GPU  Se lanzan N bloques de 1 Thread.\n",
        "add<<<N,1>>>(d_a, d_b, d_c);\n",
        "\n",
        "// Copy result back to host\n",
        "cudaError err = cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "  if(err!=cudaSuccess) {\n",
        "      printf(\"CUDA error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "  }\n",
        "printf(\" valor a[0] es %d\\n\",a[0]);\n",
        "printf(\" valor b[0] es %d\\n\",b[0]);\n",
        "printf(\"resultado c[0] es %d\\n\",c[0]);\n",
        "printf(\" valor a[2] es %d\\n\",a[2]);\n",
        "printf(\" valor b[2] es %d\\n\",b[2]);\n",
        "printf(\"resultado c[2] es %d\\n\",c[2]);\n",
        "printf(\" valor a[15] es %d\\n\",a[15]);\n",
        "printf(\" valor b[15] es %d\\n\",b[15]);\n",
        "printf(\"resultado c[15] es %d\\n\",c[15]);\n",
        "\n",
        "// Cleanup\n",
        "\n",
        "free(a); free(b);free(c); \n",
        "\n",
        "cudaFree(d_a);\n",
        "cudaFree(d_b);\n",
        "cudaFree(d_c);\n",
        "\n",
        "return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting suma2dvector.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEcMF9ZODP3r"
      },
      "source": [
        "Compilamos y ejecutamos y vemos que el resultado es correcto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-w1xk4RFDZ9t",
        "outputId": "f555b693-b500-46ea-9993-fe2e6d114242"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true suma2dvector.cu -o suma2dvector -lcudadevrt\n",
        "!./suma2dvector"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            " valor a[0] es 0\n",
            " valor b[0] es 512\n",
            "resultado c[0] es 512\n",
            " valor a[2] es 2\n",
            " valor b[2] es 510\n",
            "resultado c[2] es 512\n",
            " valor a[15] es 15\n",
            " valor b[15] es 497\n",
            "resultado c[15] es 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lUTdjJuFTnR"
      },
      "source": [
        "Comprobamos el perfil de ejecución y vemos que tarda más que en el caso anterior. Si bien al haber solo un thread por bloque, quiere decir que solo se ejecuta un warp por bloque y no maximiza el aprovechamiento de la GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPBo6tWUFWyb",
        "outputId": "1d3c903a-26ae-42ab-a953-bc06ad8c3d4b"
      },
      "source": [
        "!nvprof ./suma2dvector"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==4540== NVPROF is profiling process 4540, command: ./suma2dvector\n",
            "==4540== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            " valor a[0] es 0\n",
            " valor b[0] es 512\n",
            "resultado c[0] es 512\n",
            " valor a[2] es 2\n",
            " valor b[2] es 510\n",
            "resultado c[2] es 512\n",
            " valor a[15] es 15\n",
            " valor b[15] es 497\n",
            "resultado c[15] es 512\n",
            "==4540== Profiling application: ./suma2dvector\n",
            "==4540== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   41.47%  5.0560us         1  5.0560us  5.0560us  5.0560us  add(int*, int*, int*)\n",
            "                   37.01%  4.5120us         2  2.2560us  1.9840us  2.5280us  [CUDA memcpy HtoD]\n",
            "                   21.52%  2.6240us         1  2.6240us  2.6240us  2.6240us  [CUDA memcpy DtoH]\n",
            "      API calls:   99.46%  175.79ms         3  58.597ms  2.0500us  175.79ms  cudaMalloc\n",
            "                    0.27%  470.86us         1  470.86us  470.86us  470.86us  cuDeviceTotalMem\n",
            "                    0.11%  199.46us       101  1.9740us     154ns  94.019us  cuDeviceGetAttribute\n",
            "                    0.07%  125.02us         3  41.671us  4.2700us  106.40us  cudaFree\n",
            "                    0.06%  104.01us         3  34.671us  9.0000us  57.756us  cudaMemcpy\n",
            "                    0.01%  24.985us         1  24.985us  24.985us  24.985us  cuDeviceGetName\n",
            "                    0.01%  24.406us         1  24.406us  24.406us  24.406us  cudaLaunchKernel\n",
            "                    0.00%  7.9670us         1  7.9670us  7.9670us  7.9670us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.2150us         3     738ns     200ns  1.0740us  cuDeviceGetCount\n",
            "                    0.00%  1.6500us         2     825ns     486ns  1.1640us  cuDeviceGet\n",
            "                    0.00%     336ns         1     336ns     336ns     336ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0FbrAaUHUQI"
      },
      "source": [
        "### Ejercicio N bloques/1 thread\n",
        "Probamos haciendo paralelismo de 1 bloque de 1024 threads.\n",
        "\n",
        "Nota: en el tutorial se determinaba N como 1028, sin embargo esto supera el máximo número de threads permitidos por bloque, por lo que el resultado es incorrecto, con el fin de comparar los resultados con diferentes opciones, determinamos N como 1024 threads para este caso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7ujd3TwHcE0",
        "outputId": "6466aa11-b26e-46b7-bdbb-c06b2e26d9b5"
      },
      "source": [
        "%%writefile suma2dvectorNthreads.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "__global__ void add(int *a, int *b, int *c) {\n",
        "c[threadIdx.x] = a[threadIdx.x] + b[threadIdx.x]; \n",
        "}\n",
        "\n",
        "#define N 1024\n",
        "\n",
        "int main(void) {\n",
        "int *a, *b, *c;        // host copies of variables a, b & c\n",
        "int *d_a, *d_b, *d_c;  // device copies of variables a, b & c\n",
        "int size = N * sizeof(int);\n",
        "\n",
        "// Allocate space for host copies of a, b, c   Setup input values  \n",
        "a =  (int *) malloc(size); \n",
        "b =  (int *) malloc(size); \n",
        "c =  (int *) malloc(size); \n",
        "\n",
        "\n",
        "// Setup input values  \n",
        "\n",
        "for( int i = 0; i < N; i++ )\n",
        "\t{\n",
        "\t\ta[i] = i;\n",
        "    b[i] = N-i;\n",
        "\t\tc[i] = 0;\n",
        "\t}\n",
        "\n",
        "\n",
        "// Allocate space for device copies of a, b, c\n",
        "cudaMalloc((void **)&d_a, size);\n",
        "cudaMalloc((void **)&d_b, size);\n",
        "cudaMalloc((void **)&d_c, size);\n",
        "\n",
        "\n",
        "// Copy inputs to device\n",
        "cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n",
        "cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "// Launch add() kernel on GPU  Se lanzan 1 bloques de N Threads.\n",
        "add<<<1,N>>>(d_a, d_b, d_c);\n",
        "\n",
        "// Copy result back to host\n",
        "cudaError err = cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "  if(err!=cudaSuccess) {\n",
        "      printf(\"CUDA error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "  }\n",
        "printf(\" valor a[10] es %d\\n\",a[10]);\n",
        "printf(\" valor b[10] es %d\\n\",b[10]);\n",
        "printf(\"resultado c[10] es %d\\n\",c[10]);\n",
        "printf(\" valor a[0] es %d\\n\",a[0]);\n",
        "printf(\" valor b[0] es %d\\n\",b[0]);\n",
        "printf(\"resultado c[0] es %d\\n\",c[0]);\n",
        "printf(\" valor a[15] es %d\\n\",a[15]);\n",
        "printf(\" valor b[15] es %d\\n\",b[15]);\n",
        "printf(\"resultado c[15] es %d\\n\",c[15]);\n",
        "\n",
        "// Cleanup\n",
        "\n",
        "free(a); free(b);free(c); \n",
        "\n",
        "cudaFree(d_a);\n",
        "cudaFree(d_b);\n",
        "cudaFree(d_c);\n",
        "\n",
        "return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting suma2dvectorNthreads.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E0HiMy6IU1L"
      },
      "source": [
        "Ejecutamos este ejemplo y vemos que nuevamente el resultado es correcto.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW4YFVPkIYHr",
        "outputId": "6d911b80-b2d6-48a8-9fb0-da2cdc7306ec"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true suma2dvectorNthreads.cu -o suma2dvectorNthreads -lcudadevrt\n",
        "!./suma2dvectorNthreads"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            " valor a[10] es 10\n",
            " valor b[10] es 1014\n",
            "resultado c[10] es 1024\n",
            " valor a[0] es 0\n",
            " valor b[0] es 1024\n",
            "resultado c[0] es 1024\n",
            " valor a[15] es 15\n",
            " valor b[15] es 1009\n",
            "resultado c[15] es 1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2uXYiF_sDAu"
      },
      "source": [
        "En este caso vemos que la suma tarda bastante y tampoco se aprovechan los recursos de la GPU de manera eficiente. Esto es porque estamos realizando 1024 operaciones que acceden a memoria global en un único bloque."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhGdz2EMw5W3",
        "outputId": "c5cafd0e-d27e-419d-d50c-3ce607b15680"
      },
      "source": [
        "!nvprof ./suma2dvectorNthreads"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==4587== NVPROF is profiling process 4587, command: ./suma2dvectorNthreads\n",
            "==4587== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            " valor a[10] es 10\n",
            " valor b[10] es 1014\n",
            "resultado c[10] es 1024\n",
            " valor a[0] es 0\n",
            " valor b[0] es 1024\n",
            "resultado c[0] es 1024\n",
            " valor a[15] es 15\n",
            " valor b[15] es 1009\n",
            "resultado c[15] es 1024\n",
            "==4587== Profiling application: ./suma2dvectorNthreads\n",
            "==4587== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   45.41%  5.2160us         2  2.6080us  2.4320us  2.7840us  [CUDA memcpy HtoD]\n",
            "                   29.25%  3.3600us         1  3.3600us  3.3600us  3.3600us  add(int*, int*, int*)\n",
            "                   25.34%  2.9110us         1  2.9110us  2.9110us  2.9110us  [CUDA memcpy DtoH]\n",
            "      API calls:   99.45%  185.88ms         3  61.959ms  2.3780us  185.87ms  cudaMalloc\n",
            "                    0.29%  539.90us         1  539.90us  539.90us  539.90us  cuDeviceTotalMem\n",
            "                    0.12%  218.39us       101  2.1620us     156ns  102.01us  cuDeviceGetAttribute\n",
            "                    0.08%  154.44us         3  51.478us  5.0900us  136.19us  cudaFree\n",
            "                    0.03%  62.978us         3  20.992us  14.171us  24.481us  cudaMemcpy\n",
            "                    0.01%  24.086us         1  24.086us  24.086us  24.086us  cuDeviceGetName\n",
            "                    0.01%  23.727us         1  23.727us  23.727us  23.727us  cudaLaunchKernel\n",
            "                    0.00%  5.9620us         1  5.9620us  5.9620us  5.9620us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.6620us         3     554ns     194ns     884ns  cuDeviceGetCount\n",
            "                    0.00%  1.4100us         2     705ns     290ns  1.1200us  cuDeviceGet\n",
            "                    0.00%     308ns         1     308ns     308ns     308ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX_QYtIPxjBb"
      },
      "source": [
        "### Ejercicio N bloques/N threads\n",
        "A continuación se aprovechan los recursos que nos ofrece CUDA combinando bloques y threads para procesar el programa de forma paralela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Spb5jhxKyGz1",
        "outputId": "fdb6b056-ce55-4854-ef3d-c0d24884bc15"
      },
      "source": [
        "%%writefile suma2dvectorNBloqxNthreads.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "__global__ void add(int *a, int *b, int *c) {\n",
        "int index = threadIdx.x + blockIdx.x * blockDim.x;\n",
        " c[index] = a[index] + b[index]; \n",
        "}\n",
        "\n",
        "#define N (1024*1024)\n",
        "#define THREADS_PER_BLOCK 512 \n",
        "\n",
        "int main(void) {\n",
        "int *a, *b, *c;        // host copies of variables a, b & c\n",
        "int *d_a, *d_b, *d_c;  // device copies of variables a, b & c\n",
        "int size = N*sizeof(int);\n",
        "\n",
        "// Allocate space for host copies of a, b, c   Setup input values  \n",
        "a =  (int *) malloc(size); \n",
        "b =  (int *) malloc(size); \n",
        "c =  (int *) malloc(size); \n",
        "\n",
        "\n",
        "// Setup input values  \n",
        "\n",
        "for( int i = 0; i < N; i++ )\n",
        "\t{\n",
        "\t\ta[i] = i;\n",
        "    b[i] = N-i;\n",
        "\t\tc[i] = 0;\n",
        "\t}\n",
        "\n",
        "\n",
        "// Allocate space for device copies of a, b, c\n",
        "cudaMalloc((void **)&d_a, size);\n",
        "cudaMalloc((void **)&d_b, size);\n",
        "cudaMalloc((void **)&d_c, size);\n",
        "\n",
        "\n",
        "// Copy inputs to device\n",
        "cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n",
        "cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "// Launch add() kernel on GPU  Se lanzan NuBloq=N/THREADS_PER_BLOCK bloques de \n",
        "// THREADS_PER_BLOCK Threads.\n",
        "add<<<N/THREADS_PER_BLOCK,THREADS_PER_BLOCK>>>(d_a, d_b, d_c);\n",
        "\n",
        "// Copy result back to host\n",
        "cudaError err = cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "  if(err!=cudaSuccess) {\n",
        "      printf(\"CUDA error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "  }\n",
        "printf(\" valor a[10] es %d\\n\",a[10]);\n",
        "printf(\" valor b[10] es %d\\n\",b[10]);\n",
        "printf(\"resultado c[10] es %d\\n\",c[10]);\n",
        "printf(\" valor a[0] es %d\\n\",a[0]);\n",
        "printf(\" valor b[0] es %d\\n\",b[0]);\n",
        "printf(\"resultado c[0] es %d\\n\",c[0]);\n",
        "printf(\" valor a[15] es %d\\n\",a[15]);\n",
        "printf(\" valor b[15] es %d\\n\",b[15]);\n",
        "printf(\"resultado c[15] es %d\\n\",c[15]);\n",
        "\n",
        "// Cleanup\n",
        "\n",
        "free(a); free(b);free(c); \n",
        "\n",
        "cudaFree(d_a);\n",
        "cudaFree(d_b);\n",
        "cudaFree(d_c);\n",
        "\n",
        "return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting suma2dvectorNBloqxNthreads.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTeZiHnfxi1E"
      },
      "source": [
        "Comprobamos que el resultado es el mismo y es correcto:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxIvXWKCzBxw",
        "outputId": "689866c4-717a-4763-a75e-f253de68c89b"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true suma2dvectorNBloqxNthreads.cu -o suma2dvectorNBloqxNthreads -lcudadevrt\n",
        "!./suma2dvectorNBloqxNthreads"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            " valor a[10] es 10\n",
            " valor b[10] es 1048566\n",
            "resultado c[10] es 1048576\n",
            " valor a[0] es 0\n",
            " valor b[0] es 1048576\n",
            "resultado c[0] es 1048576\n",
            " valor a[15] es 15\n",
            " valor b[15] es 1048561\n",
            "resultado c[15] es 1048576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXqdKI-X2Tcr"
      },
      "source": [
        "Comprobamos con el perfil de ejecución que se realiza el mejor uso de la GPU hasta ahora."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H08bppmr2UKm",
        "outputId": "418ef4cc-9e05-4895-d57d-f2c43b59bf08"
      },
      "source": [
        "!nvprof ./suma2dvectorNBloqxNthreads"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==4634== NVPROF is profiling process 4634, command: ./suma2dvectorNBloqxNthreads\n",
            "==4634== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            " valor a[10] es 10\n",
            " valor b[10] es 1048566\n",
            "resultado c[10] es 1048576\n",
            " valor a[0] es 0\n",
            " valor b[0] es 1048576\n",
            "resultado c[0] es 1048576\n",
            " valor a[15] es 15\n",
            " valor b[15] es 1048561\n",
            "resultado c[15] es 1048576\n",
            "==4634== Profiling application: ./suma2dvectorNBloqxNthreads\n",
            "==4634== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   67.20%  1.3186ms         2  659.29us  632.73us  685.85us  [CUDA memcpy HtoD]\n",
            "                   27.50%  539.55us         1  539.55us  539.55us  539.55us  [CUDA memcpy DtoH]\n",
            "                    5.30%  104.10us         1  104.10us  104.10us  104.10us  add(int*, int*, int*)\n",
            "      API calls:   97.60%  183.17ms         3  61.058ms  114.71us  182.94ms  cudaMalloc\n",
            "                    1.25%  2.3552ms         3  785.06us  697.30us  883.01us  cudaMemcpy\n",
            "                    0.71%  1.3254ms         3  441.79us  129.50us  607.24us  cudaFree\n",
            "                    0.26%  489.99us         1  489.99us  489.99us  489.99us  cuDeviceTotalMem\n",
            "                    0.13%  249.10us       101  2.4660us     154ns  97.077us  cuDeviceGetAttribute\n",
            "                    0.03%  50.426us         1  50.426us  50.426us  50.426us  cudaLaunchKernel\n",
            "                    0.02%  28.349us         1  28.349us  28.349us  28.349us  cuDeviceGetName\n",
            "                    0.00%  5.9450us         1  5.9450us  5.9450us  5.9450us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.6620us         3     887ns     352ns  1.4070us  cuDeviceGetCount\n",
            "                    0.00%  1.5950us         2     797ns     445ns  1.1500us  cuDeviceGet\n",
            "                    0.00%     321ns         1     321ns     321ns     321ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Plee2mF3HSI"
      },
      "source": [
        "**Pregunta ¿Qué sucede si incrementa o disminuye el valor de N?**\n",
        "Para responder a esta pregunta, probamos a ejecutar con diferentes tamaños de N para threads."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rrQ_VSzyyWe",
        "outputId": "acf596c0-62fa-47fc-aa69-ca6f582a3132"
      },
      "source": [
        "!sed -i '/#define N/c\\#define N (1024*1024)' suma2dvectorNBloqxNthreads.cu\n",
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true suma2dvectorNBloqxNthreads.cu -o suma2dvectorNBloqxNthreads -lcudadevrt\n",
        "!nvprof ./suma2dvectorNBloqxNthreads"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==4679== NVPROF is profiling process 4679, command: ./suma2dvectorNBloqxNthreads\n",
            "==4679== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            " valor a[10] es 10\n",
            " valor b[10] es 1048566\n",
            "resultado c[10] es 1048576\n",
            " valor a[0] es 0\n",
            " valor b[0] es 1048576\n",
            "resultado c[0] es 1048576\n",
            " valor a[15] es 15\n",
            " valor b[15] es 1048561\n",
            "resultado c[15] es 1048576\n",
            "==4679== Profiling application: ./suma2dvectorNBloqxNthreads\n",
            "==4679== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   64.31%  1.1589ms         2  579.45us  578.36us  580.54us  [CUDA memcpy HtoD]\n",
            "                   29.94%  539.51us         1  539.51us  539.51us  539.51us  [CUDA memcpy DtoH]\n",
            "                    5.74%  103.52us         1  103.52us  103.52us  103.52us  add(int*, int*, int*)\n",
            "      API calls:   97.80%  192.61ms         3  64.204ms  114.16us  192.37ms  cudaMalloc\n",
            "                    1.15%  2.2560ms         3  752.02us  654.53us  867.79us  cudaMemcpy\n",
            "                    0.70%  1.3755ms         3  458.50us  207.89us  589.83us  cudaFree\n",
            "                    0.24%  471.85us         1  471.85us  471.85us  471.85us  cuDeviceTotalMem\n",
            "                    0.08%  157.02us       101  1.5540us     129ns  64.298us  cuDeviceGetAttribute\n",
            "                    0.02%  34.926us         1  34.926us  34.926us  34.926us  cudaLaunchKernel\n",
            "                    0.01%  22.136us         1  22.136us  22.136us  22.136us  cuDeviceGetName\n",
            "                    0.00%  5.1570us         1  5.1570us  5.1570us  5.1570us  cuDeviceGetPCIBusId\n",
            "                    0.00%  3.1440us         3  1.0480us     417ns  1.8470us  cuDeviceGetCount\n",
            "                    0.00%  1.4890us         2     744ns     333ns  1.1560us  cuDeviceGet\n",
            "                    0.00%     243ns         1     243ns     243ns     243ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUIhc1ZLzN_F"
      },
      "source": [
        "En este caso hemos probado para un número menor de threads y vemos que el rendimiento es menor. Si lo incrementamos vemos que el resultado de la aplicación sigue siendo correcto pero porque ocupa el máximo número de threads permitidos, si bien el tamaño produce errores de memoria."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgS0a4mYzY8w",
        "outputId": "7d026a87-7618-409b-c5ea-0c6ba87c1df4"
      },
      "source": [
        "!sed -i '/#define N/c\\#define N (1024*1024*500)' suma2dvectorNBloqxNthreads.cu\n",
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true suma2dvectorNBloqxNthreads.cu -o suma2dvectorNBloqxNthreads -lcudadevrt\n",
        "!nvprof ./suma2dvectorNBloqxNthreads"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "tcmalloc: large alloc 2097152000 bytes == 0x555ebebe6000 @  0x7f2ec3db01e7 0x555ebd422272 0x7f2ec2de1bf7 0x555ebd421fda\n",
            "tcmalloc: large alloc 2097152000 bytes == 0x555f3bbe6000 @  0x7f2ec3db01e7 0x555ebd422283 0x7f2ec2de1bf7 0x555ebd421fda\n",
            "tcmalloc: large alloc 2097152000 bytes == 0x555fb8be6000 @  0x7f2ec3db01e7 0x555ebd422294 0x7f2ec2de1bf7 0x555ebd421fda\n",
            "==4724== NVPROF is profiling process 4724, command: ./suma2dvectorNBloqxNthreads\n",
            "==4724== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            " valor a[10] es 10\n",
            " valor b[10] es 524287990\n",
            "resultado c[10] es 524288000\n",
            " valor a[0] es 0\n",
            " valor b[0] es 524288000\n",
            "resultado c[0] es 524288000\n",
            " valor a[15] es 15\n",
            " valor b[15] es 524287985\n",
            "resultado c[15] es 524288000\n",
            "==4724== Profiling application: ./suma2dvectorNBloqxNthreads\n",
            "==4724== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   69.58%  554.16ms         2  277.08ms  262.82ms  291.34ms  [CUDA memcpy HtoD]\n",
            "                   25.19%  200.63ms         1  200.63ms  200.63ms  200.63ms  [CUDA memcpy DtoH]\n",
            "                    5.23%  41.678ms         1  41.678ms  41.678ms  41.678ms  add(int*, int*, int*)\n",
            "      API calls:   59.34%  796.84ms         3  265.61ms  242.46ms  291.40ms  cudaMemcpy\n",
            "                   26.05%  349.77ms         3  116.59ms  2.0844ms  176.08ms  cudaFree\n",
            "                   14.54%  195.25ms         3  65.085ms  2.6419ms  189.67ms  cudaMalloc\n",
            "                    0.04%  550.38us         1  550.38us  550.38us  550.38us  cuDeviceTotalMem\n",
            "                    0.02%  217.31us       101  2.1510us     155ns  105.43us  cuDeviceGetAttribute\n",
            "                    0.00%  51.508us         1  51.508us  51.508us  51.508us  cuDeviceGetName\n",
            "                    0.00%  42.084us         1  42.084us  42.084us  42.084us  cudaLaunchKernel\n",
            "                    0.00%  5.9560us         1  5.9560us  5.9560us  5.9560us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.8030us         3     601ns     208ns     990ns  cuDeviceGetCount\n",
            "                    0.00%  1.5300us         2     765ns     369ns  1.1610us  cuDeviceGet\n",
            "                    0.00%     283ns         1     283ns     283ns     283ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhpaNyib0t-b"
      },
      "source": [
        "## Ejercicio: suma de matrices cuadradas de tamaño N\n",
        "En este caso se realiza la suma de matrices para cualquier *N*, sin tener que definir números fijos y precalcular el número de bloques dado un número de threads. Para ello se ha definido el número de threads por bloque *MAX* como $\\frac{N + MAX - 1}{MAX}$. Asimismo, fijando $MAX=8$ se obtiene un tamaño de grid de $8*8 = 64$, lo que en otras palabras significa que podemos llegar al máximo número de threads sin violar ninguna restricción de CUDA. Además, se ha debido cambiar el *kernel* de la función *add* a una estructura bidimensional para que realice la suma $a_{i,j}+b_{i,j}$ y se guarde el resultado en $c_{i,j}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIb86yWWqN8T",
        "outputId": "2734b902-cc4d-4b01-8fd5-80ed37353de7"
      },
      "source": [
        "%%writefile sumamatrix_NxN.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define N 1024\n",
        "#define MAX 8\n",
        "\n",
        "__global__ void add(float *a, float *b, float *c, int n) {\n",
        "  int j = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  int i = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "  int index = i*n + j;\n",
        "  \n",
        "  if (i < n && j < n)\n",
        "    c[index] = a[index] + b[index];\n",
        "}\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "  float *a, *b, *c;\n",
        "  float *d_a, *d_b, *d_c;\n",
        "  int size = N*N*sizeof(float);\n",
        "\n",
        "  a = (float *) malloc(size);\n",
        "  b = (float *) malloc(size);\n",
        "  c = (float *) malloc(size);\n",
        "\n",
        "  // Inicializamos los datos\n",
        "  for (int i = 0; i < N; i++ ){\n",
        "      for (int j = 0; j < N; j++){\n",
        "          a[i*N + j] = 1.0f;\n",
        "          b[i*N + j] = 2.0f;\n",
        "      }\n",
        "  }\n",
        "\n",
        "  cudaMalloc((void **)&d_a, size);\n",
        "  cudaMalloc((void **)&d_b, size);\n",
        "  cudaMalloc((void **)&d_c, size);\n",
        "\n",
        "  cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "  // grid size\n",
        "  dim3 dimGrid((N + MAX - 1)/MAX , (N + MAX - 1)/MAX, 1);\n",
        "  \n",
        "  // block size\n",
        "  dim3 dimBlock(MAX, MAX, 1);\n",
        "\n",
        "  // Launch add() kernel on GPU\n",
        "\n",
        "  add<<<dimGrid, dimBlock>>>(d_a, d_b, d_c, N);\n",
        "\n",
        "    // Copy result back to host\n",
        "  cudaError err = cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "    if(err!=cudaSuccess) {\n",
        "        printf(\"CUDA error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "    }\n",
        "  printf(\" valor a[10] es %f\\n\",a[10]);\n",
        "  printf(\" valor b[10] es %f\\n\",b[10]);\n",
        "  printf(\"resultado c[10] es %f\\n\",c[10]);\n",
        "  printf(\" valor a[0] es %f\\n\",a[0]);\n",
        "  printf(\" valor b[0] es %f\\n\",b[0]);\n",
        "  printf(\"resultado c[0] es %f\\n\",c[0]);\n",
        "  printf(\" valor a[15] es %f\\n\",a[0]);\n",
        "  printf(\" valor b[15] es %f\\n\",b[0]);\n",
        "  printf(\"resultado c[15] es %f\\n\",c[0]);\n",
        "\n",
        "  // Cleanup\n",
        "\n",
        "  free(a); free(b); free(c); \n",
        "\n",
        "  cudaFree(d_a);\n",
        "  cudaFree(d_b);\n",
        "  cudaFree(d_c);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting sumamatrix_NxN.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow545eeb41Ke"
      },
      "source": [
        "Vemos que la ejecución es correcta:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5IXknL8u5m6",
        "outputId": "fe35846f-32a8-48ee-b07d-810026cae07d"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true sumamatrix_NxN.cu -o sumamatrix_NxN -lcudadevrt\n",
        "!./sumamatrix_NxN"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            " valor a[10] es 1.000000\n",
            " valor b[10] es 2.000000\n",
            "resultado c[10] es 3.000000\n",
            " valor a[0] es 1.000000\n",
            " valor b[0] es 2.000000\n",
            "resultado c[0] es 3.000000\n",
            " valor a[15] es 1.000000\n",
            " valor b[15] es 2.000000\n",
            "resultado c[15] es 3.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYFb5Wxt45RY"
      },
      "source": [
        "Además vemos que se ejecuta en un tiempo razonable (1.7 ms)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j38xyYx5qPar",
        "outputId": "e42235a1-ca3b-4455-b942-bd51bf904ed7"
      },
      "source": [
        "!nvprof ./sumamatrix_NxN"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==4773== NVPROF is profiling process 4773, command: ./sumamatrix_NxN\n",
            "==4773== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            " valor a[10] es 1.000000\n",
            " valor b[10] es 2.000000\n",
            "resultado c[10] es 3.000000\n",
            " valor a[0] es 1.000000\n",
            " valor b[0] es 2.000000\n",
            "resultado c[0] es 3.000000\n",
            " valor a[15] es 1.000000\n",
            " valor b[15] es 2.000000\n",
            "resultado c[15] es 3.000000\n",
            "==4773== Profiling application: ./sumamatrix_NxN\n",
            "==4773== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   56.24%  1.4031ms         1  1.4031ms  1.4031ms  1.4031ms  [CUDA memcpy DtoH]\n",
            "                   39.31%  980.89us         2  490.44us  482.75us  498.14us  [CUDA memcpy HtoD]\n",
            "                    4.45%  111.01us         1  111.01us  111.01us  111.01us  add(float*, float*, float*, int)\n",
            "      API calls:   97.07%  188.76ms         3  62.919ms  133.85us  188.47ms  cudaMalloc\n",
            "                    2.04%  3.9617ms         3  1.3206ms  580.55us  2.7676ms  cudaMemcpy\n",
            "                    0.48%  938.76us         3  312.92us  138.43us  401.54us  cudaFree\n",
            "                    0.29%  559.19us         1  559.19us  559.19us  559.19us  cuDeviceTotalMem\n",
            "                    0.09%  173.84us       101  1.7210us     161ns  71.920us  cuDeviceGetAttribute\n",
            "                    0.01%  28.735us         1  28.735us  28.735us  28.735us  cudaLaunchKernel\n",
            "                    0.01%  23.934us         1  23.934us  23.934us  23.934us  cuDeviceGetName\n",
            "                    0.00%  5.8230us         1  5.8230us  5.8230us  5.8230us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.0170us         3     672ns     204ns  1.0400us  cuDeviceGetCount\n",
            "                    0.00%  1.6700us         2     835ns     467ns  1.2030us  cuDeviceGet\n",
            "                    0.00%     327ns         1     327ns     327ns     327ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZJjBuKx3VYo"
      },
      "source": [
        "#Ejercicio: ejecución de Stencil1D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uKzTtEeISQw"
      },
      "source": [
        "En este ejercicio vamos a estudiar el efecto de la memoria compartida en una GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13R0IYfr599H"
      },
      "source": [
        "### Sin memoria compartida\n",
        "Primero realizamos una versión sin memoria compartida.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85-QjdtR5sDK",
        "outputId": "7724112a-c470-47a2-8ed1-d7ad6564709e"
      },
      "source": [
        "%%writefile stencil1d_base.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "#define RADIUS        3\n",
        "#define BLOCK_SIZE    256\n",
        "#define NUM_ELEMENTS  (4096*2)\n",
        "\n",
        "// CUDA API error checking macro\n",
        "#define cudaCheck(error) \\\n",
        "  if (error != cudaSuccess) { \\\n",
        "    printf(\"Fatal error: %s at %s:%d\\n\", \\\n",
        "      cudaGetErrorString(error), \\\n",
        "      __FILE__, __LINE__); \\\n",
        "    exit(1); \\\n",
        "  }\n",
        "\n",
        "__global__ void stencil_1d(int *in, int *out) \n",
        "{\n",
        "    // Just one global index \n",
        "    int index = threadIdx.x + (blockIdx.x * blockDim.x) + RADIUS;\n",
        "\n",
        "    // Apply the stencil\n",
        "    int result = 0;\n",
        "    for (int offset = -RADIUS ; offset <= RADIUS ; offset++)\n",
        "        result += in[index + offset];\n",
        "\n",
        "    // Store the result\n",
        "    out[index-RADIUS] = result;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  unsigned int i;\n",
        "  int h_in[NUM_ELEMENTS + 2 * RADIUS], h_out[NUM_ELEMENTS];\n",
        "  int *d_in, *d_out;\n",
        "\n",
        "  // Initialize host data\n",
        "  for( i = 0; i < (NUM_ELEMENTS + 2*RADIUS); ++i )\n",
        "    h_in[i] = 1; // With a value of 1 and RADIUS of 3, all output values should be 7\n",
        "\n",
        "  // Allocate space on the device\n",
        "  cudaCheck( cudaMalloc( &d_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int)) );\n",
        "  cudaCheck( cudaMalloc( &d_out, NUM_ELEMENTS * sizeof(int)) );\n",
        "\n",
        "  // Copy input data to device\n",
        "  cudaCheck( cudaMemcpy( d_in, h_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int), cudaMemcpyHostToDevice) );\n",
        "\n",
        "  stencil_1d<<< (NUM_ELEMENTS + BLOCK_SIZE - 1)/BLOCK_SIZE, BLOCK_SIZE >>> (d_in, d_out);\n",
        "\n",
        "  cudaCheck( cudaMemcpy( h_out, d_out, NUM_ELEMENTS * sizeof(int), cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // Verify every out value is 7\n",
        "  for( i = 0; i < NUM_ELEMENTS; ++i )\n",
        "    if (h_out[i] != 7)\n",
        "    {\n",
        "      printf(\"Element h_out[%d] == %d != 7\\n\", i, h_out[i]);\n",
        "      break;\n",
        "    }\n",
        "\n",
        "  if (i == NUM_ELEMENTS)\n",
        "    printf(\"SUCCESS!\\n\");\n",
        "\n",
        "  // Free out memory\n",
        "  cudaFree(d_in);\n",
        "  cudaFree(d_out);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing stencil1d_base.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fC5zcttL7KO_",
        "outputId": "7ea3e223-d30c-4229-d2a4-fa1472f41ecc"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true stencil1d_base.cu -o stencil1d_base -lcudadevrt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0QKT55-7J-L",
        "outputId": "3f013594-8f1a-4087-b435-5ff40b806378"
      },
      "source": [
        "for i in range(100):\n",
        "  !./stencil1d_base"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDyxs9Tg8BHy"
      },
      "source": [
        "Ejecutamos el código 100 veces y vemos que no ha ocurrido problema. Comprobamos la ejecución con $\\texttt{nvprof}$ y vemos que se dedica un tiempo considerable a la copia de elementos, por lo que podemos mejorar en problema con un uso eficiente de memoria compartida."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9olPv5qi8NVi",
        "outputId": "14f69b72-8903-4911-f304-cf2564e27683"
      },
      "source": [
        "!nvprof ./stencil1d_base "
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==6670== NVPROF is profiling process 6670, command: ./stencil1d_base\n",
            "==6670== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "SUCCESS!\n",
            "==6670== Profiling application: ./stencil1d_base\n",
            "==6670== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   44.18%  8.9920us         1  8.9920us  8.9920us  8.9920us  [CUDA memcpy HtoD]\n",
            "                   33.65%  6.8480us         1  6.8480us  6.8480us  6.8480us  [CUDA memcpy DtoH]\n",
            "                   22.17%  4.5120us         1  4.5120us  4.5120us  4.5120us  stencil_1d(int*, int*)\n",
            "      API calls:   99.44%  194.14ms         2  97.072ms  4.6390us  194.14ms  cudaMalloc\n",
            "                    0.27%  533.76us         1  533.76us  533.76us  533.76us  cuDeviceTotalMem\n",
            "                    0.11%  206.41us       101  2.0430us     160ns  98.306us  cuDeviceGetAttribute\n",
            "                    0.09%  184.01us         2  92.005us  12.488us  171.52us  cudaFree\n",
            "                    0.05%  90.978us         2  45.489us  39.812us  51.166us  cudaMemcpy\n",
            "                    0.02%  35.246us         1  35.246us  35.246us  35.246us  cudaLaunchKernel\n",
            "                    0.01%  24.450us         1  24.450us  24.450us  24.450us  cuDeviceGetName\n",
            "                    0.01%  17.621us         1  17.621us  17.621us  17.621us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.3160us         3     772ns     235ns  1.1040us  cuDeviceGetCount\n",
            "                    0.00%  1.5380us         2     769ns     354ns  1.1840us  cuDeviceGet\n",
            "                    0.00%     312ns         1     312ns     312ns     312ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6tDS3q28yAc"
      },
      "source": [
        "### Versión con memoria compartida, pero sin sincronización de threads\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGaG1nrF3hKZ",
        "outputId": "43a40edc-6323-4db6-8a69-cc4701cc543e"
      },
      "source": [
        "%%writefile stenciltest_sm_1.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "#define RADIUS        3\n",
        "#define BLOCK_SIZE    256\n",
        "#define NUM_ELEMENTS  (4096*2)\n",
        "\n",
        "// CUDA API error checking macro\n",
        "#define cudaCheck(error) \\\n",
        "  if (error != cudaSuccess) { \\\n",
        "    printf(\"Fatal error: %s at %s:%d\\n\", \\\n",
        "      cudaGetErrorString(error), \\\n",
        "      __FILE__, __LINE__); \\\n",
        "    exit(1); \\\n",
        "  }\n",
        "\n",
        "__global__ void stencil_1d(int *in, int *out) \n",
        "{\n",
        "    __shared__ int temp[BLOCK_SIZE + 2 * RADIUS];\n",
        "    int gindex = threadIdx.x + (blockIdx.x * blockDim.x) + RADIUS;\n",
        "    int lindex = threadIdx.x + RADIUS;\n",
        "\n",
        "    // Read input elements into shared memory\n",
        "    temp[lindex] = in[gindex];\n",
        "    if (threadIdx.x < RADIUS) \n",
        "    {\n",
        "        temp[lindex - RADIUS] = in[gindex - RADIUS];\n",
        "        temp[lindex + BLOCK_SIZE] = in[gindex + BLOCK_SIZE];\n",
        "    }\n",
        "\n",
        "    // Make sure all threads get to this point before proceeding!\n",
        "    //__syncthreads();\n",
        "\n",
        "    // Apply the stencil\n",
        "    int result = 0;\n",
        "    for (int offset = -RADIUS ; offset <= RADIUS ; offset++)\n",
        "        result += temp[lindex + offset];\n",
        "\n",
        "    // Store the result\n",
        "    out[gindex-RADIUS] = result;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  unsigned int i;\n",
        "  int h_in[NUM_ELEMENTS + 2 * RADIUS], h_out[NUM_ELEMENTS];\n",
        "  int *d_in, *d_out;\n",
        "\n",
        "  // Initialize host data\n",
        "  for( i = 0; i < (NUM_ELEMENTS + 2*RADIUS); ++i )\n",
        "    h_in[i] = 1; // With a value of 1 and RADIUS of 3, all output values should be 7\n",
        "\n",
        "  // Allocate space on the device\n",
        "  cudaCheck( cudaMalloc( &d_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int)) );\n",
        "  cudaCheck( cudaMalloc( &d_out, NUM_ELEMENTS * sizeof(int)) );\n",
        "\n",
        "  // Copy input data to device\n",
        "  cudaCheck( cudaMemcpy( d_in, h_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int), cudaMemcpyHostToDevice) );\n",
        "\n",
        "  stencil_1d<<< (NUM_ELEMENTS + BLOCK_SIZE - 1)/BLOCK_SIZE, BLOCK_SIZE >>> (d_in, d_out);\n",
        "\n",
        "  cudaCheck( cudaMemcpy( h_out, d_out, NUM_ELEMENTS * sizeof(int), cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // Verify every out value is 7\n",
        "  for( i = 0; i < NUM_ELEMENTS; ++i )\n",
        "    if (h_out[i] != 7)\n",
        "    {\n",
        "      printf(\"Element h_out[%d] == %d != 7\\n\", i, h_out[i]);\n",
        "      break;\n",
        "    }\n",
        "\n",
        "  if (i == NUM_ELEMENTS)\n",
        "    printf(\"SUCCESS!\\n\");\n",
        "\n",
        "  // Free out memory\n",
        "  cudaFree(d_in);\n",
        "  cudaFree(d_out);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting stenciltest_sm_1.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXahjwTKG4Xy",
        "outputId": "08a2f938-3d06-4976-d6c9-d45cc301e690"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true stenciltest_sm_1.cu -o ./stenciltest_sm_1 -lcudadevrt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4t7fl5s2IIMw",
        "outputId": "c0ec6b4b-ca37-442a-da9e-d1571e0ba851"
      },
      "source": [
        "for i in range(100):\n",
        "  !./stenciltest_sm_1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_VaWf_3EF-C",
        "outputId": "adda8353-036a-45bd-980e-c6fae350eb7d"
      },
      "source": [
        "!nvprof ./stenciltest_sm_1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==5471== NVPROF is profiling process 5471, command: ./stenciltest_sm_1\n",
            "==5471== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "SUCCESS!\n",
            "==5471== Profiling application: ./stenciltest_sm_1\n",
            "==5471== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   46.68%  8.9920us         1  8.9920us  8.9920us  8.9920us  [CUDA memcpy HtoD]\n",
            "                   35.05%  6.7520us         1  6.7520us  6.7520us  6.7520us  [CUDA memcpy DtoH]\n",
            "                   18.27%  3.5200us         1  3.5200us  3.5200us  3.5200us  stencil_1d(int*, int*)\n",
            "      API calls:   99.32%  177.25ms         2  88.625ms  7.0200us  177.24ms  cudaMalloc\n",
            "                    0.31%  544.53us         1  544.53us  544.53us  544.53us  cuDeviceTotalMem\n",
            "                    0.14%  254.84us       101  2.5230us     158ns  129.45us  cuDeviceGetAttribute\n",
            "                    0.10%  175.36us         2  87.678us  13.820us  161.54us  cudaFree\n",
            "                    0.08%  135.16us         2  67.577us  56.000us  79.155us  cudaMemcpy\n",
            "                    0.03%  62.297us         1  62.297us  62.297us  62.297us  cuDeviceGetName\n",
            "                    0.02%  35.636us         1  35.636us  35.636us  35.636us  cudaLaunchKernel\n",
            "                    0.00%  7.6460us         1  7.6460us  7.6460us  7.6460us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.7470us         3     915ns     250ns  1.3860us  cuDeviceGetCount\n",
            "                    0.00%  1.5520us         2     776ns     299ns  1.2530us  cuDeviceGet\n",
            "                    0.00%     269ns         1     269ns     269ns     269ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piM0l12_9z8f"
      },
      "source": [
        "En este caso vemos que existen errores ya que no se utiliza la función $\\texttt{__syncthreads()}$ para hacer que no exista la condición de carrera con los threads. Por ello procedemos a utilizar la función y conseguir que el efecto de la memoria compartida sea seguro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SEy0nUw-Rie"
      },
      "source": [
        "### Versión con memoria compartida y sincronía de threads."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76ur4eykFTkP",
        "outputId": "2d3023c7-8f6e-40d4-fea7-ee538c5fb4ad"
      },
      "source": [
        "%%writefile stenciltest_sm_2.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "#define RADIUS        3\n",
        "#define BLOCK_SIZE    256\n",
        "#define NUM_ELEMENTS  (4096*2)\n",
        "\n",
        "// CUDA API error checking macro\n",
        "#define cudaCheck(error) \\\n",
        "  if (error != cudaSuccess) { \\\n",
        "    printf(\"Fatal error: %s at %s:%d\\n\", \\\n",
        "      cudaGetErrorString(error), \\\n",
        "      __FILE__, __LINE__); \\\n",
        "    exit(1); \\\n",
        "  }\n",
        "\n",
        "__global__ void stencil_1d(int *in, int *out) \n",
        "{\n",
        "    __shared__ int temp[BLOCK_SIZE + 2 * RADIUS];\n",
        "    int gindex = threadIdx.x + (blockIdx.x * blockDim.x) + RADIUS;\n",
        "    int lindex = threadIdx.x + RADIUS;\n",
        "\n",
        "    // Read input elements into shared memory\n",
        "    temp[lindex] = in[gindex];\n",
        "    if (threadIdx.x < RADIUS) \n",
        "    {\n",
        "        temp[lindex - RADIUS] = in[gindex - RADIUS];\n",
        "        temp[lindex + BLOCK_SIZE] = in[gindex + BLOCK_SIZE];\n",
        "    }\n",
        "\n",
        "    // Make sure all threads get to this point before proceeding!\n",
        "    __syncthreads();\n",
        "\n",
        "    // Apply the stencil\n",
        "    int result = 0;\n",
        "    for (int offset = -RADIUS ; offset <= RADIUS ; offset++)\n",
        "        result += temp[lindex + offset];\n",
        "\n",
        "    // Store the result\n",
        "    out[gindex-RADIUS] = result;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  unsigned int i;\n",
        "  int h_in[NUM_ELEMENTS + 2 * RADIUS], h_out[NUM_ELEMENTS];\n",
        "  int *d_in, *d_out;\n",
        "\n",
        "  // Initialize host data\n",
        "  for( i = 0; i < (NUM_ELEMENTS + 2*RADIUS); ++i )\n",
        "    h_in[i] = 1; // With a value of 1 and RADIUS of 3, all output values should be 7\n",
        "\n",
        "  // Allocate space on the device\n",
        "  cudaCheck( cudaMalloc( &d_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int)) );\n",
        "  cudaCheck( cudaMalloc( &d_out, NUM_ELEMENTS * sizeof(int)) );\n",
        "\n",
        "  // Copy input data to device\n",
        "  cudaCheck( cudaMemcpy( d_in, h_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int), cudaMemcpyHostToDevice) );\n",
        "\n",
        "  stencil_1d<<< (NUM_ELEMENTS + BLOCK_SIZE - 1)/BLOCK_SIZE, BLOCK_SIZE >>> (d_in, d_out);\n",
        "\n",
        "  cudaCheck( cudaMemcpy( h_out, d_out, NUM_ELEMENTS * sizeof(int), cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // Verify every out value is 7\n",
        "  for( i = 0; i < NUM_ELEMENTS; ++i )\n",
        "    if (h_out[i] != 7)\n",
        "    {\n",
        "      printf(\"Element h_out[%d] == %d != 7\\n\", i, h_out[i]);\n",
        "      break;\n",
        "    }\n",
        "\n",
        "  if (i == NUM_ELEMENTS)\n",
        "    printf(\"SUCCESS!\\n\");\n",
        "\n",
        "  // Free out memory\n",
        "  cudaFree(d_in);\n",
        "  cudaFree(d_out);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing stenciltest_sm_2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CamqRgGsHxU-",
        "outputId": "66774288-31b6-466c-de98-f6d20bbd872d"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true stenciltest_sm_2.cu -o ./stenciltest_sm_2 -lcudadevrt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9tmF1F4H1jY",
        "outputId": "e01cbfbc-5ceb-44f3-b4f3-be1023329173"
      },
      "source": [
        "for i in range(100):\n",
        "  !./stenciltest_sm_2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bodklbXLH-LF",
        "outputId": "6f28572c-8ac2-4bb2-f747-af338184fdac"
      },
      "source": [
        "!nvprof ./stenciltest_sm_2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==7068== NVPROF is profiling process 7068, command: ./stenciltest_sm_2\n",
            "==7068== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "==7068== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
            "SUCCESS!\n",
            "==7068== Profiling application: ./stenciltest_sm_2\n",
            "==7068== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   45.80%  9.0560us         1  9.0560us  9.0560us  9.0560us  [CUDA memcpy HtoD]\n",
            "                   34.14%  6.7520us         1  6.7520us  6.7520us  6.7520us  [CUDA memcpy DtoH]\n",
            "                   20.06%  3.9670us         1  3.9670us  3.9670us  3.9670us  stencil_1d(int*, int*)\n",
            "      API calls:   99.44%  179.65ms         2  89.823ms  9.0080us  179.64ms  cudaMalloc\n",
            "                    0.28%  497.58us         1  497.58us  497.58us  497.58us  cuDeviceTotalMem\n",
            "                    0.10%  181.25us       101  1.7940us     146ns  71.646us  cuDeviceGetAttribute\n",
            "                    0.08%  137.44us         2  68.719us  15.557us  121.88us  cudaFree\n",
            "                    0.05%  87.153us         2  43.576us  29.600us  57.553us  cudaMemcpy\n",
            "                    0.03%  57.506us         1  57.506us  57.506us  57.506us  cudaLaunchKernel\n",
            "                    0.02%  32.587us         1  32.587us  32.587us  32.587us  cuDeviceGetName\n",
            "                    0.01%  17.291us         1  17.291us  17.291us  17.291us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.8820us         3     960ns     194ns  1.5100us  cuDeviceGetCount\n",
            "                    0.00%  1.9540us         2     977ns     408ns  1.5460us  cuDeviceGet\n",
            "                    0.00%     273ns         1     273ns     273ns     273ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9VI8UK6-pl9"
      },
      "source": [
        "Gracias a la utilización de la barrera se garantiza que todos los threads lleguen a tiempo y se realice una correcta ejecución. \n",
        "\n",
        "Vemos que en este caso no hay una diferencia notable en tiempos de ejecución, lo que puede deberse a un tamaño bajo de radio y vector. Cambiamos los valores y comparamos con la versión base y con la de memoria compartida con sincronía de threads."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyZfMttJ_5RL"
      },
      "source": [
        "!sed -i '/#define RADIUS/c\\#define RADIUS 5' stencil1d_base.cu\n",
        "!sed -i '/#define NUM_ELEMENTS/c\\#define NUM_ELEMENTS (4096*16)' stencil1d_base.cu\n",
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true stencil1d_base.cu -o stencil1d_base -lcudadevrt\n",
        "!nvprof ./stencil1d_base"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrlPuv4M_5LB"
      },
      "source": [
        "!sed -i '/#define RADIUS/c\\#define RADIUS 5' stenciltest_sm_2.cu\n",
        "!sed -i '/#define NUM_ELEMENTS/c\\#define NUM_ELEMENTS (4096*16)' stenciltest_sm_2.cu\n",
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true stenciltest_sm_2.cu -o stenciltest_sm_2 -lcudadevrt\n",
        "!nvprof ./stenciltest_sm_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU0BHCiQJIEd"
      },
      "source": [
        "#EJERCICIO OPCIONAL: Producto de matrices en GPU\n",
        "\n",
        "En este código, solo se utiliza un bloque con 16x16 threads y el reparto de trabajo a los threads se ha definido de manera bidimensional.\n",
        "En la ejecución serie de la CPU las operaciones realizadas son $O(N^3)$ mientras que para la GPU se ha reducido a $O(N^2)$ las operaciones que realiza cada thread.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOQ4Bjol1DRq"
      },
      "source": [
        "Primero ejecutamos el algoritmo proporcionado en CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HL_ZsWh6_Dy",
        "outputId": "be2645c8-62a6-440d-8a15-0eb4562294c0"
      },
      "source": [
        "%%writefile matrixMul_cpu.cu\n",
        "#include <stdio.h>\n",
        "#define N 16\n",
        "#define M 1\n",
        "\n",
        "void matrixMultCPU(int a[N][N], int b[N][N], int c[N][N]){\n",
        "    int n, m;\n",
        "    for (int i = 0; i < N; i++){\n",
        "        for (int j = 0; j < N; j++){\n",
        "            int sum = 0;\n",
        "            for (int k = 0; k < N; k++){\n",
        "                m = a[i][k];\n",
        "                n = b[k][j];\n",
        "                sum += m*n;\n",
        "            }\n",
        "            c[i][j] = sum;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    int a[N][N], b[N][N], c[N][N];\n",
        "    int cont, i , j;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    for (i = 0; i < N; i++){\n",
        "        cont = 0;\n",
        "        for (j = 0; j < N; j++){\n",
        "            a[i][j] = cont;\n",
        "            b[i][j] = cont;\n",
        "            cont++;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    int nIter = 1000;\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    for (int m = 0; m < nIter; m++){\n",
        "        matrixMultCPU(a, b, c);\n",
        "    }\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float msecTotal = 0.0;\n",
        "    cudaEventElapsedTime(&msecTotal, start, stop);\n",
        "\n",
        "    for (int y = 0; y < N; y++){\n",
        "        for (int x = 0; x < N; x++){\n",
        "            printf(\"[%d][%d]=%d \", y, x, c[x][y]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    float msecPerKernelExecution = msecTotal / nIter;\n",
        "    double flopsPerMMull = 2.0 * N * N * N;\n",
        "    double gigaFlops = (flopsPerMMull * 1.0e-9f) / (msecPerKernelExecution / 1000.0f);\n",
        "\n",
        "    printf(\"GigaFlops: %f\", gigaFlops);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing matrixMul_cpu.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucU3ablX7fvi",
        "outputId": "ad3d186a-632c-48d0-8581-78efdee80fae"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true matrixMul_cpu.cu -o matrixMul_cpu -lcudadevrt\n",
        "!nvprof ./matrixMul_cpu"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==1626== NVPROF is profiling process 1626, command: ./matrixMul_cpu\n",
            "==1626== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "[0][0]=0 [0][1]=0 [0][2]=0 [0][3]=0 [0][4]=0 [0][5]=0 [0][6]=0 [0][7]=0 [0][8]=0 [0][9]=0 [0][10]=0 [0][11]=0 [0][12]=0 [0][13]=0 [0][14]=0 [0][15]=0 \n",
            "[1][0]=120 [1][1]=120 [1][2]=120 [1][3]=120 [1][4]=120 [1][5]=120 [1][6]=120 [1][7]=120 [1][8]=120 [1][9]=120 [1][10]=120 [1][11]=120 [1][12]=120 [1][13]=120 [1][14]=120 [1][15]=120 \n",
            "[2][0]=240 [2][1]=240 [2][2]=240 [2][3]=240 [2][4]=240 [2][5]=240 [2][6]=240 [2][7]=240 [2][8]=240 [2][9]=240 [2][10]=240 [2][11]=240 [2][12]=240 [2][13]=240 [2][14]=240 [2][15]=240 \n",
            "[3][0]=360 [3][1]=360 [3][2]=360 [3][3]=360 [3][4]=360 [3][5]=360 [3][6]=360 [3][7]=360 [3][8]=360 [3][9]=360 [3][10]=360 [3][11]=360 [3][12]=360 [3][13]=360 [3][14]=360 [3][15]=360 \n",
            "[4][0]=480 [4][1]=480 [4][2]=480 [4][3]=480 [4][4]=480 [4][5]=480 [4][6]=480 [4][7]=480 [4][8]=480 [4][9]=480 [4][10]=480 [4][11]=480 [4][12]=480 [4][13]=480 [4][14]=480 [4][15]=480 \n",
            "[5][0]=600 [5][1]=600 [5][2]=600 [5][3]=600 [5][4]=600 [5][5]=600 [5][6]=600 [5][7]=600 [5][8]=600 [5][9]=600 [5][10]=600 [5][11]=600 [5][12]=600 [5][13]=600 [5][14]=600 [5][15]=600 \n",
            "[6][0]=720 [6][1]=720 [6][2]=720 [6][3]=720 [6][4]=720 [6][5]=720 [6][6]=720 [6][7]=720 [6][8]=720 [6][9]=720 [6][10]=720 [6][11]=720 [6][12]=720 [6][13]=720 [6][14]=720 [6][15]=720 \n",
            "[7][0]=840 [7][1]=840 [7][2]=840 [7][3]=840 [7][4]=840 [7][5]=840 [7][6]=840 [7][7]=840 [7][8]=840 [7][9]=840 [7][10]=840 [7][11]=840 [7][12]=840 [7][13]=840 [7][14]=840 [7][15]=840 \n",
            "[8][0]=960 [8][1]=960 [8][2]=960 [8][3]=960 [8][4]=960 [8][5]=960 [8][6]=960 [8][7]=960 [8][8]=960 [8][9]=960 [8][10]=960 [8][11]=960 [8][12]=960 [8][13]=960 [8][14]=960 [8][15]=960 \n",
            "[9][0]=1080 [9][1]=1080 [9][2]=1080 [9][3]=1080 [9][4]=1080 [9][5]=1080 [9][6]=1080 [9][7]=1080 [9][8]=1080 [9][9]=1080 [9][10]=1080 [9][11]=1080 [9][12]=1080 [9][13]=1080 [9][14]=1080 [9][15]=1080 \n",
            "[10][0]=1200 [10][1]=1200 [10][2]=1200 [10][3]=1200 [10][4]=1200 [10][5]=1200 [10][6]=1200 [10][7]=1200 [10][8]=1200 [10][9]=1200 [10][10]=1200 [10][11]=1200 [10][12]=1200 [10][13]=1200 [10][14]=1200 [10][15]=1200 \n",
            "[11][0]=1320 [11][1]=1320 [11][2]=1320 [11][3]=1320 [11][4]=1320 [11][5]=1320 [11][6]=1320 [11][7]=1320 [11][8]=1320 [11][9]=1320 [11][10]=1320 [11][11]=1320 [11][12]=1320 [11][13]=1320 [11][14]=1320 [11][15]=1320 \n",
            "[12][0]=1440 [12][1]=1440 [12][2]=1440 [12][3]=1440 [12][4]=1440 [12][5]=1440 [12][6]=1440 [12][7]=1440 [12][8]=1440 [12][9]=1440 [12][10]=1440 [12][11]=1440 [12][12]=1440 [12][13]=1440 [12][14]=1440 [12][15]=1440 \n",
            "[13][0]=1560 [13][1]=1560 [13][2]=1560 [13][3]=1560 [13][4]=1560 [13][5]=1560 [13][6]=1560 [13][7]=1560 [13][8]=1560 [13][9]=1560 [13][10]=1560 [13][11]=1560 [13][12]=1560 [13][13]=1560 [13][14]=1560 [13][15]=1560 \n",
            "[14][0]=1680 [14][1]=1680 [14][2]=1680 [14][3]=1680 [14][4]=1680 [14][5]=1680 [14][6]=1680 [14][7]=1680 [14][8]=1680 [14][9]=1680 [14][10]=1680 [14][11]=1680 [14][12]=1680 [14][13]=1680 [14][14]=1680 [14][15]=1680 \n",
            "[15][0]=1800 [15][1]=1800 [15][2]=1800 [15][3]=1800 [15][4]=1800 [15][5]=1800 [15][6]=1800 [15][7]=1800 [15][8]=1800 [15][9]=1800 [15][10]=1800 [15][11]=1800 [15][12]=1800 [15][13]=1800 [15][14]=1800 [15][15]=1800 \n",
            "GigaFlops: 0.546125==1626== Profiling application: ./matrixMul_cpu\n",
            "==1626== Profiling result:\n",
            "No kernels were profiled.\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            "      API calls:   99.57%  192.85ms         2  96.423ms  1.0970us  192.85ms  cudaEventCreate\n",
            "                    0.25%  493.56us         1  493.56us  493.56us  493.56us  cuDeviceTotalMem\n",
            "                    0.13%  257.35us       101  2.5480us     143ns  138.12us  cuDeviceGetAttribute\n",
            "                    0.02%  30.278us         1  30.278us  30.278us  30.278us  cuDeviceGetName\n",
            "                    0.01%  17.863us         2  8.9310us  7.1330us  10.730us  cudaEventRecord\n",
            "                    0.01%  17.202us         1  17.202us  17.202us  17.202us  cudaEventSynchronize\n",
            "                    0.00%  5.5450us         1  5.5450us  5.5450us  5.5450us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.5170us         1  2.5170us  2.5170us  2.5170us  cudaEventElapsedTime\n",
            "                    0.00%  2.0570us         3     685ns     233ns  1.1990us  cuDeviceGetCount\n",
            "                    0.00%  2.0230us         2  1.0110us     358ns  1.6650us  cuDeviceGet\n",
            "                    0.00%     320ns         1     320ns     320ns     320ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlgsgPbl7oso"
      },
      "source": [
        "Ahora ejecutamos el ejemplo para GPU usando memoria global."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjbJNj7jvEcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4252e9a0-a04b-4ce2-9501-fd13be434813"
      },
      "source": [
        "%%writefile matrixMul_gpu.cu\n",
        "#include <stdio.h>\n",
        "#define N 16\n",
        "#define M 1\n",
        "\n",
        "__global__ void matrixMultGPU(int *a, int *b, int *c){\n",
        "    int k, sum = 0;\n",
        "    int col = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "    int fil = threadIdx.y + blockDim.y * blockIdx.y;\n",
        "    if (col < N && fil < N){\n",
        "        for (k = 0; k < N; k++){\n",
        "            sum += a[fil*N+k] * b[k*N+col];\n",
        "        }\n",
        "        c[fil*N+col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    int a[N][N], b[N][N], c[N][N];\n",
        "    int *dev_a, *dev_b, *dev_c;\n",
        "    int cont, i , j;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    for (i = 0; i < N; i++){\n",
        "        cont = 0;\n",
        "        for (j = 0; j < N; j++){\n",
        "            a[i][j] = cont;\n",
        "            b[i][j] = cont;\n",
        "            cont++;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    int size = N*N*sizeof(int);\n",
        "    cudaMalloc((void **) &dev_a, size);\n",
        "    cudaMalloc((void **) &dev_b, size);\n",
        "    cudaMalloc((void **) &dev_c, size);\n",
        "\n",
        "    cudaMemcpy(dev_a, a, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 dimGrid(M, M);\n",
        "    dim3 dimBlock(N, N);\n",
        "\n",
        "    int nIter = 1000;\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    for (int m = 0; m < nIter; m++){\n",
        "        matrixMultGPU<<<dimGrid, dimBlock>>>(dev_a, dev_b, dev_c);\n",
        "    }\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    cudaMemcpy(c, dev_c, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    float msecTotal = 0.0;\n",
        "    cudaEventElapsedTime(&msecTotal, start, stop);\n",
        "\n",
        "    cudaFree(dev_a);\n",
        "    cudaFree(dev_b);\n",
        "    cudaFree(dev_c);\n",
        "\n",
        "    for (int y = 0; y < N; y++){\n",
        "        for (int x = 0; x < N; x++){\n",
        "            printf(\"[%d][%d]=%d \", y, x, c[x][y]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    float msecPerKernelExecution = msecTotal / nIter;\n",
        "    double flopsPerMMull = 2.0 * N * N * N;\n",
        "    double gigaFlops = (flopsPerMMull * 1.0e-9f) / (msecPerKernelExecution / 1000.0f);\n",
        "\n",
        "    printf(\"GigaFlops: %f\", gigaFlops);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matrixMul_gpu.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdBDoJo6weQ1",
        "outputId": "2924ef2b-51fb-4b69-860d-54de63b0fb77"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true matrixMul_gpu.cu -o matrixMul_gpu -lcudadevrt\n",
        "!./matrixMul_gpu"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "[0][0]=0 [0][1]=0 [0][2]=0 [0][3]=0 [0][4]=0 [0][5]=0 [0][6]=0 [0][7]=0 [0][8]=0 [0][9]=0 [0][10]=0 [0][11]=0 [0][12]=0 [0][13]=0 [0][14]=0 [0][15]=0 \n",
            "[1][0]=120 [1][1]=120 [1][2]=120 [1][3]=120 [1][4]=120 [1][5]=120 [1][6]=120 [1][7]=120 [1][8]=120 [1][9]=120 [1][10]=120 [1][11]=120 [1][12]=120 [1][13]=120 [1][14]=120 [1][15]=120 \n",
            "[2][0]=240 [2][1]=240 [2][2]=240 [2][3]=240 [2][4]=240 [2][5]=240 [2][6]=240 [2][7]=240 [2][8]=240 [2][9]=240 [2][10]=240 [2][11]=240 [2][12]=240 [2][13]=240 [2][14]=240 [2][15]=240 \n",
            "[3][0]=360 [3][1]=360 [3][2]=360 [3][3]=360 [3][4]=360 [3][5]=360 [3][6]=360 [3][7]=360 [3][8]=360 [3][9]=360 [3][10]=360 [3][11]=360 [3][12]=360 [3][13]=360 [3][14]=360 [3][15]=360 \n",
            "[4][0]=480 [4][1]=480 [4][2]=480 [4][3]=480 [4][4]=480 [4][5]=480 [4][6]=480 [4][7]=480 [4][8]=480 [4][9]=480 [4][10]=480 [4][11]=480 [4][12]=480 [4][13]=480 [4][14]=480 [4][15]=480 \n",
            "[5][0]=600 [5][1]=600 [5][2]=600 [5][3]=600 [5][4]=600 [5][5]=600 [5][6]=600 [5][7]=600 [5][8]=600 [5][9]=600 [5][10]=600 [5][11]=600 [5][12]=600 [5][13]=600 [5][14]=600 [5][15]=600 \n",
            "[6][0]=720 [6][1]=720 [6][2]=720 [6][3]=720 [6][4]=720 [6][5]=720 [6][6]=720 [6][7]=720 [6][8]=720 [6][9]=720 [6][10]=720 [6][11]=720 [6][12]=720 [6][13]=720 [6][14]=720 [6][15]=720 \n",
            "[7][0]=840 [7][1]=840 [7][2]=840 [7][3]=840 [7][4]=840 [7][5]=840 [7][6]=840 [7][7]=840 [7][8]=840 [7][9]=840 [7][10]=840 [7][11]=840 [7][12]=840 [7][13]=840 [7][14]=840 [7][15]=840 \n",
            "[8][0]=960 [8][1]=960 [8][2]=960 [8][3]=960 [8][4]=960 [8][5]=960 [8][6]=960 [8][7]=960 [8][8]=960 [8][9]=960 [8][10]=960 [8][11]=960 [8][12]=960 [8][13]=960 [8][14]=960 [8][15]=960 \n",
            "[9][0]=1080 [9][1]=1080 [9][2]=1080 [9][3]=1080 [9][4]=1080 [9][5]=1080 [9][6]=1080 [9][7]=1080 [9][8]=1080 [9][9]=1080 [9][10]=1080 [9][11]=1080 [9][12]=1080 [9][13]=1080 [9][14]=1080 [9][15]=1080 \n",
            "[10][0]=1200 [10][1]=1200 [10][2]=1200 [10][3]=1200 [10][4]=1200 [10][5]=1200 [10][6]=1200 [10][7]=1200 [10][8]=1200 [10][9]=1200 [10][10]=1200 [10][11]=1200 [10][12]=1200 [10][13]=1200 [10][14]=1200 [10][15]=1200 \n",
            "[11][0]=1320 [11][1]=1320 [11][2]=1320 [11][3]=1320 [11][4]=1320 [11][5]=1320 [11][6]=1320 [11][7]=1320 [11][8]=1320 [11][9]=1320 [11][10]=1320 [11][11]=1320 [11][12]=1320 [11][13]=1320 [11][14]=1320 [11][15]=1320 \n",
            "[12][0]=1440 [12][1]=1440 [12][2]=1440 [12][3]=1440 [12][4]=1440 [12][5]=1440 [12][6]=1440 [12][7]=1440 [12][8]=1440 [12][9]=1440 [12][10]=1440 [12][11]=1440 [12][12]=1440 [12][13]=1440 [12][14]=1440 [12][15]=1440 \n",
            "[13][0]=1560 [13][1]=1560 [13][2]=1560 [13][3]=1560 [13][4]=1560 [13][5]=1560 [13][6]=1560 [13][7]=1560 [13][8]=1560 [13][9]=1560 [13][10]=1560 [13][11]=1560 [13][12]=1560 [13][13]=1560 [13][14]=1560 [13][15]=1560 \n",
            "[14][0]=1680 [14][1]=1680 [14][2]=1680 [14][3]=1680 [14][4]=1680 [14][5]=1680 [14][6]=1680 [14][7]=1680 [14][8]=1680 [14][9]=1680 [14][10]=1680 [14][11]=1680 [14][12]=1680 [14][13]=1680 [14][14]=1680 [14][15]=1680 \n",
            "[15][0]=1800 [15][1]=1800 [15][2]=1800 [15][3]=1800 [15][4]=1800 [15][5]=1800 [15][6]=1800 [15][7]=1800 [15][8]=1800 [15][9]=1800 [15][10]=1800 [15][11]=1800 [15][12]=1800 [15][13]=1800 [15][14]=1800 [15][15]=1800 \n",
            "GigaFlops: 0.801867"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmENwJd7x-Ai",
        "outputId": "267993e6-4d7a-4d29-8636-40b016ee8c44"
      },
      "source": [
        "!nvprof ./matrixMul_gpu"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==1219== NVPROF is profiling process 1219, command: ./matrixMul_gpu\n",
            "==1219== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "[0][0]=0 [0][1]=0 [0][2]=0 [0][3]=0 [0][4]=0 [0][5]=0 [0][6]=0 [0][7]=0 [0][8]=0 [0][9]=0 [0][10]=0 [0][11]=0 [0][12]=0 [0][13]=0 [0][14]=0 [0][15]=0 \n",
            "[1][0]=120 [1][1]=120 [1][2]=120 [1][3]=120 [1][4]=120 [1][5]=120 [1][6]=120 [1][7]=120 [1][8]=120 [1][9]=120 [1][10]=120 [1][11]=120 [1][12]=120 [1][13]=120 [1][14]=120 [1][15]=120 \n",
            "[2][0]=240 [2][1]=240 [2][2]=240 [2][3]=240 [2][4]=240 [2][5]=240 [2][6]=240 [2][7]=240 [2][8]=240 [2][9]=240 [2][10]=240 [2][11]=240 [2][12]=240 [2][13]=240 [2][14]=240 [2][15]=240 \n",
            "[3][0]=360 [3][1]=360 [3][2]=360 [3][3]=360 [3][4]=360 [3][5]=360 [3][6]=360 [3][7]=360 [3][8]=360 [3][9]=360 [3][10]=360 [3][11]=360 [3][12]=360 [3][13]=360 [3][14]=360 [3][15]=360 \n",
            "[4][0]=480 [4][1]=480 [4][2]=480 [4][3]=480 [4][4]=480 [4][5]=480 [4][6]=480 [4][7]=480 [4][8]=480 [4][9]=480 [4][10]=480 [4][11]=480 [4][12]=480 [4][13]=480 [4][14]=480 [4][15]=480 \n",
            "[5][0]=600 [5][1]=600 [5][2]=600 [5][3]=600 [5][4]=600 [5][5]=600 [5][6]=600 [5][7]=600 [5][8]=600 [5][9]=600 [5][10]=600 [5][11]=600 [5][12]=600 [5][13]=600 [5][14]=600 [5][15]=600 \n",
            "[6][0]=720 [6][1]=720 [6][2]=720 [6][3]=720 [6][4]=720 [6][5]=720 [6][6]=720 [6][7]=720 [6][8]=720 [6][9]=720 [6][10]=720 [6][11]=720 [6][12]=720 [6][13]=720 [6][14]=720 [6][15]=720 \n",
            "[7][0]=840 [7][1]=840 [7][2]=840 [7][3]=840 [7][4]=840 [7][5]=840 [7][6]=840 [7][7]=840 [7][8]=840 [7][9]=840 [7][10]=840 [7][11]=840 [7][12]=840 [7][13]=840 [7][14]=840 [7][15]=840 \n",
            "[8][0]=960 [8][1]=960 [8][2]=960 [8][3]=960 [8][4]=960 [8][5]=960 [8][6]=960 [8][7]=960 [8][8]=960 [8][9]=960 [8][10]=960 [8][11]=960 [8][12]=960 [8][13]=960 [8][14]=960 [8][15]=960 \n",
            "[9][0]=1080 [9][1]=1080 [9][2]=1080 [9][3]=1080 [9][4]=1080 [9][5]=1080 [9][6]=1080 [9][7]=1080 [9][8]=1080 [9][9]=1080 [9][10]=1080 [9][11]=1080 [9][12]=1080 [9][13]=1080 [9][14]=1080 [9][15]=1080 \n",
            "[10][0]=1200 [10][1]=1200 [10][2]=1200 [10][3]=1200 [10][4]=1200 [10][5]=1200 [10][6]=1200 [10][7]=1200 [10][8]=1200 [10][9]=1200 [10][10]=1200 [10][11]=1200 [10][12]=1200 [10][13]=1200 [10][14]=1200 [10][15]=1200 \n",
            "[11][0]=1320 [11][1]=1320 [11][2]=1320 [11][3]=1320 [11][4]=1320 [11][5]=1320 [11][6]=1320 [11][7]=1320 [11][8]=1320 [11][9]=1320 [11][10]=1320 [11][11]=1320 [11][12]=1320 [11][13]=1320 [11][14]=1320 [11][15]=1320 \n",
            "[12][0]=1440 [12][1]=1440 [12][2]=1440 [12][3]=1440 [12][4]=1440 [12][5]=1440 [12][6]=1440 [12][7]=1440 [12][8]=1440 [12][9]=1440 [12][10]=1440 [12][11]=1440 [12][12]=1440 [12][13]=1440 [12][14]=1440 [12][15]=1440 \n",
            "[13][0]=1560 [13][1]=1560 [13][2]=1560 [13][3]=1560 [13][4]=1560 [13][5]=1560 [13][6]=1560 [13][7]=1560 [13][8]=1560 [13][9]=1560 [13][10]=1560 [13][11]=1560 [13][12]=1560 [13][13]=1560 [13][14]=1560 [13][15]=1560 \n",
            "[14][0]=1680 [14][1]=1680 [14][2]=1680 [14][3]=1680 [14][4]=1680 [14][5]=1680 [14][6]=1680 [14][7]=1680 [14][8]=1680 [14][9]=1680 [14][10]=1680 [14][11]=1680 [14][12]=1680 [14][13]=1680 [14][14]=1680 [14][15]=1680 \n",
            "[15][0]=1800 [15][1]=1800 [15][2]=1800 [15][3]=1800 [15][4]=1800 [15][5]=1800 [15][6]=1800 [15][7]=1800 [15][8]=1800 [15][9]=1800 [15][10]=1800 [15][11]=1800 [15][12]=1800 [15][13]=1800 [15][14]=1800 [15][15]=1800 \n",
            "GigaFlops: 0.800878==1219== Profiling application: ./matrixMul_gpu\n",
            "==1219== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.87%  5.1994ms      1000  5.1990us  4.9590us  11.840us  matrixMultGPU(int*, int*, int*)\n",
            "                    0.08%  4.1280us         2  2.0640us  1.7600us  2.3680us  [CUDA memcpy HtoD]\n",
            "                    0.05%  2.5280us         1  2.5280us  2.5280us  2.5280us  [CUDA memcpy DtoH]\n",
            "      API calls:   94.75%  197.73ms         2  98.866ms     882ns  197.73ms  cudaEventCreate\n",
            "                    2.55%  5.3196ms         1  5.3196ms  5.3196ms  5.3196ms  cudaEventSynchronize\n",
            "                    2.20%  4.5844ms      1000  4.5840us  3.2950us  33.759us  cudaLaunchKernel\n",
            "                    0.22%  452.76us         1  452.76us  452.76us  452.76us  cuDeviceTotalMem\n",
            "                    0.09%  178.59us         3  59.531us  1.9640us  173.33us  cudaMalloc\n",
            "                    0.08%  174.73us       101  1.7300us     127ns  75.439us  cuDeviceGetAttribute\n",
            "                    0.06%  134.67us         3  44.891us  2.5250us  124.63us  cudaFree\n",
            "                    0.03%  55.124us         3  18.374us  12.616us  26.229us  cudaMemcpy\n",
            "                    0.01%  27.316us         1  27.316us  27.316us  27.316us  cuDeviceGetName\n",
            "                    0.00%  6.6350us         2  3.3170us  2.8760us  3.7590us  cudaEventRecord\n",
            "                    0.00%  6.5030us         1  6.5030us  6.5030us  6.5030us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.4050us         1  2.4050us  2.4050us  2.4050us  cudaEventElapsedTime\n",
            "                    0.00%  1.9680us         3     656ns     221ns  1.1530us  cuDeviceGetCount\n",
            "                    0.00%  1.3210us         2     660ns     289ns  1.0320us  cuDeviceGet\n",
            "                    0.00%     272ns         1     272ns     272ns     272ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcRkQU69C9pN"
      },
      "source": [
        "Vemos que el tiempo de ejecución es mucho mejor en GPU. Ahora variamos el número de hilos por bloque y ajustamos el tamaño de particionado, al igual que hicimos en el ejemplo de la suma de vectores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcmLBwho75SE",
        "outputId": "39cdd388-5145-4d96-d46a-595d5d6a63e7"
      },
      "source": [
        "%%writefile matrixMul_gpu2.cu\n",
        "#include <stdio.h>\n",
        "#define N 64\n",
        "#define MAX 8\n",
        "\n",
        "__global__ void matrixMultGPU(int *a, int *b, int *c){\n",
        "    int k, sum = 0;\n",
        "    int col = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "    int fil = threadIdx.y + blockDim.y * blockIdx.y;\n",
        "    if (col < N && fil < N){\n",
        "        for (k = 0; k < N; k++){\n",
        "            sum += a[fil*N+k] * b[k*N+col];\n",
        "        }\n",
        "        c[fil*N+col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    int a[N][N], b[N][N], c[N][N];\n",
        "    int *dev_a, *dev_b, *dev_c;\n",
        "    int cont, i , j;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    for (i = 0; i < N; i++){\n",
        "        cont = 0;\n",
        "        for (j = 0; j < N; j++){\n",
        "            a[i][j] = cont;\n",
        "            b[i][j] = cont;\n",
        "            cont++;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    int size = N*N*sizeof(int);\n",
        "    cudaMalloc((void **) &dev_a, size);\n",
        "    cudaMalloc((void **) &dev_b, size);\n",
        "    cudaMalloc((void **) &dev_c, size);\n",
        "\n",
        "    cudaMemcpy(dev_a, a, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 dimGrid((N + MAX - 1)/MAX, (N + MAX - 1)/MAX);\n",
        "    dim3 dimBlock(MAX, MAX);\n",
        "\n",
        "    int nIter = 1000;\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    for (int m = 0; m < nIter; m++){\n",
        "        matrixMultGPU<<<dimGrid, dimBlock>>>(dev_a, dev_b, dev_c);\n",
        "    }\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    cudaMemcpy(c, dev_c, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    float msecTotal = 0.0;\n",
        "    cudaEventElapsedTime(&msecTotal, start, stop);\n",
        "\n",
        "    cudaFree(dev_a);\n",
        "    cudaFree(dev_b);\n",
        "    cudaFree(dev_c);\n",
        "\n",
        "    //for (int y = 0; y < N; y++){\n",
        "    //    for (int x = 0; x < N; x++){\n",
        "    //        printf(\"[%d][%d]=%d \", y, x, c[x][y]);\n",
        "    //    }\n",
        "    //    printf(\"\\n\");\n",
        "    //}\n",
        "\n",
        "    float msecPerKernelExecution = msecTotal / nIter;\n",
        "    double flopsPerMMull = 2.0 * N * N * N;\n",
        "    double gigaFlops = (flopsPerMMull * 1.0e-9f) / (msecPerKernelExecution / 1000.0f);\n",
        "\n",
        "    printf(\"GigaFlops: %f\", gigaFlops);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matrixMul_gpu2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxEbuaBF81Hl",
        "outputId": "c6ac96fc-7579-4e06-c05c-e9a1ba09f4ad"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true matrixMul_gpu2.cu -o matrixMul_gpu2 -lcudadevrt\n",
        "!nvprof ./matrixMul_gpu2"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==1564== NVPROF is profiling process 1564, command: ./matrixMul_gpu2\n",
            "==1564== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "[0][0]=0 [0][1]=0 [0][2]=0 [0][3]=0 [0][4]=0 [0][5]=0 [0][6]=0 [0][7]=0 [0][8]=0 [0][9]=0 [0][10]=0 [0][11]=0 [0][12]=0 [0][13]=0 [0][14]=0 [0][15]=0 [0][16]=0 [0][17]=0 [0][18]=0 [0][19]=0 [0][20]=0 [0][21]=0 [0][22]=0 [0][23]=0 [0][24]=0 [0][25]=0 [0][26]=0 [0][27]=0 [0][28]=0 [0][29]=0 [0][30]=0 [0][31]=0 [0][32]=0 [0][33]=0 [0][34]=0 [0][35]=0 [0][36]=0 [0][37]=0 [0][38]=0 [0][39]=0 [0][40]=0 [0][41]=0 [0][42]=0 [0][43]=0 [0][44]=0 [0][45]=0 [0][46]=0 [0][47]=0 [0][48]=0 [0][49]=0 [0][50]=0 [0][51]=0 [0][52]=0 [0][53]=0 [0][54]=0 [0][55]=0 [0][56]=0 [0][57]=0 [0][58]=0 [0][59]=0 [0][60]=0 [0][61]=0 [0][62]=0 [0][63]=0 \n",
            "[1][0]=2016 [1][1]=2016 [1][2]=2016 [1][3]=2016 [1][4]=2016 [1][5]=2016 [1][6]=2016 [1][7]=2016 [1][8]=2016 [1][9]=2016 [1][10]=2016 [1][11]=2016 [1][12]=2016 [1][13]=2016 [1][14]=2016 [1][15]=2016 [1][16]=2016 [1][17]=2016 [1][18]=2016 [1][19]=2016 [1][20]=2016 [1][21]=2016 [1][22]=2016 [1][23]=2016 [1][24]=2016 [1][25]=2016 [1][26]=2016 [1][27]=2016 [1][28]=2016 [1][29]=2016 [1][30]=2016 [1][31]=2016 [1][32]=2016 [1][33]=2016 [1][34]=2016 [1][35]=2016 [1][36]=2016 [1][37]=2016 [1][38]=2016 [1][39]=2016 [1][40]=2016 [1][41]=2016 [1][42]=2016 [1][43]=2016 [1][44]=2016 [1][45]=2016 [1][46]=2016 [1][47]=2016 [1][48]=2016 [1][49]=2016 [1][50]=2016 [1][51]=2016 [1][52]=2016 [1][53]=2016 [1][54]=2016 [1][55]=2016 [1][56]=2016 [1][57]=2016 [1][58]=2016 [1][59]=2016 [1][60]=2016 [1][61]=2016 [1][62]=2016 [1][63]=2016 \n",
            "[2][0]=4032 [2][1]=4032 [2][2]=4032 [2][3]=4032 [2][4]=4032 [2][5]=4032 [2][6]=4032 [2][7]=4032 [2][8]=4032 [2][9]=4032 [2][10]=4032 [2][11]=4032 [2][12]=4032 [2][13]=4032 [2][14]=4032 [2][15]=4032 [2][16]=4032 [2][17]=4032 [2][18]=4032 [2][19]=4032 [2][20]=4032 [2][21]=4032 [2][22]=4032 [2][23]=4032 [2][24]=4032 [2][25]=4032 [2][26]=4032 [2][27]=4032 [2][28]=4032 [2][29]=4032 [2][30]=4032 [2][31]=4032 [2][32]=4032 [2][33]=4032 [2][34]=4032 [2][35]=4032 [2][36]=4032 [2][37]=4032 [2][38]=4032 [2][39]=4032 [2][40]=4032 [2][41]=4032 [2][42]=4032 [2][43]=4032 [2][44]=4032 [2][45]=4032 [2][46]=4032 [2][47]=4032 [2][48]=4032 [2][49]=4032 [2][50]=4032 [2][51]=4032 [2][52]=4032 [2][53]=4032 [2][54]=4032 [2][55]=4032 [2][56]=4032 [2][57]=4032 [2][58]=4032 [2][59]=4032 [2][60]=4032 [2][61]=4032 [2][62]=4032 [2][63]=4032 \n",
            "[3][0]=6048 [3][1]=6048 [3][2]=6048 [3][3]=6048 [3][4]=6048 [3][5]=6048 [3][6]=6048 [3][7]=6048 [3][8]=6048 [3][9]=6048 [3][10]=6048 [3][11]=6048 [3][12]=6048 [3][13]=6048 [3][14]=6048 [3][15]=6048 [3][16]=6048 [3][17]=6048 [3][18]=6048 [3][19]=6048 [3][20]=6048 [3][21]=6048 [3][22]=6048 [3][23]=6048 [3][24]=6048 [3][25]=6048 [3][26]=6048 [3][27]=6048 [3][28]=6048 [3][29]=6048 [3][30]=6048 [3][31]=6048 [3][32]=6048 [3][33]=6048 [3][34]=6048 [3][35]=6048 [3][36]=6048 [3][37]=6048 [3][38]=6048 [3][39]=6048 [3][40]=6048 [3][41]=6048 [3][42]=6048 [3][43]=6048 [3][44]=6048 [3][45]=6048 [3][46]=6048 [3][47]=6048 [3][48]=6048 [3][49]=6048 [3][50]=6048 [3][51]=6048 [3][52]=6048 [3][53]=6048 [3][54]=6048 [3][55]=6048 [3][56]=6048 [3][57]=6048 [3][58]=6048 [3][59]=6048 [3][60]=6048 [3][61]=6048 [3][62]=6048 [3][63]=6048 \n",
            "[4][0]=8064 [4][1]=8064 [4][2]=8064 [4][3]=8064 [4][4]=8064 [4][5]=8064 [4][6]=8064 [4][7]=8064 [4][8]=8064 [4][9]=8064 [4][10]=8064 [4][11]=8064 [4][12]=8064 [4][13]=8064 [4][14]=8064 [4][15]=8064 [4][16]=8064 [4][17]=8064 [4][18]=8064 [4][19]=8064 [4][20]=8064 [4][21]=8064 [4][22]=8064 [4][23]=8064 [4][24]=8064 [4][25]=8064 [4][26]=8064 [4][27]=8064 [4][28]=8064 [4][29]=8064 [4][30]=8064 [4][31]=8064 [4][32]=8064 [4][33]=8064 [4][34]=8064 [4][35]=8064 [4][36]=8064 [4][37]=8064 [4][38]=8064 [4][39]=8064 [4][40]=8064 [4][41]=8064 [4][42]=8064 [4][43]=8064 [4][44]=8064 [4][45]=8064 [4][46]=8064 [4][47]=8064 [4][48]=8064 [4][49]=8064 [4][50]=8064 [4][51]=8064 [4][52]=8064 [4][53]=8064 [4][54]=8064 [4][55]=8064 [4][56]=8064 [4][57]=8064 [4][58]=8064 [4][59]=8064 [4][60]=8064 [4][61]=8064 [4][62]=8064 [4][63]=8064 \n",
            "[5][0]=10080 [5][1]=10080 [5][2]=10080 [5][3]=10080 [5][4]=10080 [5][5]=10080 [5][6]=10080 [5][7]=10080 [5][8]=10080 [5][9]=10080 [5][10]=10080 [5][11]=10080 [5][12]=10080 [5][13]=10080 [5][14]=10080 [5][15]=10080 [5][16]=10080 [5][17]=10080 [5][18]=10080 [5][19]=10080 [5][20]=10080 [5][21]=10080 [5][22]=10080 [5][23]=10080 [5][24]=10080 [5][25]=10080 [5][26]=10080 [5][27]=10080 [5][28]=10080 [5][29]=10080 [5][30]=10080 [5][31]=10080 [5][32]=10080 [5][33]=10080 [5][34]=10080 [5][35]=10080 [5][36]=10080 [5][37]=10080 [5][38]=10080 [5][39]=10080 [5][40]=10080 [5][41]=10080 [5][42]=10080 [5][43]=10080 [5][44]=10080 [5][45]=10080 [5][46]=10080 [5][47]=10080 [5][48]=10080 [5][49]=10080 [5][50]=10080 [5][51]=10080 [5][52]=10080 [5][53]=10080 [5][54]=10080 [5][55]=10080 [5][56]=10080 [5][57]=10080 [5][58]=10080 [5][59]=10080 [5][60]=10080 [5][61]=10080 [5][62]=10080 [5][63]=10080 \n",
            "[6][0]=12096 [6][1]=12096 [6][2]=12096 [6][3]=12096 [6][4]=12096 [6][5]=12096 [6][6]=12096 [6][7]=12096 [6][8]=12096 [6][9]=12096 [6][10]=12096 [6][11]=12096 [6][12]=12096 [6][13]=12096 [6][14]=12096 [6][15]=12096 [6][16]=12096 [6][17]=12096 [6][18]=12096 [6][19]=12096 [6][20]=12096 [6][21]=12096 [6][22]=12096 [6][23]=12096 [6][24]=12096 [6][25]=12096 [6][26]=12096 [6][27]=12096 [6][28]=12096 [6][29]=12096 [6][30]=12096 [6][31]=12096 [6][32]=12096 [6][33]=12096 [6][34]=12096 [6][35]=12096 [6][36]=12096 [6][37]=12096 [6][38]=12096 [6][39]=12096 [6][40]=12096 [6][41]=12096 [6][42]=12096 [6][43]=12096 [6][44]=12096 [6][45]=12096 [6][46]=12096 [6][47]=12096 [6][48]=12096 [6][49]=12096 [6][50]=12096 [6][51]=12096 [6][52]=12096 [6][53]=12096 [6][54]=12096 [6][55]=12096 [6][56]=12096 [6][57]=12096 [6][58]=12096 [6][59]=12096 [6][60]=12096 [6][61]=12096 [6][62]=12096 [6][63]=12096 \n",
            "[7][0]=14112 [7][1]=14112 [7][2]=14112 [7][3]=14112 [7][4]=14112 [7][5]=14112 [7][6]=14112 [7][7]=14112 [7][8]=14112 [7][9]=14112 [7][10]=14112 [7][11]=14112 [7][12]=14112 [7][13]=14112 [7][14]=14112 [7][15]=14112 [7][16]=14112 [7][17]=14112 [7][18]=14112 [7][19]=14112 [7][20]=14112 [7][21]=14112 [7][22]=14112 [7][23]=14112 [7][24]=14112 [7][25]=14112 [7][26]=14112 [7][27]=14112 [7][28]=14112 [7][29]=14112 [7][30]=14112 [7][31]=14112 [7][32]=14112 [7][33]=14112 [7][34]=14112 [7][35]=14112 [7][36]=14112 [7][37]=14112 [7][38]=14112 [7][39]=14112 [7][40]=14112 [7][41]=14112 [7][42]=14112 [7][43]=14112 [7][44]=14112 [7][45]=14112 [7][46]=14112 [7][47]=14112 [7][48]=14112 [7][49]=14112 [7][50]=14112 [7][51]=14112 [7][52]=14112 [7][53]=14112 [7][54]=14112 [7][55]=14112 [7][56]=14112 [7][57]=14112 [7][58]=14112 [7][59]=14112 [7][60]=14112 [7][61]=14112 [7][62]=14112 [7][63]=14112 \n",
            "[8][0]=16128 [8][1]=16128 [8][2]=16128 [8][3]=16128 [8][4]=16128 [8][5]=16128 [8][6]=16128 [8][7]=16128 [8][8]=16128 [8][9]=16128 [8][10]=16128 [8][11]=16128 [8][12]=16128 [8][13]=16128 [8][14]=16128 [8][15]=16128 [8][16]=16128 [8][17]=16128 [8][18]=16128 [8][19]=16128 [8][20]=16128 [8][21]=16128 [8][22]=16128 [8][23]=16128 [8][24]=16128 [8][25]=16128 [8][26]=16128 [8][27]=16128 [8][28]=16128 [8][29]=16128 [8][30]=16128 [8][31]=16128 [8][32]=16128 [8][33]=16128 [8][34]=16128 [8][35]=16128 [8][36]=16128 [8][37]=16128 [8][38]=16128 [8][39]=16128 [8][40]=16128 [8][41]=16128 [8][42]=16128 [8][43]=16128 [8][44]=16128 [8][45]=16128 [8][46]=16128 [8][47]=16128 [8][48]=16128 [8][49]=16128 [8][50]=16128 [8][51]=16128 [8][52]=16128 [8][53]=16128 [8][54]=16128 [8][55]=16128 [8][56]=16128 [8][57]=16128 [8][58]=16128 [8][59]=16128 [8][60]=16128 [8][61]=16128 [8][62]=16128 [8][63]=16128 \n",
            "[9][0]=18144 [9][1]=18144 [9][2]=18144 [9][3]=18144 [9][4]=18144 [9][5]=18144 [9][6]=18144 [9][7]=18144 [9][8]=18144 [9][9]=18144 [9][10]=18144 [9][11]=18144 [9][12]=18144 [9][13]=18144 [9][14]=18144 [9][15]=18144 [9][16]=18144 [9][17]=18144 [9][18]=18144 [9][19]=18144 [9][20]=18144 [9][21]=18144 [9][22]=18144 [9][23]=18144 [9][24]=18144 [9][25]=18144 [9][26]=18144 [9][27]=18144 [9][28]=18144 [9][29]=18144 [9][30]=18144 [9][31]=18144 [9][32]=18144 [9][33]=18144 [9][34]=18144 [9][35]=18144 [9][36]=18144 [9][37]=18144 [9][38]=18144 [9][39]=18144 [9][40]=18144 [9][41]=18144 [9][42]=18144 [9][43]=18144 [9][44]=18144 [9][45]=18144 [9][46]=18144 [9][47]=18144 [9][48]=18144 [9][49]=18144 [9][50]=18144 [9][51]=18144 [9][52]=18144 [9][53]=18144 [9][54]=18144 [9][55]=18144 [9][56]=18144 [9][57]=18144 [9][58]=18144 [9][59]=18144 [9][60]=18144 [9][61]=18144 [9][62]=18144 [9][63]=18144 \n",
            "[10][0]=20160 [10][1]=20160 [10][2]=20160 [10][3]=20160 [10][4]=20160 [10][5]=20160 [10][6]=20160 [10][7]=20160 [10][8]=20160 [10][9]=20160 [10][10]=20160 [10][11]=20160 [10][12]=20160 [10][13]=20160 [10][14]=20160 [10][15]=20160 [10][16]=20160 [10][17]=20160 [10][18]=20160 [10][19]=20160 [10][20]=20160 [10][21]=20160 [10][22]=20160 [10][23]=20160 [10][24]=20160 [10][25]=20160 [10][26]=20160 [10][27]=20160 [10][28]=20160 [10][29]=20160 [10][30]=20160 [10][31]=20160 [10][32]=20160 [10][33]=20160 [10][34]=20160 [10][35]=20160 [10][36]=20160 [10][37]=20160 [10][38]=20160 [10][39]=20160 [10][40]=20160 [10][41]=20160 [10][42]=20160 [10][43]=20160 [10][44]=20160 [10][45]=20160 [10][46]=20160 [10][47]=20160 [10][48]=20160 [10][49]=20160 [10][50]=20160 [10][51]=20160 [10][52]=20160 [10][53]=20160 [10][54]=20160 [10][55]=20160 [10][56]=20160 [10][57]=20160 [10][58]=20160 [10][59]=20160 [10][60]=20160 [10][61]=20160 [10][62]=20160 [10][63]=20160 \n",
            "[11][0]=22176 [11][1]=22176 [11][2]=22176 [11][3]=22176 [11][4]=22176 [11][5]=22176 [11][6]=22176 [11][7]=22176 [11][8]=22176 [11][9]=22176 [11][10]=22176 [11][11]=22176 [11][12]=22176 [11][13]=22176 [11][14]=22176 [11][15]=22176 [11][16]=22176 [11][17]=22176 [11][18]=22176 [11][19]=22176 [11][20]=22176 [11][21]=22176 [11][22]=22176 [11][23]=22176 [11][24]=22176 [11][25]=22176 [11][26]=22176 [11][27]=22176 [11][28]=22176 [11][29]=22176 [11][30]=22176 [11][31]=22176 [11][32]=22176 [11][33]=22176 [11][34]=22176 [11][35]=22176 [11][36]=22176 [11][37]=22176 [11][38]=22176 [11][39]=22176 [11][40]=22176 [11][41]=22176 [11][42]=22176 [11][43]=22176 [11][44]=22176 [11][45]=22176 [11][46]=22176 [11][47]=22176 [11][48]=22176 [11][49]=22176 [11][50]=22176 [11][51]=22176 [11][52]=22176 [11][53]=22176 [11][54]=22176 [11][55]=22176 [11][56]=22176 [11][57]=22176 [11][58]=22176 [11][59]=22176 [11][60]=22176 [11][61]=22176 [11][62]=22176 [11][63]=22176 \n",
            "[12][0]=24192 [12][1]=24192 [12][2]=24192 [12][3]=24192 [12][4]=24192 [12][5]=24192 [12][6]=24192 [12][7]=24192 [12][8]=24192 [12][9]=24192 [12][10]=24192 [12][11]=24192 [12][12]=24192 [12][13]=24192 [12][14]=24192 [12][15]=24192 [12][16]=24192 [12][17]=24192 [12][18]=24192 [12][19]=24192 [12][20]=24192 [12][21]=24192 [12][22]=24192 [12][23]=24192 [12][24]=24192 [12][25]=24192 [12][26]=24192 [12][27]=24192 [12][28]=24192 [12][29]=24192 [12][30]=24192 [12][31]=24192 [12][32]=24192 [12][33]=24192 [12][34]=24192 [12][35]=24192 [12][36]=24192 [12][37]=24192 [12][38]=24192 [12][39]=24192 [12][40]=24192 [12][41]=24192 [12][42]=24192 [12][43]=24192 [12][44]=24192 [12][45]=24192 [12][46]=24192 [12][47]=24192 [12][48]=24192 [12][49]=24192 [12][50]=24192 [12][51]=24192 [12][52]=24192 [12][53]=24192 [12][54]=24192 [12][55]=24192 [12][56]=24192 [12][57]=24192 [12][58]=24192 [12][59]=24192 [12][60]=24192 [12][61]=24192 [12][62]=24192 [12][63]=24192 \n",
            "[13][0]=26208 [13][1]=26208 [13][2]=26208 [13][3]=26208 [13][4]=26208 [13][5]=26208 [13][6]=26208 [13][7]=26208 [13][8]=26208 [13][9]=26208 [13][10]=26208 [13][11]=26208 [13][12]=26208 [13][13]=26208 [13][14]=26208 [13][15]=26208 [13][16]=26208 [13][17]=26208 [13][18]=26208 [13][19]=26208 [13][20]=26208 [13][21]=26208 [13][22]=26208 [13][23]=26208 [13][24]=26208 [13][25]=26208 [13][26]=26208 [13][27]=26208 [13][28]=26208 [13][29]=26208 [13][30]=26208 [13][31]=26208 [13][32]=26208 [13][33]=26208 [13][34]=26208 [13][35]=26208 [13][36]=26208 [13][37]=26208 [13][38]=26208 [13][39]=26208 [13][40]=26208 [13][41]=26208 [13][42]=26208 [13][43]=26208 [13][44]=26208 [13][45]=26208 [13][46]=26208 [13][47]=26208 [13][48]=26208 [13][49]=26208 [13][50]=26208 [13][51]=26208 [13][52]=26208 [13][53]=26208 [13][54]=26208 [13][55]=26208 [13][56]=26208 [13][57]=26208 [13][58]=26208 [13][59]=26208 [13][60]=26208 [13][61]=26208 [13][62]=26208 [13][63]=26208 \n",
            "[14][0]=28224 [14][1]=28224 [14][2]=28224 [14][3]=28224 [14][4]=28224 [14][5]=28224 [14][6]=28224 [14][7]=28224 [14][8]=28224 [14][9]=28224 [14][10]=28224 [14][11]=28224 [14][12]=28224 [14][13]=28224 [14][14]=28224 [14][15]=28224 [14][16]=28224 [14][17]=28224 [14][18]=28224 [14][19]=28224 [14][20]=28224 [14][21]=28224 [14][22]=28224 [14][23]=28224 [14][24]=28224 [14][25]=28224 [14][26]=28224 [14][27]=28224 [14][28]=28224 [14][29]=28224 [14][30]=28224 [14][31]=28224 [14][32]=28224 [14][33]=28224 [14][34]=28224 [14][35]=28224 [14][36]=28224 [14][37]=28224 [14][38]=28224 [14][39]=28224 [14][40]=28224 [14][41]=28224 [14][42]=28224 [14][43]=28224 [14][44]=28224 [14][45]=28224 [14][46]=28224 [14][47]=28224 [14][48]=28224 [14][49]=28224 [14][50]=28224 [14][51]=28224 [14][52]=28224 [14][53]=28224 [14][54]=28224 [14][55]=28224 [14][56]=28224 [14][57]=28224 [14][58]=28224 [14][59]=28224 [14][60]=28224 [14][61]=28224 [14][62]=28224 [14][63]=28224 \n",
            "[15][0]=30240 [15][1]=30240 [15][2]=30240 [15][3]=30240 [15][4]=30240 [15][5]=30240 [15][6]=30240 [15][7]=30240 [15][8]=30240 [15][9]=30240 [15][10]=30240 [15][11]=30240 [15][12]=30240 [15][13]=30240 [15][14]=30240 [15][15]=30240 [15][16]=30240 [15][17]=30240 [15][18]=30240 [15][19]=30240 [15][20]=30240 [15][21]=30240 [15][22]=30240 [15][23]=30240 [15][24]=30240 [15][25]=30240 [15][26]=30240 [15][27]=30240 [15][28]=30240 [15][29]=30240 [15][30]=30240 [15][31]=30240 [15][32]=30240 [15][33]=30240 [15][34]=30240 [15][35]=30240 [15][36]=30240 [15][37]=30240 [15][38]=30240 [15][39]=30240 [15][40]=30240 [15][41]=30240 [15][42]=30240 [15][43]=30240 [15][44]=30240 [15][45]=30240 [15][46]=30240 [15][47]=30240 [15][48]=30240 [15][49]=30240 [15][50]=30240 [15][51]=30240 [15][52]=30240 [15][53]=30240 [15][54]=30240 [15][55]=30240 [15][56]=30240 [15][57]=30240 [15][58]=30240 [15][59]=30240 [15][60]=30240 [15][61]=30240 [15][62]=30240 [15][63]=30240 \n",
            "[16][0]=32256 [16][1]=32256 [16][2]=32256 [16][3]=32256 [16][4]=32256 [16][5]=32256 [16][6]=32256 [16][7]=32256 [16][8]=32256 [16][9]=32256 [16][10]=32256 [16][11]=32256 [16][12]=32256 [16][13]=32256 [16][14]=32256 [16][15]=32256 [16][16]=32256 [16][17]=32256 [16][18]=32256 [16][19]=32256 [16][20]=32256 [16][21]=32256 [16][22]=32256 [16][23]=32256 [16][24]=32256 [16][25]=32256 [16][26]=32256 [16][27]=32256 [16][28]=32256 [16][29]=32256 [16][30]=32256 [16][31]=32256 [16][32]=32256 [16][33]=32256 [16][34]=32256 [16][35]=32256 [16][36]=32256 [16][37]=32256 [16][38]=32256 [16][39]=32256 [16][40]=32256 [16][41]=32256 [16][42]=32256 [16][43]=32256 [16][44]=32256 [16][45]=32256 [16][46]=32256 [16][47]=32256 [16][48]=32256 [16][49]=32256 [16][50]=32256 [16][51]=32256 [16][52]=32256 [16][53]=32256 [16][54]=32256 [16][55]=32256 [16][56]=32256 [16][57]=32256 [16][58]=32256 [16][59]=32256 [16][60]=32256 [16][61]=32256 [16][62]=32256 [16][63]=32256 \n",
            "[17][0]=34272 [17][1]=34272 [17][2]=34272 [17][3]=34272 [17][4]=34272 [17][5]=34272 [17][6]=34272 [17][7]=34272 [17][8]=34272 [17][9]=34272 [17][10]=34272 [17][11]=34272 [17][12]=34272 [17][13]=34272 [17][14]=34272 [17][15]=34272 [17][16]=34272 [17][17]=34272 [17][18]=34272 [17][19]=34272 [17][20]=34272 [17][21]=34272 [17][22]=34272 [17][23]=34272 [17][24]=34272 [17][25]=34272 [17][26]=34272 [17][27]=34272 [17][28]=34272 [17][29]=34272 [17][30]=34272 [17][31]=34272 [17][32]=34272 [17][33]=34272 [17][34]=34272 [17][35]=34272 [17][36]=34272 [17][37]=34272 [17][38]=34272 [17][39]=34272 [17][40]=34272 [17][41]=34272 [17][42]=34272 [17][43]=34272 [17][44]=34272 [17][45]=34272 [17][46]=34272 [17][47]=34272 [17][48]=34272 [17][49]=34272 [17][50]=34272 [17][51]=34272 [17][52]=34272 [17][53]=34272 [17][54]=34272 [17][55]=34272 [17][56]=34272 [17][57]=34272 [17][58]=34272 [17][59]=34272 [17][60]=34272 [17][61]=34272 [17][62]=34272 [17][63]=34272 \n",
            "[18][0]=36288 [18][1]=36288 [18][2]=36288 [18][3]=36288 [18][4]=36288 [18][5]=36288 [18][6]=36288 [18][7]=36288 [18][8]=36288 [18][9]=36288 [18][10]=36288 [18][11]=36288 [18][12]=36288 [18][13]=36288 [18][14]=36288 [18][15]=36288 [18][16]=36288 [18][17]=36288 [18][18]=36288 [18][19]=36288 [18][20]=36288 [18][21]=36288 [18][22]=36288 [18][23]=36288 [18][24]=36288 [18][25]=36288 [18][26]=36288 [18][27]=36288 [18][28]=36288 [18][29]=36288 [18][30]=36288 [18][31]=36288 [18][32]=36288 [18][33]=36288 [18][34]=36288 [18][35]=36288 [18][36]=36288 [18][37]=36288 [18][38]=36288 [18][39]=36288 [18][40]=36288 [18][41]=36288 [18][42]=36288 [18][43]=36288 [18][44]=36288 [18][45]=36288 [18][46]=36288 [18][47]=36288 [18][48]=36288 [18][49]=36288 [18][50]=36288 [18][51]=36288 [18][52]=36288 [18][53]=36288 [18][54]=36288 [18][55]=36288 [18][56]=36288 [18][57]=36288 [18][58]=36288 [18][59]=36288 [18][60]=36288 [18][61]=36288 [18][62]=36288 [18][63]=36288 \n",
            "[19][0]=38304 [19][1]=38304 [19][2]=38304 [19][3]=38304 [19][4]=38304 [19][5]=38304 [19][6]=38304 [19][7]=38304 [19][8]=38304 [19][9]=38304 [19][10]=38304 [19][11]=38304 [19][12]=38304 [19][13]=38304 [19][14]=38304 [19][15]=38304 [19][16]=38304 [19][17]=38304 [19][18]=38304 [19][19]=38304 [19][20]=38304 [19][21]=38304 [19][22]=38304 [19][23]=38304 [19][24]=38304 [19][25]=38304 [19][26]=38304 [19][27]=38304 [19][28]=38304 [19][29]=38304 [19][30]=38304 [19][31]=38304 [19][32]=38304 [19][33]=38304 [19][34]=38304 [19][35]=38304 [19][36]=38304 [19][37]=38304 [19][38]=38304 [19][39]=38304 [19][40]=38304 [19][41]=38304 [19][42]=38304 [19][43]=38304 [19][44]=38304 [19][45]=38304 [19][46]=38304 [19][47]=38304 [19][48]=38304 [19][49]=38304 [19][50]=38304 [19][51]=38304 [19][52]=38304 [19][53]=38304 [19][54]=38304 [19][55]=38304 [19][56]=38304 [19][57]=38304 [19][58]=38304 [19][59]=38304 [19][60]=38304 [19][61]=38304 [19][62]=38304 [19][63]=38304 \n",
            "[20][0]=40320 [20][1]=40320 [20][2]=40320 [20][3]=40320 [20][4]=40320 [20][5]=40320 [20][6]=40320 [20][7]=40320 [20][8]=40320 [20][9]=40320 [20][10]=40320 [20][11]=40320 [20][12]=40320 [20][13]=40320 [20][14]=40320 [20][15]=40320 [20][16]=40320 [20][17]=40320 [20][18]=40320 [20][19]=40320 [20][20]=40320 [20][21]=40320 [20][22]=40320 [20][23]=40320 [20][24]=40320 [20][25]=40320 [20][26]=40320 [20][27]=40320 [20][28]=40320 [20][29]=40320 [20][30]=40320 [20][31]=40320 [20][32]=40320 [20][33]=40320 [20][34]=40320 [20][35]=40320 [20][36]=40320 [20][37]=40320 [20][38]=40320 [20][39]=40320 [20][40]=40320 [20][41]=40320 [20][42]=40320 [20][43]=40320 [20][44]=40320 [20][45]=40320 [20][46]=40320 [20][47]=40320 [20][48]=40320 [20][49]=40320 [20][50]=40320 [20][51]=40320 [20][52]=40320 [20][53]=40320 [20][54]=40320 [20][55]=40320 [20][56]=40320 [20][57]=40320 [20][58]=40320 [20][59]=40320 [20][60]=40320 [20][61]=40320 [20][62]=40320 [20][63]=40320 \n",
            "[21][0]=42336 [21][1]=42336 [21][2]=42336 [21][3]=42336 [21][4]=42336 [21][5]=42336 [21][6]=42336 [21][7]=42336 [21][8]=42336 [21][9]=42336 [21][10]=42336 [21][11]=42336 [21][12]=42336 [21][13]=42336 [21][14]=42336 [21][15]=42336 [21][16]=42336 [21][17]=42336 [21][18]=42336 [21][19]=42336 [21][20]=42336 [21][21]=42336 [21][22]=42336 [21][23]=42336 [21][24]=42336 [21][25]=42336 [21][26]=42336 [21][27]=42336 [21][28]=42336 [21][29]=42336 [21][30]=42336 [21][31]=42336 [21][32]=42336 [21][33]=42336 [21][34]=42336 [21][35]=42336 [21][36]=42336 [21][37]=42336 [21][38]=42336 [21][39]=42336 [21][40]=42336 [21][41]=42336 [21][42]=42336 [21][43]=42336 [21][44]=42336 [21][45]=42336 [21][46]=42336 [21][47]=42336 [21][48]=42336 [21][49]=42336 [21][50]=42336 [21][51]=42336 [21][52]=42336 [21][53]=42336 [21][54]=42336 [21][55]=42336 [21][56]=42336 [21][57]=42336 [21][58]=42336 [21][59]=42336 [21][60]=42336 [21][61]=42336 [21][62]=42336 [21][63]=42336 \n",
            "[22][0]=44352 [22][1]=44352 [22][2]=44352 [22][3]=44352 [22][4]=44352 [22][5]=44352 [22][6]=44352 [22][7]=44352 [22][8]=44352 [22][9]=44352 [22][10]=44352 [22][11]=44352 [22][12]=44352 [22][13]=44352 [22][14]=44352 [22][15]=44352 [22][16]=44352 [22][17]=44352 [22][18]=44352 [22][19]=44352 [22][20]=44352 [22][21]=44352 [22][22]=44352 [22][23]=44352 [22][24]=44352 [22][25]=44352 [22][26]=44352 [22][27]=44352 [22][28]=44352 [22][29]=44352 [22][30]=44352 [22][31]=44352 [22][32]=44352 [22][33]=44352 [22][34]=44352 [22][35]=44352 [22][36]=44352 [22][37]=44352 [22][38]=44352 [22][39]=44352 [22][40]=44352 [22][41]=44352 [22][42]=44352 [22][43]=44352 [22][44]=44352 [22][45]=44352 [22][46]=44352 [22][47]=44352 [22][48]=44352 [22][49]=44352 [22][50]=44352 [22][51]=44352 [22][52]=44352 [22][53]=44352 [22][54]=44352 [22][55]=44352 [22][56]=44352 [22][57]=44352 [22][58]=44352 [22][59]=44352 [22][60]=44352 [22][61]=44352 [22][62]=44352 [22][63]=44352 \n",
            "[23][0]=46368 [23][1]=46368 [23][2]=46368 [23][3]=46368 [23][4]=46368 [23][5]=46368 [23][6]=46368 [23][7]=46368 [23][8]=46368 [23][9]=46368 [23][10]=46368 [23][11]=46368 [23][12]=46368 [23][13]=46368 [23][14]=46368 [23][15]=46368 [23][16]=46368 [23][17]=46368 [23][18]=46368 [23][19]=46368 [23][20]=46368 [23][21]=46368 [23][22]=46368 [23][23]=46368 [23][24]=46368 [23][25]=46368 [23][26]=46368 [23][27]=46368 [23][28]=46368 [23][29]=46368 [23][30]=46368 [23][31]=46368 [23][32]=46368 [23][33]=46368 [23][34]=46368 [23][35]=46368 [23][36]=46368 [23][37]=46368 [23][38]=46368 [23][39]=46368 [23][40]=46368 [23][41]=46368 [23][42]=46368 [23][43]=46368 [23][44]=46368 [23][45]=46368 [23][46]=46368 [23][47]=46368 [23][48]=46368 [23][49]=46368 [23][50]=46368 [23][51]=46368 [23][52]=46368 [23][53]=46368 [23][54]=46368 [23][55]=46368 [23][56]=46368 [23][57]=46368 [23][58]=46368 [23][59]=46368 [23][60]=46368 [23][61]=46368 [23][62]=46368 [23][63]=46368 \n",
            "[24][0]=48384 [24][1]=48384 [24][2]=48384 [24][3]=48384 [24][4]=48384 [24][5]=48384 [24][6]=48384 [24][7]=48384 [24][8]=48384 [24][9]=48384 [24][10]=48384 [24][11]=48384 [24][12]=48384 [24][13]=48384 [24][14]=48384 [24][15]=48384 [24][16]=48384 [24][17]=48384 [24][18]=48384 [24][19]=48384 [24][20]=48384 [24][21]=48384 [24][22]=48384 [24][23]=48384 [24][24]=48384 [24][25]=48384 [24][26]=48384 [24][27]=48384 [24][28]=48384 [24][29]=48384 [24][30]=48384 [24][31]=48384 [24][32]=48384 [24][33]=48384 [24][34]=48384 [24][35]=48384 [24][36]=48384 [24][37]=48384 [24][38]=48384 [24][39]=48384 [24][40]=48384 [24][41]=48384 [24][42]=48384 [24][43]=48384 [24][44]=48384 [24][45]=48384 [24][46]=48384 [24][47]=48384 [24][48]=48384 [24][49]=48384 [24][50]=48384 [24][51]=48384 [24][52]=48384 [24][53]=48384 [24][54]=48384 [24][55]=48384 [24][56]=48384 [24][57]=48384 [24][58]=48384 [24][59]=48384 [24][60]=48384 [24][61]=48384 [24][62]=48384 [24][63]=48384 \n",
            "[25][0]=50400 [25][1]=50400 [25][2]=50400 [25][3]=50400 [25][4]=50400 [25][5]=50400 [25][6]=50400 [25][7]=50400 [25][8]=50400 [25][9]=50400 [25][10]=50400 [25][11]=50400 [25][12]=50400 [25][13]=50400 [25][14]=50400 [25][15]=50400 [25][16]=50400 [25][17]=50400 [25][18]=50400 [25][19]=50400 [25][20]=50400 [25][21]=50400 [25][22]=50400 [25][23]=50400 [25][24]=50400 [25][25]=50400 [25][26]=50400 [25][27]=50400 [25][28]=50400 [25][29]=50400 [25][30]=50400 [25][31]=50400 [25][32]=50400 [25][33]=50400 [25][34]=50400 [25][35]=50400 [25][36]=50400 [25][37]=50400 [25][38]=50400 [25][39]=50400 [25][40]=50400 [25][41]=50400 [25][42]=50400 [25][43]=50400 [25][44]=50400 [25][45]=50400 [25][46]=50400 [25][47]=50400 [25][48]=50400 [25][49]=50400 [25][50]=50400 [25][51]=50400 [25][52]=50400 [25][53]=50400 [25][54]=50400 [25][55]=50400 [25][56]=50400 [25][57]=50400 [25][58]=50400 [25][59]=50400 [25][60]=50400 [25][61]=50400 [25][62]=50400 [25][63]=50400 \n",
            "[26][0]=52416 [26][1]=52416 [26][2]=52416 [26][3]=52416 [26][4]=52416 [26][5]=52416 [26][6]=52416 [26][7]=52416 [26][8]=52416 [26][9]=52416 [26][10]=52416 [26][11]=52416 [26][12]=52416 [26][13]=52416 [26][14]=52416 [26][15]=52416 [26][16]=52416 [26][17]=52416 [26][18]=52416 [26][19]=52416 [26][20]=52416 [26][21]=52416 [26][22]=52416 [26][23]=52416 [26][24]=52416 [26][25]=52416 [26][26]=52416 [26][27]=52416 [26][28]=52416 [26][29]=52416 [26][30]=52416 [26][31]=52416 [26][32]=52416 [26][33]=52416 [26][34]=52416 [26][35]=52416 [26][36]=52416 [26][37]=52416 [26][38]=52416 [26][39]=52416 [26][40]=52416 [26][41]=52416 [26][42]=52416 [26][43]=52416 [26][44]=52416 [26][45]=52416 [26][46]=52416 [26][47]=52416 [26][48]=52416 [26][49]=52416 [26][50]=52416 [26][51]=52416 [26][52]=52416 [26][53]=52416 [26][54]=52416 [26][55]=52416 [26][56]=52416 [26][57]=52416 [26][58]=52416 [26][59]=52416 [26][60]=52416 [26][61]=52416 [26][62]=52416 [26][63]=52416 \n",
            "[27][0]=54432 [27][1]=54432 [27][2]=54432 [27][3]=54432 [27][4]=54432 [27][5]=54432 [27][6]=54432 [27][7]=54432 [27][8]=54432 [27][9]=54432 [27][10]=54432 [27][11]=54432 [27][12]=54432 [27][13]=54432 [27][14]=54432 [27][15]=54432 [27][16]=54432 [27][17]=54432 [27][18]=54432 [27][19]=54432 [27][20]=54432 [27][21]=54432 [27][22]=54432 [27][23]=54432 [27][24]=54432 [27][25]=54432 [27][26]=54432 [27][27]=54432 [27][28]=54432 [27][29]=54432 [27][30]=54432 [27][31]=54432 [27][32]=54432 [27][33]=54432 [27][34]=54432 [27][35]=54432 [27][36]=54432 [27][37]=54432 [27][38]=54432 [27][39]=54432 [27][40]=54432 [27][41]=54432 [27][42]=54432 [27][43]=54432 [27][44]=54432 [27][45]=54432 [27][46]=54432 [27][47]=54432 [27][48]=54432 [27][49]=54432 [27][50]=54432 [27][51]=54432 [27][52]=54432 [27][53]=54432 [27][54]=54432 [27][55]=54432 [27][56]=54432 [27][57]=54432 [27][58]=54432 [27][59]=54432 [27][60]=54432 [27][61]=54432 [27][62]=54432 [27][63]=54432 \n",
            "[28][0]=56448 [28][1]=56448 [28][2]=56448 [28][3]=56448 [28][4]=56448 [28][5]=56448 [28][6]=56448 [28][7]=56448 [28][8]=56448 [28][9]=56448 [28][10]=56448 [28][11]=56448 [28][12]=56448 [28][13]=56448 [28][14]=56448 [28][15]=56448 [28][16]=56448 [28][17]=56448 [28][18]=56448 [28][19]=56448 [28][20]=56448 [28][21]=56448 [28][22]=56448 [28][23]=56448 [28][24]=56448 [28][25]=56448 [28][26]=56448 [28][27]=56448 [28][28]=56448 [28][29]=56448 [28][30]=56448 [28][31]=56448 [28][32]=56448 [28][33]=56448 [28][34]=56448 [28][35]=56448 [28][36]=56448 [28][37]=56448 [28][38]=56448 [28][39]=56448 [28][40]=56448 [28][41]=56448 [28][42]=56448 [28][43]=56448 [28][44]=56448 [28][45]=56448 [28][46]=56448 [28][47]=56448 [28][48]=56448 [28][49]=56448 [28][50]=56448 [28][51]=56448 [28][52]=56448 [28][53]=56448 [28][54]=56448 [28][55]=56448 [28][56]=56448 [28][57]=56448 [28][58]=56448 [28][59]=56448 [28][60]=56448 [28][61]=56448 [28][62]=56448 [28][63]=56448 \n",
            "[29][0]=58464 [29][1]=58464 [29][2]=58464 [29][3]=58464 [29][4]=58464 [29][5]=58464 [29][6]=58464 [29][7]=58464 [29][8]=58464 [29][9]=58464 [29][10]=58464 [29][11]=58464 [29][12]=58464 [29][13]=58464 [29][14]=58464 [29][15]=58464 [29][16]=58464 [29][17]=58464 [29][18]=58464 [29][19]=58464 [29][20]=58464 [29][21]=58464 [29][22]=58464 [29][23]=58464 [29][24]=58464 [29][25]=58464 [29][26]=58464 [29][27]=58464 [29][28]=58464 [29][29]=58464 [29][30]=58464 [29][31]=58464 [29][32]=58464 [29][33]=58464 [29][34]=58464 [29][35]=58464 [29][36]=58464 [29][37]=58464 [29][38]=58464 [29][39]=58464 [29][40]=58464 [29][41]=58464 [29][42]=58464 [29][43]=58464 [29][44]=58464 [29][45]=58464 [29][46]=58464 [29][47]=58464 [29][48]=58464 [29][49]=58464 [29][50]=58464 [29][51]=58464 [29][52]=58464 [29][53]=58464 [29][54]=58464 [29][55]=58464 [29][56]=58464 [29][57]=58464 [29][58]=58464 [29][59]=58464 [29][60]=58464 [29][61]=58464 [29][62]=58464 [29][63]=58464 \n",
            "[30][0]=60480 [30][1]=60480 [30][2]=60480 [30][3]=60480 [30][4]=60480 [30][5]=60480 [30][6]=60480 [30][7]=60480 [30][8]=60480 [30][9]=60480 [30][10]=60480 [30][11]=60480 [30][12]=60480 [30][13]=60480 [30][14]=60480 [30][15]=60480 [30][16]=60480 [30][17]=60480 [30][18]=60480 [30][19]=60480 [30][20]=60480 [30][21]=60480 [30][22]=60480 [30][23]=60480 [30][24]=60480 [30][25]=60480 [30][26]=60480 [30][27]=60480 [30][28]=60480 [30][29]=60480 [30][30]=60480 [30][31]=60480 [30][32]=60480 [30][33]=60480 [30][34]=60480 [30][35]=60480 [30][36]=60480 [30][37]=60480 [30][38]=60480 [30][39]=60480 [30][40]=60480 [30][41]=60480 [30][42]=60480 [30][43]=60480 [30][44]=60480 [30][45]=60480 [30][46]=60480 [30][47]=60480 [30][48]=60480 [30][49]=60480 [30][50]=60480 [30][51]=60480 [30][52]=60480 [30][53]=60480 [30][54]=60480 [30][55]=60480 [30][56]=60480 [30][57]=60480 [30][58]=60480 [30][59]=60480 [30][60]=60480 [30][61]=60480 [30][62]=60480 [30][63]=60480 \n",
            "[31][0]=62496 [31][1]=62496 [31][2]=62496 [31][3]=62496 [31][4]=62496 [31][5]=62496 [31][6]=62496 [31][7]=62496 [31][8]=62496 [31][9]=62496 [31][10]=62496 [31][11]=62496 [31][12]=62496 [31][13]=62496 [31][14]=62496 [31][15]=62496 [31][16]=62496 [31][17]=62496 [31][18]=62496 [31][19]=62496 [31][20]=62496 [31][21]=62496 [31][22]=62496 [31][23]=62496 [31][24]=62496 [31][25]=62496 [31][26]=62496 [31][27]=62496 [31][28]=62496 [31][29]=62496 [31][30]=62496 [31][31]=62496 [31][32]=62496 [31][33]=62496 [31][34]=62496 [31][35]=62496 [31][36]=62496 [31][37]=62496 [31][38]=62496 [31][39]=62496 [31][40]=62496 [31][41]=62496 [31][42]=62496 [31][43]=62496 [31][44]=62496 [31][45]=62496 [31][46]=62496 [31][47]=62496 [31][48]=62496 [31][49]=62496 [31][50]=62496 [31][51]=62496 [31][52]=62496 [31][53]=62496 [31][54]=62496 [31][55]=62496 [31][56]=62496 [31][57]=62496 [31][58]=62496 [31][59]=62496 [31][60]=62496 [31][61]=62496 [31][62]=62496 [31][63]=62496 \n",
            "[32][0]=64512 [32][1]=64512 [32][2]=64512 [32][3]=64512 [32][4]=64512 [32][5]=64512 [32][6]=64512 [32][7]=64512 [32][8]=64512 [32][9]=64512 [32][10]=64512 [32][11]=64512 [32][12]=64512 [32][13]=64512 [32][14]=64512 [32][15]=64512 [32][16]=64512 [32][17]=64512 [32][18]=64512 [32][19]=64512 [32][20]=64512 [32][21]=64512 [32][22]=64512 [32][23]=64512 [32][24]=64512 [32][25]=64512 [32][26]=64512 [32][27]=64512 [32][28]=64512 [32][29]=64512 [32][30]=64512 [32][31]=64512 [32][32]=64512 [32][33]=64512 [32][34]=64512 [32][35]=64512 [32][36]=64512 [32][37]=64512 [32][38]=64512 [32][39]=64512 [32][40]=64512 [32][41]=64512 [32][42]=64512 [32][43]=64512 [32][44]=64512 [32][45]=64512 [32][46]=64512 [32][47]=64512 [32][48]=64512 [32][49]=64512 [32][50]=64512 [32][51]=64512 [32][52]=64512 [32][53]=64512 [32][54]=64512 [32][55]=64512 [32][56]=64512 [32][57]=64512 [32][58]=64512 [32][59]=64512 [32][60]=64512 [32][61]=64512 [32][62]=64512 [32][63]=64512 \n",
            "[33][0]=66528 [33][1]=66528 [33][2]=66528 [33][3]=66528 [33][4]=66528 [33][5]=66528 [33][6]=66528 [33][7]=66528 [33][8]=66528 [33][9]=66528 [33][10]=66528 [33][11]=66528 [33][12]=66528 [33][13]=66528 [33][14]=66528 [33][15]=66528 [33][16]=66528 [33][17]=66528 [33][18]=66528 [33][19]=66528 [33][20]=66528 [33][21]=66528 [33][22]=66528 [33][23]=66528 [33][24]=66528 [33][25]=66528 [33][26]=66528 [33][27]=66528 [33][28]=66528 [33][29]=66528 [33][30]=66528 [33][31]=66528 [33][32]=66528 [33][33]=66528 [33][34]=66528 [33][35]=66528 [33][36]=66528 [33][37]=66528 [33][38]=66528 [33][39]=66528 [33][40]=66528 [33][41]=66528 [33][42]=66528 [33][43]=66528 [33][44]=66528 [33][45]=66528 [33][46]=66528 [33][47]=66528 [33][48]=66528 [33][49]=66528 [33][50]=66528 [33][51]=66528 [33][52]=66528 [33][53]=66528 [33][54]=66528 [33][55]=66528 [33][56]=66528 [33][57]=66528 [33][58]=66528 [33][59]=66528 [33][60]=66528 [33][61]=66528 [33][62]=66528 [33][63]=66528 \n",
            "[34][0]=68544 [34][1]=68544 [34][2]=68544 [34][3]=68544 [34][4]=68544 [34][5]=68544 [34][6]=68544 [34][7]=68544 [34][8]=68544 [34][9]=68544 [34][10]=68544 [34][11]=68544 [34][12]=68544 [34][13]=68544 [34][14]=68544 [34][15]=68544 [34][16]=68544 [34][17]=68544 [34][18]=68544 [34][19]=68544 [34][20]=68544 [34][21]=68544 [34][22]=68544 [34][23]=68544 [34][24]=68544 [34][25]=68544 [34][26]=68544 [34][27]=68544 [34][28]=68544 [34][29]=68544 [34][30]=68544 [34][31]=68544 [34][32]=68544 [34][33]=68544 [34][34]=68544 [34][35]=68544 [34][36]=68544 [34][37]=68544 [34][38]=68544 [34][39]=68544 [34][40]=68544 [34][41]=68544 [34][42]=68544 [34][43]=68544 [34][44]=68544 [34][45]=68544 [34][46]=68544 [34][47]=68544 [34][48]=68544 [34][49]=68544 [34][50]=68544 [34][51]=68544 [34][52]=68544 [34][53]=68544 [34][54]=68544 [34][55]=68544 [34][56]=68544 [34][57]=68544 [34][58]=68544 [34][59]=68544 [34][60]=68544 [34][61]=68544 [34][62]=68544 [34][63]=68544 \n",
            "[35][0]=70560 [35][1]=70560 [35][2]=70560 [35][3]=70560 [35][4]=70560 [35][5]=70560 [35][6]=70560 [35][7]=70560 [35][8]=70560 [35][9]=70560 [35][10]=70560 [35][11]=70560 [35][12]=70560 [35][13]=70560 [35][14]=70560 [35][15]=70560 [35][16]=70560 [35][17]=70560 [35][18]=70560 [35][19]=70560 [35][20]=70560 [35][21]=70560 [35][22]=70560 [35][23]=70560 [35][24]=70560 [35][25]=70560 [35][26]=70560 [35][27]=70560 [35][28]=70560 [35][29]=70560 [35][30]=70560 [35][31]=70560 [35][32]=70560 [35][33]=70560 [35][34]=70560 [35][35]=70560 [35][36]=70560 [35][37]=70560 [35][38]=70560 [35][39]=70560 [35][40]=70560 [35][41]=70560 [35][42]=70560 [35][43]=70560 [35][44]=70560 [35][45]=70560 [35][46]=70560 [35][47]=70560 [35][48]=70560 [35][49]=70560 [35][50]=70560 [35][51]=70560 [35][52]=70560 [35][53]=70560 [35][54]=70560 [35][55]=70560 [35][56]=70560 [35][57]=70560 [35][58]=70560 [35][59]=70560 [35][60]=70560 [35][61]=70560 [35][62]=70560 [35][63]=70560 \n",
            "[36][0]=72576 [36][1]=72576 [36][2]=72576 [36][3]=72576 [36][4]=72576 [36][5]=72576 [36][6]=72576 [36][7]=72576 [36][8]=72576 [36][9]=72576 [36][10]=72576 [36][11]=72576 [36][12]=72576 [36][13]=72576 [36][14]=72576 [36][15]=72576 [36][16]=72576 [36][17]=72576 [36][18]=72576 [36][19]=72576 [36][20]=72576 [36][21]=72576 [36][22]=72576 [36][23]=72576 [36][24]=72576 [36][25]=72576 [36][26]=72576 [36][27]=72576 [36][28]=72576 [36][29]=72576 [36][30]=72576 [36][31]=72576 [36][32]=72576 [36][33]=72576 [36][34]=72576 [36][35]=72576 [36][36]=72576 [36][37]=72576 [36][38]=72576 [36][39]=72576 [36][40]=72576 [36][41]=72576 [36][42]=72576 [36][43]=72576 [36][44]=72576 [36][45]=72576 [36][46]=72576 [36][47]=72576 [36][48]=72576 [36][49]=72576 [36][50]=72576 [36][51]=72576 [36][52]=72576 [36][53]=72576 [36][54]=72576 [36][55]=72576 [36][56]=72576 [36][57]=72576 [36][58]=72576 [36][59]=72576 [36][60]=72576 [36][61]=72576 [36][62]=72576 [36][63]=72576 \n",
            "[37][0]=74592 [37][1]=74592 [37][2]=74592 [37][3]=74592 [37][4]=74592 [37][5]=74592 [37][6]=74592 [37][7]=74592 [37][8]=74592 [37][9]=74592 [37][10]=74592 [37][11]=74592 [37][12]=74592 [37][13]=74592 [37][14]=74592 [37][15]=74592 [37][16]=74592 [37][17]=74592 [37][18]=74592 [37][19]=74592 [37][20]=74592 [37][21]=74592 [37][22]=74592 [37][23]=74592 [37][24]=74592 [37][25]=74592 [37][26]=74592 [37][27]=74592 [37][28]=74592 [37][29]=74592 [37][30]=74592 [37][31]=74592 [37][32]=74592 [37][33]=74592 [37][34]=74592 [37][35]=74592 [37][36]=74592 [37][37]=74592 [37][38]=74592 [37][39]=74592 [37][40]=74592 [37][41]=74592 [37][42]=74592 [37][43]=74592 [37][44]=74592 [37][45]=74592 [37][46]=74592 [37][47]=74592 [37][48]=74592 [37][49]=74592 [37][50]=74592 [37][51]=74592 [37][52]=74592 [37][53]=74592 [37][54]=74592 [37][55]=74592 [37][56]=74592 [37][57]=74592 [37][58]=74592 [37][59]=74592 [37][60]=74592 [37][61]=74592 [37][62]=74592 [37][63]=74592 \n",
            "[38][0]=76608 [38][1]=76608 [38][2]=76608 [38][3]=76608 [38][4]=76608 [38][5]=76608 [38][6]=76608 [38][7]=76608 [38][8]=76608 [38][9]=76608 [38][10]=76608 [38][11]=76608 [38][12]=76608 [38][13]=76608 [38][14]=76608 [38][15]=76608 [38][16]=76608 [38][17]=76608 [38][18]=76608 [38][19]=76608 [38][20]=76608 [38][21]=76608 [38][22]=76608 [38][23]=76608 [38][24]=76608 [38][25]=76608 [38][26]=76608 [38][27]=76608 [38][28]=76608 [38][29]=76608 [38][30]=76608 [38][31]=76608 [38][32]=76608 [38][33]=76608 [38][34]=76608 [38][35]=76608 [38][36]=76608 [38][37]=76608 [38][38]=76608 [38][39]=76608 [38][40]=76608 [38][41]=76608 [38][42]=76608 [38][43]=76608 [38][44]=76608 [38][45]=76608 [38][46]=76608 [38][47]=76608 [38][48]=76608 [38][49]=76608 [38][50]=76608 [38][51]=76608 [38][52]=76608 [38][53]=76608 [38][54]=76608 [38][55]=76608 [38][56]=76608 [38][57]=76608 [38][58]=76608 [38][59]=76608 [38][60]=76608 [38][61]=76608 [38][62]=76608 [38][63]=76608 \n",
            "[39][0]=78624 [39][1]=78624 [39][2]=78624 [39][3]=78624 [39][4]=78624 [39][5]=78624 [39][6]=78624 [39][7]=78624 [39][8]=78624 [39][9]=78624 [39][10]=78624 [39][11]=78624 [39][12]=78624 [39][13]=78624 [39][14]=78624 [39][15]=78624 [39][16]=78624 [39][17]=78624 [39][18]=78624 [39][19]=78624 [39][20]=78624 [39][21]=78624 [39][22]=78624 [39][23]=78624 [39][24]=78624 [39][25]=78624 [39][26]=78624 [39][27]=78624 [39][28]=78624 [39][29]=78624 [39][30]=78624 [39][31]=78624 [39][32]=78624 [39][33]=78624 [39][34]=78624 [39][35]=78624 [39][36]=78624 [39][37]=78624 [39][38]=78624 [39][39]=78624 [39][40]=78624 [39][41]=78624 [39][42]=78624 [39][43]=78624 [39][44]=78624 [39][45]=78624 [39][46]=78624 [39][47]=78624 [39][48]=78624 [39][49]=78624 [39][50]=78624 [39][51]=78624 [39][52]=78624 [39][53]=78624 [39][54]=78624 [39][55]=78624 [39][56]=78624 [39][57]=78624 [39][58]=78624 [39][59]=78624 [39][60]=78624 [39][61]=78624 [39][62]=78624 [39][63]=78624 \n",
            "[40][0]=80640 [40][1]=80640 [40][2]=80640 [40][3]=80640 [40][4]=80640 [40][5]=80640 [40][6]=80640 [40][7]=80640 [40][8]=80640 [40][9]=80640 [40][10]=80640 [40][11]=80640 [40][12]=80640 [40][13]=80640 [40][14]=80640 [40][15]=80640 [40][16]=80640 [40][17]=80640 [40][18]=80640 [40][19]=80640 [40][20]=80640 [40][21]=80640 [40][22]=80640 [40][23]=80640 [40][24]=80640 [40][25]=80640 [40][26]=80640 [40][27]=80640 [40][28]=80640 [40][29]=80640 [40][30]=80640 [40][31]=80640 [40][32]=80640 [40][33]=80640 [40][34]=80640 [40][35]=80640 [40][36]=80640 [40][37]=80640 [40][38]=80640 [40][39]=80640 [40][40]=80640 [40][41]=80640 [40][42]=80640 [40][43]=80640 [40][44]=80640 [40][45]=80640 [40][46]=80640 [40][47]=80640 [40][48]=80640 [40][49]=80640 [40][50]=80640 [40][51]=80640 [40][52]=80640 [40][53]=80640 [40][54]=80640 [40][55]=80640 [40][56]=80640 [40][57]=80640 [40][58]=80640 [40][59]=80640 [40][60]=80640 [40][61]=80640 [40][62]=80640 [40][63]=80640 \n",
            "[41][0]=82656 [41][1]=82656 [41][2]=82656 [41][3]=82656 [41][4]=82656 [41][5]=82656 [41][6]=82656 [41][7]=82656 [41][8]=82656 [41][9]=82656 [41][10]=82656 [41][11]=82656 [41][12]=82656 [41][13]=82656 [41][14]=82656 [41][15]=82656 [41][16]=82656 [41][17]=82656 [41][18]=82656 [41][19]=82656 [41][20]=82656 [41][21]=82656 [41][22]=82656 [41][23]=82656 [41][24]=82656 [41][25]=82656 [41][26]=82656 [41][27]=82656 [41][28]=82656 [41][29]=82656 [41][30]=82656 [41][31]=82656 [41][32]=82656 [41][33]=82656 [41][34]=82656 [41][35]=82656 [41][36]=82656 [41][37]=82656 [41][38]=82656 [41][39]=82656 [41][40]=82656 [41][41]=82656 [41][42]=82656 [41][43]=82656 [41][44]=82656 [41][45]=82656 [41][46]=82656 [41][47]=82656 [41][48]=82656 [41][49]=82656 [41][50]=82656 [41][51]=82656 [41][52]=82656 [41][53]=82656 [41][54]=82656 [41][55]=82656 [41][56]=82656 [41][57]=82656 [41][58]=82656 [41][59]=82656 [41][60]=82656 [41][61]=82656 [41][62]=82656 [41][63]=82656 \n",
            "[42][0]=84672 [42][1]=84672 [42][2]=84672 [42][3]=84672 [42][4]=84672 [42][5]=84672 [42][6]=84672 [42][7]=84672 [42][8]=84672 [42][9]=84672 [42][10]=84672 [42][11]=84672 [42][12]=84672 [42][13]=84672 [42][14]=84672 [42][15]=84672 [42][16]=84672 [42][17]=84672 [42][18]=84672 [42][19]=84672 [42][20]=84672 [42][21]=84672 [42][22]=84672 [42][23]=84672 [42][24]=84672 [42][25]=84672 [42][26]=84672 [42][27]=84672 [42][28]=84672 [42][29]=84672 [42][30]=84672 [42][31]=84672 [42][32]=84672 [42][33]=84672 [42][34]=84672 [42][35]=84672 [42][36]=84672 [42][37]=84672 [42][38]=84672 [42][39]=84672 [42][40]=84672 [42][41]=84672 [42][42]=84672 [42][43]=84672 [42][44]=84672 [42][45]=84672 [42][46]=84672 [42][47]=84672 [42][48]=84672 [42][49]=84672 [42][50]=84672 [42][51]=84672 [42][52]=84672 [42][53]=84672 [42][54]=84672 [42][55]=84672 [42][56]=84672 [42][57]=84672 [42][58]=84672 [42][59]=84672 [42][60]=84672 [42][61]=84672 [42][62]=84672 [42][63]=84672 \n",
            "[43][0]=86688 [43][1]=86688 [43][2]=86688 [43][3]=86688 [43][4]=86688 [43][5]=86688 [43][6]=86688 [43][7]=86688 [43][8]=86688 [43][9]=86688 [43][10]=86688 [43][11]=86688 [43][12]=86688 [43][13]=86688 [43][14]=86688 [43][15]=86688 [43][16]=86688 [43][17]=86688 [43][18]=86688 [43][19]=86688 [43][20]=86688 [43][21]=86688 [43][22]=86688 [43][23]=86688 [43][24]=86688 [43][25]=86688 [43][26]=86688 [43][27]=86688 [43][28]=86688 [43][29]=86688 [43][30]=86688 [43][31]=86688 [43][32]=86688 [43][33]=86688 [43][34]=86688 [43][35]=86688 [43][36]=86688 [43][37]=86688 [43][38]=86688 [43][39]=86688 [43][40]=86688 [43][41]=86688 [43][42]=86688 [43][43]=86688 [43][44]=86688 [43][45]=86688 [43][46]=86688 [43][47]=86688 [43][48]=86688 [43][49]=86688 [43][50]=86688 [43][51]=86688 [43][52]=86688 [43][53]=86688 [43][54]=86688 [43][55]=86688 [43][56]=86688 [43][57]=86688 [43][58]=86688 [43][59]=86688 [43][60]=86688 [43][61]=86688 [43][62]=86688 [43][63]=86688 \n",
            "[44][0]=88704 [44][1]=88704 [44][2]=88704 [44][3]=88704 [44][4]=88704 [44][5]=88704 [44][6]=88704 [44][7]=88704 [44][8]=88704 [44][9]=88704 [44][10]=88704 [44][11]=88704 [44][12]=88704 [44][13]=88704 [44][14]=88704 [44][15]=88704 [44][16]=88704 [44][17]=88704 [44][18]=88704 [44][19]=88704 [44][20]=88704 [44][21]=88704 [44][22]=88704 [44][23]=88704 [44][24]=88704 [44][25]=88704 [44][26]=88704 [44][27]=88704 [44][28]=88704 [44][29]=88704 [44][30]=88704 [44][31]=88704 [44][32]=88704 [44][33]=88704 [44][34]=88704 [44][35]=88704 [44][36]=88704 [44][37]=88704 [44][38]=88704 [44][39]=88704 [44][40]=88704 [44][41]=88704 [44][42]=88704 [44][43]=88704 [44][44]=88704 [44][45]=88704 [44][46]=88704 [44][47]=88704 [44][48]=88704 [44][49]=88704 [44][50]=88704 [44][51]=88704 [44][52]=88704 [44][53]=88704 [44][54]=88704 [44][55]=88704 [44][56]=88704 [44][57]=88704 [44][58]=88704 [44][59]=88704 [44][60]=88704 [44][61]=88704 [44][62]=88704 [44][63]=88704 \n",
            "[45][0]=90720 [45][1]=90720 [45][2]=90720 [45][3]=90720 [45][4]=90720 [45][5]=90720 [45][6]=90720 [45][7]=90720 [45][8]=90720 [45][9]=90720 [45][10]=90720 [45][11]=90720 [45][12]=90720 [45][13]=90720 [45][14]=90720 [45][15]=90720 [45][16]=90720 [45][17]=90720 [45][18]=90720 [45][19]=90720 [45][20]=90720 [45][21]=90720 [45][22]=90720 [45][23]=90720 [45][24]=90720 [45][25]=90720 [45][26]=90720 [45][27]=90720 [45][28]=90720 [45][29]=90720 [45][30]=90720 [45][31]=90720 [45][32]=90720 [45][33]=90720 [45][34]=90720 [45][35]=90720 [45][36]=90720 [45][37]=90720 [45][38]=90720 [45][39]=90720 [45][40]=90720 [45][41]=90720 [45][42]=90720 [45][43]=90720 [45][44]=90720 [45][45]=90720 [45][46]=90720 [45][47]=90720 [45][48]=90720 [45][49]=90720 [45][50]=90720 [45][51]=90720 [45][52]=90720 [45][53]=90720 [45][54]=90720 [45][55]=90720 [45][56]=90720 [45][57]=90720 [45][58]=90720 [45][59]=90720 [45][60]=90720 [45][61]=90720 [45][62]=90720 [45][63]=90720 \n",
            "[46][0]=92736 [46][1]=92736 [46][2]=92736 [46][3]=92736 [46][4]=92736 [46][5]=92736 [46][6]=92736 [46][7]=92736 [46][8]=92736 [46][9]=92736 [46][10]=92736 [46][11]=92736 [46][12]=92736 [46][13]=92736 [46][14]=92736 [46][15]=92736 [46][16]=92736 [46][17]=92736 [46][18]=92736 [46][19]=92736 [46][20]=92736 [46][21]=92736 [46][22]=92736 [46][23]=92736 [46][24]=92736 [46][25]=92736 [46][26]=92736 [46][27]=92736 [46][28]=92736 [46][29]=92736 [46][30]=92736 [46][31]=92736 [46][32]=92736 [46][33]=92736 [46][34]=92736 [46][35]=92736 [46][36]=92736 [46][37]=92736 [46][38]=92736 [46][39]=92736 [46][40]=92736 [46][41]=92736 [46][42]=92736 [46][43]=92736 [46][44]=92736 [46][45]=92736 [46][46]=92736 [46][47]=92736 [46][48]=92736 [46][49]=92736 [46][50]=92736 [46][51]=92736 [46][52]=92736 [46][53]=92736 [46][54]=92736 [46][55]=92736 [46][56]=92736 [46][57]=92736 [46][58]=92736 [46][59]=92736 [46][60]=92736 [46][61]=92736 [46][62]=92736 [46][63]=92736 \n",
            "[47][0]=94752 [47][1]=94752 [47][2]=94752 [47][3]=94752 [47][4]=94752 [47][5]=94752 [47][6]=94752 [47][7]=94752 [47][8]=94752 [47][9]=94752 [47][10]=94752 [47][11]=94752 [47][12]=94752 [47][13]=94752 [47][14]=94752 [47][15]=94752 [47][16]=94752 [47][17]=94752 [47][18]=94752 [47][19]=94752 [47][20]=94752 [47][21]=94752 [47][22]=94752 [47][23]=94752 [47][24]=94752 [47][25]=94752 [47][26]=94752 [47][27]=94752 [47][28]=94752 [47][29]=94752 [47][30]=94752 [47][31]=94752 [47][32]=94752 [47][33]=94752 [47][34]=94752 [47][35]=94752 [47][36]=94752 [47][37]=94752 [47][38]=94752 [47][39]=94752 [47][40]=94752 [47][41]=94752 [47][42]=94752 [47][43]=94752 [47][44]=94752 [47][45]=94752 [47][46]=94752 [47][47]=94752 [47][48]=94752 [47][49]=94752 [47][50]=94752 [47][51]=94752 [47][52]=94752 [47][53]=94752 [47][54]=94752 [47][55]=94752 [47][56]=94752 [47][57]=94752 [47][58]=94752 [47][59]=94752 [47][60]=94752 [47][61]=94752 [47][62]=94752 [47][63]=94752 \n",
            "[48][0]=96768 [48][1]=96768 [48][2]=96768 [48][3]=96768 [48][4]=96768 [48][5]=96768 [48][6]=96768 [48][7]=96768 [48][8]=96768 [48][9]=96768 [48][10]=96768 [48][11]=96768 [48][12]=96768 [48][13]=96768 [48][14]=96768 [48][15]=96768 [48][16]=96768 [48][17]=96768 [48][18]=96768 [48][19]=96768 [48][20]=96768 [48][21]=96768 [48][22]=96768 [48][23]=96768 [48][24]=96768 [48][25]=96768 [48][26]=96768 [48][27]=96768 [48][28]=96768 [48][29]=96768 [48][30]=96768 [48][31]=96768 [48][32]=96768 [48][33]=96768 [48][34]=96768 [48][35]=96768 [48][36]=96768 [48][37]=96768 [48][38]=96768 [48][39]=96768 [48][40]=96768 [48][41]=96768 [48][42]=96768 [48][43]=96768 [48][44]=96768 [48][45]=96768 [48][46]=96768 [48][47]=96768 [48][48]=96768 [48][49]=96768 [48][50]=96768 [48][51]=96768 [48][52]=96768 [48][53]=96768 [48][54]=96768 [48][55]=96768 [48][56]=96768 [48][57]=96768 [48][58]=96768 [48][59]=96768 [48][60]=96768 [48][61]=96768 [48][62]=96768 [48][63]=96768 \n",
            "[49][0]=98784 [49][1]=98784 [49][2]=98784 [49][3]=98784 [49][4]=98784 [49][5]=98784 [49][6]=98784 [49][7]=98784 [49][8]=98784 [49][9]=98784 [49][10]=98784 [49][11]=98784 [49][12]=98784 [49][13]=98784 [49][14]=98784 [49][15]=98784 [49][16]=98784 [49][17]=98784 [49][18]=98784 [49][19]=98784 [49][20]=98784 [49][21]=98784 [49][22]=98784 [49][23]=98784 [49][24]=98784 [49][25]=98784 [49][26]=98784 [49][27]=98784 [49][28]=98784 [49][29]=98784 [49][30]=98784 [49][31]=98784 [49][32]=98784 [49][33]=98784 [49][34]=98784 [49][35]=98784 [49][36]=98784 [49][37]=98784 [49][38]=98784 [49][39]=98784 [49][40]=98784 [49][41]=98784 [49][42]=98784 [49][43]=98784 [49][44]=98784 [49][45]=98784 [49][46]=98784 [49][47]=98784 [49][48]=98784 [49][49]=98784 [49][50]=98784 [49][51]=98784 [49][52]=98784 [49][53]=98784 [49][54]=98784 [49][55]=98784 [49][56]=98784 [49][57]=98784 [49][58]=98784 [49][59]=98784 [49][60]=98784 [49][61]=98784 [49][62]=98784 [49][63]=98784 \n",
            "[50][0]=100800 [50][1]=100800 [50][2]=100800 [50][3]=100800 [50][4]=100800 [50][5]=100800 [50][6]=100800 [50][7]=100800 [50][8]=100800 [50][9]=100800 [50][10]=100800 [50][11]=100800 [50][12]=100800 [50][13]=100800 [50][14]=100800 [50][15]=100800 [50][16]=100800 [50][17]=100800 [50][18]=100800 [50][19]=100800 [50][20]=100800 [50][21]=100800 [50][22]=100800 [50][23]=100800 [50][24]=100800 [50][25]=100800 [50][26]=100800 [50][27]=100800 [50][28]=100800 [50][29]=100800 [50][30]=100800 [50][31]=100800 [50][32]=100800 [50][33]=100800 [50][34]=100800 [50][35]=100800 [50][36]=100800 [50][37]=100800 [50][38]=100800 [50][39]=100800 [50][40]=100800 [50][41]=100800 [50][42]=100800 [50][43]=100800 [50][44]=100800 [50][45]=100800 [50][46]=100800 [50][47]=100800 [50][48]=100800 [50][49]=100800 [50][50]=100800 [50][51]=100800 [50][52]=100800 [50][53]=100800 [50][54]=100800 [50][55]=100800 [50][56]=100800 [50][57]=100800 [50][58]=100800 [50][59]=100800 [50][60]=100800 [50][61]=100800 [50][62]=100800 [50][63]=100800 \n",
            "[51][0]=102816 [51][1]=102816 [51][2]=102816 [51][3]=102816 [51][4]=102816 [51][5]=102816 [51][6]=102816 [51][7]=102816 [51][8]=102816 [51][9]=102816 [51][10]=102816 [51][11]=102816 [51][12]=102816 [51][13]=102816 [51][14]=102816 [51][15]=102816 [51][16]=102816 [51][17]=102816 [51][18]=102816 [51][19]=102816 [51][20]=102816 [51][21]=102816 [51][22]=102816 [51][23]=102816 [51][24]=102816 [51][25]=102816 [51][26]=102816 [51][27]=102816 [51][28]=102816 [51][29]=102816 [51][30]=102816 [51][31]=102816 [51][32]=102816 [51][33]=102816 [51][34]=102816 [51][35]=102816 [51][36]=102816 [51][37]=102816 [51][38]=102816 [51][39]=102816 [51][40]=102816 [51][41]=102816 [51][42]=102816 [51][43]=102816 [51][44]=102816 [51][45]=102816 [51][46]=102816 [51][47]=102816 [51][48]=102816 [51][49]=102816 [51][50]=102816 [51][51]=102816 [51][52]=102816 [51][53]=102816 [51][54]=102816 [51][55]=102816 [51][56]=102816 [51][57]=102816 [51][58]=102816 [51][59]=102816 [51][60]=102816 [51][61]=102816 [51][62]=102816 [51][63]=102816 \n",
            "[52][0]=104832 [52][1]=104832 [52][2]=104832 [52][3]=104832 [52][4]=104832 [52][5]=104832 [52][6]=104832 [52][7]=104832 [52][8]=104832 [52][9]=104832 [52][10]=104832 [52][11]=104832 [52][12]=104832 [52][13]=104832 [52][14]=104832 [52][15]=104832 [52][16]=104832 [52][17]=104832 [52][18]=104832 [52][19]=104832 [52][20]=104832 [52][21]=104832 [52][22]=104832 [52][23]=104832 [52][24]=104832 [52][25]=104832 [52][26]=104832 [52][27]=104832 [52][28]=104832 [52][29]=104832 [52][30]=104832 [52][31]=104832 [52][32]=104832 [52][33]=104832 [52][34]=104832 [52][35]=104832 [52][36]=104832 [52][37]=104832 [52][38]=104832 [52][39]=104832 [52][40]=104832 [52][41]=104832 [52][42]=104832 [52][43]=104832 [52][44]=104832 [52][45]=104832 [52][46]=104832 [52][47]=104832 [52][48]=104832 [52][49]=104832 [52][50]=104832 [52][51]=104832 [52][52]=104832 [52][53]=104832 [52][54]=104832 [52][55]=104832 [52][56]=104832 [52][57]=104832 [52][58]=104832 [52][59]=104832 [52][60]=104832 [52][61]=104832 [52][62]=104832 [52][63]=104832 \n",
            "[53][0]=106848 [53][1]=106848 [53][2]=106848 [53][3]=106848 [53][4]=106848 [53][5]=106848 [53][6]=106848 [53][7]=106848 [53][8]=106848 [53][9]=106848 [53][10]=106848 [53][11]=106848 [53][12]=106848 [53][13]=106848 [53][14]=106848 [53][15]=106848 [53][16]=106848 [53][17]=106848 [53][18]=106848 [53][19]=106848 [53][20]=106848 [53][21]=106848 [53][22]=106848 [53][23]=106848 [53][24]=106848 [53][25]=106848 [53][26]=106848 [53][27]=106848 [53][28]=106848 [53][29]=106848 [53][30]=106848 [53][31]=106848 [53][32]=106848 [53][33]=106848 [53][34]=106848 [53][35]=106848 [53][36]=106848 [53][37]=106848 [53][38]=106848 [53][39]=106848 [53][40]=106848 [53][41]=106848 [53][42]=106848 [53][43]=106848 [53][44]=106848 [53][45]=106848 [53][46]=106848 [53][47]=106848 [53][48]=106848 [53][49]=106848 [53][50]=106848 [53][51]=106848 [53][52]=106848 [53][53]=106848 [53][54]=106848 [53][55]=106848 [53][56]=106848 [53][57]=106848 [53][58]=106848 [53][59]=106848 [53][60]=106848 [53][61]=106848 [53][62]=106848 [53][63]=106848 \n",
            "[54][0]=108864 [54][1]=108864 [54][2]=108864 [54][3]=108864 [54][4]=108864 [54][5]=108864 [54][6]=108864 [54][7]=108864 [54][8]=108864 [54][9]=108864 [54][10]=108864 [54][11]=108864 [54][12]=108864 [54][13]=108864 [54][14]=108864 [54][15]=108864 [54][16]=108864 [54][17]=108864 [54][18]=108864 [54][19]=108864 [54][20]=108864 [54][21]=108864 [54][22]=108864 [54][23]=108864 [54][24]=108864 [54][25]=108864 [54][26]=108864 [54][27]=108864 [54][28]=108864 [54][29]=108864 [54][30]=108864 [54][31]=108864 [54][32]=108864 [54][33]=108864 [54][34]=108864 [54][35]=108864 [54][36]=108864 [54][37]=108864 [54][38]=108864 [54][39]=108864 [54][40]=108864 [54][41]=108864 [54][42]=108864 [54][43]=108864 [54][44]=108864 [54][45]=108864 [54][46]=108864 [54][47]=108864 [54][48]=108864 [54][49]=108864 [54][50]=108864 [54][51]=108864 [54][52]=108864 [54][53]=108864 [54][54]=108864 [54][55]=108864 [54][56]=108864 [54][57]=108864 [54][58]=108864 [54][59]=108864 [54][60]=108864 [54][61]=108864 [54][62]=108864 [54][63]=108864 \n",
            "[55][0]=110880 [55][1]=110880 [55][2]=110880 [55][3]=110880 [55][4]=110880 [55][5]=110880 [55][6]=110880 [55][7]=110880 [55][8]=110880 [55][9]=110880 [55][10]=110880 [55][11]=110880 [55][12]=110880 [55][13]=110880 [55][14]=110880 [55][15]=110880 [55][16]=110880 [55][17]=110880 [55][18]=110880 [55][19]=110880 [55][20]=110880 [55][21]=110880 [55][22]=110880 [55][23]=110880 [55][24]=110880 [55][25]=110880 [55][26]=110880 [55][27]=110880 [55][28]=110880 [55][29]=110880 [55][30]=110880 [55][31]=110880 [55][32]=110880 [55][33]=110880 [55][34]=110880 [55][35]=110880 [55][36]=110880 [55][37]=110880 [55][38]=110880 [55][39]=110880 [55][40]=110880 [55][41]=110880 [55][42]=110880 [55][43]=110880 [55][44]=110880 [55][45]=110880 [55][46]=110880 [55][47]=110880 [55][48]=110880 [55][49]=110880 [55][50]=110880 [55][51]=110880 [55][52]=110880 [55][53]=110880 [55][54]=110880 [55][55]=110880 [55][56]=110880 [55][57]=110880 [55][58]=110880 [55][59]=110880 [55][60]=110880 [55][61]=110880 [55][62]=110880 [55][63]=110880 \n",
            "[56][0]=112896 [56][1]=112896 [56][2]=112896 [56][3]=112896 [56][4]=112896 [56][5]=112896 [56][6]=112896 [56][7]=112896 [56][8]=112896 [56][9]=112896 [56][10]=112896 [56][11]=112896 [56][12]=112896 [56][13]=112896 [56][14]=112896 [56][15]=112896 [56][16]=112896 [56][17]=112896 [56][18]=112896 [56][19]=112896 [56][20]=112896 [56][21]=112896 [56][22]=112896 [56][23]=112896 [56][24]=112896 [56][25]=112896 [56][26]=112896 [56][27]=112896 [56][28]=112896 [56][29]=112896 [56][30]=112896 [56][31]=112896 [56][32]=112896 [56][33]=112896 [56][34]=112896 [56][35]=112896 [56][36]=112896 [56][37]=112896 [56][38]=112896 [56][39]=112896 [56][40]=112896 [56][41]=112896 [56][42]=112896 [56][43]=112896 [56][44]=112896 [56][45]=112896 [56][46]=112896 [56][47]=112896 [56][48]=112896 [56][49]=112896 [56][50]=112896 [56][51]=112896 [56][52]=112896 [56][53]=112896 [56][54]=112896 [56][55]=112896 [56][56]=112896 [56][57]=112896 [56][58]=112896 [56][59]=112896 [56][60]=112896 [56][61]=112896 [56][62]=112896 [56][63]=112896 \n",
            "[57][0]=114912 [57][1]=114912 [57][2]=114912 [57][3]=114912 [57][4]=114912 [57][5]=114912 [57][6]=114912 [57][7]=114912 [57][8]=114912 [57][9]=114912 [57][10]=114912 [57][11]=114912 [57][12]=114912 [57][13]=114912 [57][14]=114912 [57][15]=114912 [57][16]=114912 [57][17]=114912 [57][18]=114912 [57][19]=114912 [57][20]=114912 [57][21]=114912 [57][22]=114912 [57][23]=114912 [57][24]=114912 [57][25]=114912 [57][26]=114912 [57][27]=114912 [57][28]=114912 [57][29]=114912 [57][30]=114912 [57][31]=114912 [57][32]=114912 [57][33]=114912 [57][34]=114912 [57][35]=114912 [57][36]=114912 [57][37]=114912 [57][38]=114912 [57][39]=114912 [57][40]=114912 [57][41]=114912 [57][42]=114912 [57][43]=114912 [57][44]=114912 [57][45]=114912 [57][46]=114912 [57][47]=114912 [57][48]=114912 [57][49]=114912 [57][50]=114912 [57][51]=114912 [57][52]=114912 [57][53]=114912 [57][54]=114912 [57][55]=114912 [57][56]=114912 [57][57]=114912 [57][58]=114912 [57][59]=114912 [57][60]=114912 [57][61]=114912 [57][62]=114912 [57][63]=114912 \n",
            "[58][0]=116928 [58][1]=116928 [58][2]=116928 [58][3]=116928 [58][4]=116928 [58][5]=116928 [58][6]=116928 [58][7]=116928 [58][8]=116928 [58][9]=116928 [58][10]=116928 [58][11]=116928 [58][12]=116928 [58][13]=116928 [58][14]=116928 [58][15]=116928 [58][16]=116928 [58][17]=116928 [58][18]=116928 [58][19]=116928 [58][20]=116928 [58][21]=116928 [58][22]=116928 [58][23]=116928 [58][24]=116928 [58][25]=116928 [58][26]=116928 [58][27]=116928 [58][28]=116928 [58][29]=116928 [58][30]=116928 [58][31]=116928 [58][32]=116928 [58][33]=116928 [58][34]=116928 [58][35]=116928 [58][36]=116928 [58][37]=116928 [58][38]=116928 [58][39]=116928 [58][40]=116928 [58][41]=116928 [58][42]=116928 [58][43]=116928 [58][44]=116928 [58][45]=116928 [58][46]=116928 [58][47]=116928 [58][48]=116928 [58][49]=116928 [58][50]=116928 [58][51]=116928 [58][52]=116928 [58][53]=116928 [58][54]=116928 [58][55]=116928 [58][56]=116928 [58][57]=116928 [58][58]=116928 [58][59]=116928 [58][60]=116928 [58][61]=116928 [58][62]=116928 [58][63]=116928 \n",
            "[59][0]=118944 [59][1]=118944 [59][2]=118944 [59][3]=118944 [59][4]=118944 [59][5]=118944 [59][6]=118944 [59][7]=118944 [59][8]=118944 [59][9]=118944 [59][10]=118944 [59][11]=118944 [59][12]=118944 [59][13]=118944 [59][14]=118944 [59][15]=118944 [59][16]=118944 [59][17]=118944 [59][18]=118944 [59][19]=118944 [59][20]=118944 [59][21]=118944 [59][22]=118944 [59][23]=118944 [59][24]=118944 [59][25]=118944 [59][26]=118944 [59][27]=118944 [59][28]=118944 [59][29]=118944 [59][30]=118944 [59][31]=118944 [59][32]=118944 [59][33]=118944 [59][34]=118944 [59][35]=118944 [59][36]=118944 [59][37]=118944 [59][38]=118944 [59][39]=118944 [59][40]=118944 [59][41]=118944 [59][42]=118944 [59][43]=118944 [59][44]=118944 [59][45]=118944 [59][46]=118944 [59][47]=118944 [59][48]=118944 [59][49]=118944 [59][50]=118944 [59][51]=118944 [59][52]=118944 [59][53]=118944 [59][54]=118944 [59][55]=118944 [59][56]=118944 [59][57]=118944 [59][58]=118944 [59][59]=118944 [59][60]=118944 [59][61]=118944 [59][62]=118944 [59][63]=118944 \n",
            "[60][0]=120960 [60][1]=120960 [60][2]=120960 [60][3]=120960 [60][4]=120960 [60][5]=120960 [60][6]=120960 [60][7]=120960 [60][8]=120960 [60][9]=120960 [60][10]=120960 [60][11]=120960 [60][12]=120960 [60][13]=120960 [60][14]=120960 [60][15]=120960 [60][16]=120960 [60][17]=120960 [60][18]=120960 [60][19]=120960 [60][20]=120960 [60][21]=120960 [60][22]=120960 [60][23]=120960 [60][24]=120960 [60][25]=120960 [60][26]=120960 [60][27]=120960 [60][28]=120960 [60][29]=120960 [60][30]=120960 [60][31]=120960 [60][32]=120960 [60][33]=120960 [60][34]=120960 [60][35]=120960 [60][36]=120960 [60][37]=120960 [60][38]=120960 [60][39]=120960 [60][40]=120960 [60][41]=120960 [60][42]=120960 [60][43]=120960 [60][44]=120960 [60][45]=120960 [60][46]=120960 [60][47]=120960 [60][48]=120960 [60][49]=120960 [60][50]=120960 [60][51]=120960 [60][52]=120960 [60][53]=120960 [60][54]=120960 [60][55]=120960 [60][56]=120960 [60][57]=120960 [60][58]=120960 [60][59]=120960 [60][60]=120960 [60][61]=120960 [60][62]=120960 [60][63]=120960 \n",
            "[61][0]=122976 [61][1]=122976 [61][2]=122976 [61][3]=122976 [61][4]=122976 [61][5]=122976 [61][6]=122976 [61][7]=122976 [61][8]=122976 [61][9]=122976 [61][10]=122976 [61][11]=122976 [61][12]=122976 [61][13]=122976 [61][14]=122976 [61][15]=122976 [61][16]=122976 [61][17]=122976 [61][18]=122976 [61][19]=122976 [61][20]=122976 [61][21]=122976 [61][22]=122976 [61][23]=122976 [61][24]=122976 [61][25]=122976 [61][26]=122976 [61][27]=122976 [61][28]=122976 [61][29]=122976 [61][30]=122976 [61][31]=122976 [61][32]=122976 [61][33]=122976 [61][34]=122976 [61][35]=122976 [61][36]=122976 [61][37]=122976 [61][38]=122976 [61][39]=122976 [61][40]=122976 [61][41]=122976 [61][42]=122976 [61][43]=122976 [61][44]=122976 [61][45]=122976 [61][46]=122976 [61][47]=122976 [61][48]=122976 [61][49]=122976 [61][50]=122976 [61][51]=122976 [61][52]=122976 [61][53]=122976 [61][54]=122976 [61][55]=122976 [61][56]=122976 [61][57]=122976 [61][58]=122976 [61][59]=122976 [61][60]=122976 [61][61]=122976 [61][62]=122976 [61][63]=122976 \n",
            "[62][0]=124992 [62][1]=124992 [62][2]=124992 [62][3]=124992 [62][4]=124992 [62][5]=124992 [62][6]=124992 [62][7]=124992 [62][8]=124992 [62][9]=124992 [62][10]=124992 [62][11]=124992 [62][12]=124992 [62][13]=124992 [62][14]=124992 [62][15]=124992 [62][16]=124992 [62][17]=124992 [62][18]=124992 [62][19]=124992 [62][20]=124992 [62][21]=124992 [62][22]=124992 [62][23]=124992 [62][24]=124992 [62][25]=124992 [62][26]=124992 [62][27]=124992 [62][28]=124992 [62][29]=124992 [62][30]=124992 [62][31]=124992 [62][32]=124992 [62][33]=124992 [62][34]=124992 [62][35]=124992 [62][36]=124992 [62][37]=124992 [62][38]=124992 [62][39]=124992 [62][40]=124992 [62][41]=124992 [62][42]=124992 [62][43]=124992 [62][44]=124992 [62][45]=124992 [62][46]=124992 [62][47]=124992 [62][48]=124992 [62][49]=124992 [62][50]=124992 [62][51]=124992 [62][52]=124992 [62][53]=124992 [62][54]=124992 [62][55]=124992 [62][56]=124992 [62][57]=124992 [62][58]=124992 [62][59]=124992 [62][60]=124992 [62][61]=124992 [62][62]=124992 [62][63]=124992 \n",
            "[63][0]=127008 [63][1]=127008 [63][2]=127008 [63][3]=127008 [63][4]=127008 [63][5]=127008 [63][6]=127008 [63][7]=127008 [63][8]=127008 [63][9]=127008 [63][10]=127008 [63][11]=127008 [63][12]=127008 [63][13]=127008 [63][14]=127008 [63][15]=127008 [63][16]=127008 [63][17]=127008 [63][18]=127008 [63][19]=127008 [63][20]=127008 [63][21]=127008 [63][22]=127008 [63][23]=127008 [63][24]=127008 [63][25]=127008 [63][26]=127008 [63][27]=127008 [63][28]=127008 [63][29]=127008 [63][30]=127008 [63][31]=127008 [63][32]=127008 [63][33]=127008 [63][34]=127008 [63][35]=127008 [63][36]=127008 [63][37]=127008 [63][38]=127008 [63][39]=127008 [63][40]=127008 [63][41]=127008 [63][42]=127008 [63][43]=127008 [63][44]=127008 [63][45]=127008 [63][46]=127008 [63][47]=127008 [63][48]=127008 [63][49]=127008 [63][50]=127008 [63][51]=127008 [63][52]=127008 [63][53]=127008 [63][54]=127008 [63][55]=127008 [63][56]=127008 [63][57]=127008 [63][58]=127008 [63][59]=127008 [63][60]=127008 [63][61]=127008 [63][62]=127008 [63][63]=127008 \n",
            "GigaFlops: 20.981188==1564== Profiling application: ./matrixMul_gpu2\n",
            "==1564== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.92%  19.804ms      1000  19.803us  19.136us  20.479us  matrixMultGPU(int*, int*, int*)\n",
            "                    0.05%  10.528us         2  5.2640us  5.1840us  5.3440us  [CUDA memcpy HtoD]\n",
            "                    0.02%  4.6720us         1  4.6720us  4.6720us  4.6720us  [CUDA memcpy DtoH]\n",
            "      API calls:   88.92%  206.94ms         2  103.47ms  1.1730us  206.94ms  cudaEventCreate\n",
            "                    8.54%  19.867ms         1  19.867ms  19.867ms  19.867ms  cudaEventSynchronize\n",
            "                    2.04%  4.7514ms      1000  4.7510us  3.5490us  38.557us  cudaLaunchKernel\n",
            "                    0.22%  503.88us         1  503.88us  503.88us  503.88us  cuDeviceTotalMem\n",
            "                    0.08%  179.87us       101  1.7800us     144ns  77.925us  cuDeviceGetAttribute\n",
            "                    0.08%  178.02us         3  59.341us  2.0790us  171.52us  cudaMalloc\n",
            "                    0.07%  151.61us         3  50.536us  2.8820us  139.05us  cudaFree\n",
            "                    0.05%  106.22us         3  35.406us  17.734us  64.862us  cudaMemcpy\n",
            "                    0.01%  26.124us         1  26.124us  26.124us  26.124us  cuDeviceGetName\n",
            "                    0.00%  10.473us         2  5.2360us  3.2380us  7.2350us  cudaEventRecord\n",
            "                    0.00%  5.5880us         1  5.5880us  5.5880us  5.5880us  cuDeviceGetPCIBusId\n",
            "                    0.00%  3.2730us         1  3.2730us  3.2730us  3.2730us  cudaEventElapsedTime\n",
            "                    0.00%  1.7300us         3     576ns     170ns     847ns  cuDeviceGetCount\n",
            "                    0.00%  1.2270us         2     613ns     268ns     959ns  cuDeviceGet\n",
            "                    0.00%     307ns         1     307ns     307ns     307ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Xb4PI9f9jZA"
      },
      "source": [
        "Comparamos el producto de NxN con el uso de memoria compartida y con optimizaciones:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXBalU6W9iFZ",
        "outputId": "c4cbf1a5-7cc9-4c81-b74a-4d7101e09abf"
      },
      "source": [
        "%%writefile matrixMul_gpu3.cu\n",
        "#include <stdio.h>\n",
        "#define N 64\n",
        "#define MAX 8\n",
        "\n",
        "__global__ void matrixMultGPU(int *a, int *b, int *c, int n){\n",
        "    int k, sum = 0;\n",
        "    int thx = threadIdx.x;\n",
        "    int thy = threadIdx.y;\n",
        "    int col = thx + blockDim.x * blockIdx.x;\n",
        "    int fil = thy + blockDim.y * blockIdx.y;\n",
        "\n",
        "    __shared__ float Aij[MAX][MAX];\n",
        "    __shared__ float Bij[MAX][MAX];\n",
        "\n",
        "    for (int lim = 0; lim < (n + MAX - 1)/MAX; lim++){\n",
        "        if ((thy + (lim*MAX)) < n  && col < n){\n",
        "            Aij[thy][thx] = a[(col*n) + (thy + (lim*MAX))];\n",
        "        }\n",
        "        else {\n",
        "            Aij[thy][thx] = 0.0;\n",
        "        }\n",
        "        if ((thx + (lim*MAX)) < n  && fil < n){\n",
        "            Bij[thy][thx] = b[((thx + (lim*MAX))*n) + fil];\n",
        "        }\n",
        "        else{\n",
        "            Bij[thy][thx] = 0.0;\n",
        "        }\n",
        "        __syncthreads();\n",
        "    #pragma unroll\n",
        "        for (k = 0; k < MAX; k++){\n",
        "            sum += Aij[k][thx] * Bij[thy][k];\n",
        "        }\n",
        "\n",
        "        __syncthreads();\n",
        "    }\n",
        "    if (col < n && fil < n){\n",
        "        c[col * n + fil] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    int a[N][N], b[N][N], c[N][N];\n",
        "    int *dev_a, *dev_b, *dev_c;\n",
        "    int cont, i , j;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    for (i = 0; i < N; i++){\n",
        "        cont = 0;\n",
        "        for (j = 0; j < N; j++){\n",
        "            a[i][j] = cont;\n",
        "            b[i][j] = cont;\n",
        "            cont++;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    int size = N*N*sizeof(int);\n",
        "    cudaMalloc((void **) &dev_a, size);\n",
        "    cudaMalloc((void **) &dev_b, size);\n",
        "    cudaMalloc((void **) &dev_c, size);\n",
        "\n",
        "    cudaMemcpy(dev_a, a, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 dimGrid((N + MAX - 1)/MAX, (N + MAX - 1)/MAX);\n",
        "    dim3 dimBlock(MAX, MAX);\n",
        "\n",
        "    int nIter = 1000;\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    for (int m = 0; m < nIter; m++){\n",
        "        matrixMultGPU<<<dimGrid, dimBlock>>>(dev_a, dev_b, dev_c, N);\n",
        "    }\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    cudaMemcpy(c, dev_c, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    float msecTotal = 0.0;\n",
        "    cudaEventElapsedTime(&msecTotal, start, stop);\n",
        "\n",
        "    cudaFree(dev_a);\n",
        "    cudaFree(dev_b);\n",
        "    cudaFree(dev_c);\n",
        "\n",
        "    //for (int y = 0; y < N; y++){\n",
        "    //    for (int x = 0; x < N; x++){\n",
        "    //        printf(\"[%d][%d]=%d \", y, x, c[x][y]);\n",
        "    //    }\n",
        "    //    printf(\"\\n\");\n",
        "    //}\n",
        "\n",
        "    float msecPerKernelExecution = msecTotal / nIter;\n",
        "    double flopsPerMMull = 2.0 * N * N * N;\n",
        "    double gigaFlops = (flopsPerMMull * 1.0e-9f) / (msecPerKernelExecution / 1000.0f);\n",
        "\n",
        "    printf(\"GigaFlops: %f\", gigaFlops);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matrixMul_gpu3.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9WNnL2GA1RK",
        "outputId": "bc74a8b1-3ed7-44aa-f18d-fb5e167a1a8b"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true matrixMul_gpu3.cu -o matrixMul_gpu3 -lcudadevrt\n",
        "!nvprof ./matrixMul_gpu3"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==1944== NVPROF is profiling process 1944, command: ./matrixMul_gpu3\n",
            "==1944== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "==1944== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
            "[0][0]=0 [0][1]=0 [0][2]=0 [0][3]=0 [0][4]=0 [0][5]=0 [0][6]=0 [0][7]=0 [0][8]=0 [0][9]=0 [0][10]=0 [0][11]=0 [0][12]=0 [0][13]=0 [0][14]=0 [0][15]=0 [0][16]=0 [0][17]=0 [0][18]=0 [0][19]=0 [0][20]=0 [0][21]=0 [0][22]=0 [0][23]=0 [0][24]=0 [0][25]=0 [0][26]=0 [0][27]=0 [0][28]=0 [0][29]=0 [0][30]=0 [0][31]=0 [0][32]=0 [0][33]=0 [0][34]=0 [0][35]=0 [0][36]=0 [0][37]=0 [0][38]=0 [0][39]=0 [0][40]=0 [0][41]=0 [0][42]=0 [0][43]=0 [0][44]=0 [0][45]=0 [0][46]=0 [0][47]=0 [0][48]=0 [0][49]=0 [0][50]=0 [0][51]=0 [0][52]=0 [0][53]=0 [0][54]=0 [0][55]=0 [0][56]=0 [0][57]=0 [0][58]=0 [0][59]=0 [0][60]=0 [0][61]=0 [0][62]=0 [0][63]=0 \n",
            "[1][0]=2016 [1][1]=2016 [1][2]=2016 [1][3]=2016 [1][4]=2016 [1][5]=2016 [1][6]=2016 [1][7]=2016 [1][8]=2016 [1][9]=2016 [1][10]=2016 [1][11]=2016 [1][12]=2016 [1][13]=2016 [1][14]=2016 [1][15]=2016 [1][16]=2016 [1][17]=2016 [1][18]=2016 [1][19]=2016 [1][20]=2016 [1][21]=2016 [1][22]=2016 [1][23]=2016 [1][24]=2016 [1][25]=2016 [1][26]=2016 [1][27]=2016 [1][28]=2016 [1][29]=2016 [1][30]=2016 [1][31]=2016 [1][32]=2016 [1][33]=2016 [1][34]=2016 [1][35]=2016 [1][36]=2016 [1][37]=2016 [1][38]=2016 [1][39]=2016 [1][40]=2016 [1][41]=2016 [1][42]=2016 [1][43]=2016 [1][44]=2016 [1][45]=2016 [1][46]=2016 [1][47]=2016 [1][48]=2016 [1][49]=2016 [1][50]=2016 [1][51]=2016 [1][52]=2016 [1][53]=2016 [1][54]=2016 [1][55]=2016 [1][56]=2016 [1][57]=2016 [1][58]=2016 [1][59]=2016 [1][60]=2016 [1][61]=2016 [1][62]=2016 [1][63]=2016 \n",
            "[2][0]=4032 [2][1]=4032 [2][2]=4032 [2][3]=4032 [2][4]=4032 [2][5]=4032 [2][6]=4032 [2][7]=4032 [2][8]=4032 [2][9]=4032 [2][10]=4032 [2][11]=4032 [2][12]=4032 [2][13]=4032 [2][14]=4032 [2][15]=4032 [2][16]=4032 [2][17]=4032 [2][18]=4032 [2][19]=4032 [2][20]=4032 [2][21]=4032 [2][22]=4032 [2][23]=4032 [2][24]=4032 [2][25]=4032 [2][26]=4032 [2][27]=4032 [2][28]=4032 [2][29]=4032 [2][30]=4032 [2][31]=4032 [2][32]=4032 [2][33]=4032 [2][34]=4032 [2][35]=4032 [2][36]=4032 [2][37]=4032 [2][38]=4032 [2][39]=4032 [2][40]=4032 [2][41]=4032 [2][42]=4032 [2][43]=4032 [2][44]=4032 [2][45]=4032 [2][46]=4032 [2][47]=4032 [2][48]=4032 [2][49]=4032 [2][50]=4032 [2][51]=4032 [2][52]=4032 [2][53]=4032 [2][54]=4032 [2][55]=4032 [2][56]=4032 [2][57]=4032 [2][58]=4032 [2][59]=4032 [2][60]=4032 [2][61]=4032 [2][62]=4032 [2][63]=4032 \n",
            "[3][0]=6048 [3][1]=6048 [3][2]=6048 [3][3]=6048 [3][4]=6048 [3][5]=6048 [3][6]=6048 [3][7]=6048 [3][8]=6048 [3][9]=6048 [3][10]=6048 [3][11]=6048 [3][12]=6048 [3][13]=6048 [3][14]=6048 [3][15]=6048 [3][16]=6048 [3][17]=6048 [3][18]=6048 [3][19]=6048 [3][20]=6048 [3][21]=6048 [3][22]=6048 [3][23]=6048 [3][24]=6048 [3][25]=6048 [3][26]=6048 [3][27]=6048 [3][28]=6048 [3][29]=6048 [3][30]=6048 [3][31]=6048 [3][32]=6048 [3][33]=6048 [3][34]=6048 [3][35]=6048 [3][36]=6048 [3][37]=6048 [3][38]=6048 [3][39]=6048 [3][40]=6048 [3][41]=6048 [3][42]=6048 [3][43]=6048 [3][44]=6048 [3][45]=6048 [3][46]=6048 [3][47]=6048 [3][48]=6048 [3][49]=6048 [3][50]=6048 [3][51]=6048 [3][52]=6048 [3][53]=6048 [3][54]=6048 [3][55]=6048 [3][56]=6048 [3][57]=6048 [3][58]=6048 [3][59]=6048 [3][60]=6048 [3][61]=6048 [3][62]=6048 [3][63]=6048 \n",
            "[4][0]=8064 [4][1]=8064 [4][2]=8064 [4][3]=8064 [4][4]=8064 [4][5]=8064 [4][6]=8064 [4][7]=8064 [4][8]=8064 [4][9]=8064 [4][10]=8064 [4][11]=8064 [4][12]=8064 [4][13]=8064 [4][14]=8064 [4][15]=8064 [4][16]=8064 [4][17]=8064 [4][18]=8064 [4][19]=8064 [4][20]=8064 [4][21]=8064 [4][22]=8064 [4][23]=8064 [4][24]=8064 [4][25]=8064 [4][26]=8064 [4][27]=8064 [4][28]=8064 [4][29]=8064 [4][30]=8064 [4][31]=8064 [4][32]=8064 [4][33]=8064 [4][34]=8064 [4][35]=8064 [4][36]=8064 [4][37]=8064 [4][38]=8064 [4][39]=8064 [4][40]=8064 [4][41]=8064 [4][42]=8064 [4][43]=8064 [4][44]=8064 [4][45]=8064 [4][46]=8064 [4][47]=8064 [4][48]=8064 [4][49]=8064 [4][50]=8064 [4][51]=8064 [4][52]=8064 [4][53]=8064 [4][54]=8064 [4][55]=8064 [4][56]=8064 [4][57]=8064 [4][58]=8064 [4][59]=8064 [4][60]=8064 [4][61]=8064 [4][62]=8064 [4][63]=8064 \n",
            "[5][0]=10080 [5][1]=10080 [5][2]=10080 [5][3]=10080 [5][4]=10080 [5][5]=10080 [5][6]=10080 [5][7]=10080 [5][8]=10080 [5][9]=10080 [5][10]=10080 [5][11]=10080 [5][12]=10080 [5][13]=10080 [5][14]=10080 [5][15]=10080 [5][16]=10080 [5][17]=10080 [5][18]=10080 [5][19]=10080 [5][20]=10080 [5][21]=10080 [5][22]=10080 [5][23]=10080 [5][24]=10080 [5][25]=10080 [5][26]=10080 [5][27]=10080 [5][28]=10080 [5][29]=10080 [5][30]=10080 [5][31]=10080 [5][32]=10080 [5][33]=10080 [5][34]=10080 [5][35]=10080 [5][36]=10080 [5][37]=10080 [5][38]=10080 [5][39]=10080 [5][40]=10080 [5][41]=10080 [5][42]=10080 [5][43]=10080 [5][44]=10080 [5][45]=10080 [5][46]=10080 [5][47]=10080 [5][48]=10080 [5][49]=10080 [5][50]=10080 [5][51]=10080 [5][52]=10080 [5][53]=10080 [5][54]=10080 [5][55]=10080 [5][56]=10080 [5][57]=10080 [5][58]=10080 [5][59]=10080 [5][60]=10080 [5][61]=10080 [5][62]=10080 [5][63]=10080 \n",
            "[6][0]=12096 [6][1]=12096 [6][2]=12096 [6][3]=12096 [6][4]=12096 [6][5]=12096 [6][6]=12096 [6][7]=12096 [6][8]=12096 [6][9]=12096 [6][10]=12096 [6][11]=12096 [6][12]=12096 [6][13]=12096 [6][14]=12096 [6][15]=12096 [6][16]=12096 [6][17]=12096 [6][18]=12096 [6][19]=12096 [6][20]=12096 [6][21]=12096 [6][22]=12096 [6][23]=12096 [6][24]=12096 [6][25]=12096 [6][26]=12096 [6][27]=12096 [6][28]=12096 [6][29]=12096 [6][30]=12096 [6][31]=12096 [6][32]=12096 [6][33]=12096 [6][34]=12096 [6][35]=12096 [6][36]=12096 [6][37]=12096 [6][38]=12096 [6][39]=12096 [6][40]=12096 [6][41]=12096 [6][42]=12096 [6][43]=12096 [6][44]=12096 [6][45]=12096 [6][46]=12096 [6][47]=12096 [6][48]=12096 [6][49]=12096 [6][50]=12096 [6][51]=12096 [6][52]=12096 [6][53]=12096 [6][54]=12096 [6][55]=12096 [6][56]=12096 [6][57]=12096 [6][58]=12096 [6][59]=12096 [6][60]=12096 [6][61]=12096 [6][62]=12096 [6][63]=12096 \n",
            "[7][0]=14112 [7][1]=14112 [7][2]=14112 [7][3]=14112 [7][4]=14112 [7][5]=14112 [7][6]=14112 [7][7]=14112 [7][8]=14112 [7][9]=14112 [7][10]=14112 [7][11]=14112 [7][12]=14112 [7][13]=14112 [7][14]=14112 [7][15]=14112 [7][16]=14112 [7][17]=14112 [7][18]=14112 [7][19]=14112 [7][20]=14112 [7][21]=14112 [7][22]=14112 [7][23]=14112 [7][24]=14112 [7][25]=14112 [7][26]=14112 [7][27]=14112 [7][28]=14112 [7][29]=14112 [7][30]=14112 [7][31]=14112 [7][32]=14112 [7][33]=14112 [7][34]=14112 [7][35]=14112 [7][36]=14112 [7][37]=14112 [7][38]=14112 [7][39]=14112 [7][40]=14112 [7][41]=14112 [7][42]=14112 [7][43]=14112 [7][44]=14112 [7][45]=14112 [7][46]=14112 [7][47]=14112 [7][48]=14112 [7][49]=14112 [7][50]=14112 [7][51]=14112 [7][52]=14112 [7][53]=14112 [7][54]=14112 [7][55]=14112 [7][56]=14112 [7][57]=14112 [7][58]=14112 [7][59]=14112 [7][60]=14112 [7][61]=14112 [7][62]=14112 [7][63]=14112 \n",
            "[8][0]=16128 [8][1]=16128 [8][2]=16128 [8][3]=16128 [8][4]=16128 [8][5]=16128 [8][6]=16128 [8][7]=16128 [8][8]=16128 [8][9]=16128 [8][10]=16128 [8][11]=16128 [8][12]=16128 [8][13]=16128 [8][14]=16128 [8][15]=16128 [8][16]=16128 [8][17]=16128 [8][18]=16128 [8][19]=16128 [8][20]=16128 [8][21]=16128 [8][22]=16128 [8][23]=16128 [8][24]=16128 [8][25]=16128 [8][26]=16128 [8][27]=16128 [8][28]=16128 [8][29]=16128 [8][30]=16128 [8][31]=16128 [8][32]=16128 [8][33]=16128 [8][34]=16128 [8][35]=16128 [8][36]=16128 [8][37]=16128 [8][38]=16128 [8][39]=16128 [8][40]=16128 [8][41]=16128 [8][42]=16128 [8][43]=16128 [8][44]=16128 [8][45]=16128 [8][46]=16128 [8][47]=16128 [8][48]=16128 [8][49]=16128 [8][50]=16128 [8][51]=16128 [8][52]=16128 [8][53]=16128 [8][54]=16128 [8][55]=16128 [8][56]=16128 [8][57]=16128 [8][58]=16128 [8][59]=16128 [8][60]=16128 [8][61]=16128 [8][62]=16128 [8][63]=16128 \n",
            "[9][0]=18144 [9][1]=18144 [9][2]=18144 [9][3]=18144 [9][4]=18144 [9][5]=18144 [9][6]=18144 [9][7]=18144 [9][8]=18144 [9][9]=18144 [9][10]=18144 [9][11]=18144 [9][12]=18144 [9][13]=18144 [9][14]=18144 [9][15]=18144 [9][16]=18144 [9][17]=18144 [9][18]=18144 [9][19]=18144 [9][20]=18144 [9][21]=18144 [9][22]=18144 [9][23]=18144 [9][24]=18144 [9][25]=18144 [9][26]=18144 [9][27]=18144 [9][28]=18144 [9][29]=18144 [9][30]=18144 [9][31]=18144 [9][32]=18144 [9][33]=18144 [9][34]=18144 [9][35]=18144 [9][36]=18144 [9][37]=18144 [9][38]=18144 [9][39]=18144 [9][40]=18144 [9][41]=18144 [9][42]=18144 [9][43]=18144 [9][44]=18144 [9][45]=18144 [9][46]=18144 [9][47]=18144 [9][48]=18144 [9][49]=18144 [9][50]=18144 [9][51]=18144 [9][52]=18144 [9][53]=18144 [9][54]=18144 [9][55]=18144 [9][56]=18144 [9][57]=18144 [9][58]=18144 [9][59]=18144 [9][60]=18144 [9][61]=18144 [9][62]=18144 [9][63]=18144 \n",
            "[10][0]=20160 [10][1]=20160 [10][2]=20160 [10][3]=20160 [10][4]=20160 [10][5]=20160 [10][6]=20160 [10][7]=20160 [10][8]=20160 [10][9]=20160 [10][10]=20160 [10][11]=20160 [10][12]=20160 [10][13]=20160 [10][14]=20160 [10][15]=20160 [10][16]=20160 [10][17]=20160 [10][18]=20160 [10][19]=20160 [10][20]=20160 [10][21]=20160 [10][22]=20160 [10][23]=20160 [10][24]=20160 [10][25]=20160 [10][26]=20160 [10][27]=20160 [10][28]=20160 [10][29]=20160 [10][30]=20160 [10][31]=20160 [10][32]=20160 [10][33]=20160 [10][34]=20160 [10][35]=20160 [10][36]=20160 [10][37]=20160 [10][38]=20160 [10][39]=20160 [10][40]=20160 [10][41]=20160 [10][42]=20160 [10][43]=20160 [10][44]=20160 [10][45]=20160 [10][46]=20160 [10][47]=20160 [10][48]=20160 [10][49]=20160 [10][50]=20160 [10][51]=20160 [10][52]=20160 [10][53]=20160 [10][54]=20160 [10][55]=20160 [10][56]=20160 [10][57]=20160 [10][58]=20160 [10][59]=20160 [10][60]=20160 [10][61]=20160 [10][62]=20160 [10][63]=20160 \n",
            "[11][0]=22176 [11][1]=22176 [11][2]=22176 [11][3]=22176 [11][4]=22176 [11][5]=22176 [11][6]=22176 [11][7]=22176 [11][8]=22176 [11][9]=22176 [11][10]=22176 [11][11]=22176 [11][12]=22176 [11][13]=22176 [11][14]=22176 [11][15]=22176 [11][16]=22176 [11][17]=22176 [11][18]=22176 [11][19]=22176 [11][20]=22176 [11][21]=22176 [11][22]=22176 [11][23]=22176 [11][24]=22176 [11][25]=22176 [11][26]=22176 [11][27]=22176 [11][28]=22176 [11][29]=22176 [11][30]=22176 [11][31]=22176 [11][32]=22176 [11][33]=22176 [11][34]=22176 [11][35]=22176 [11][36]=22176 [11][37]=22176 [11][38]=22176 [11][39]=22176 [11][40]=22176 [11][41]=22176 [11][42]=22176 [11][43]=22176 [11][44]=22176 [11][45]=22176 [11][46]=22176 [11][47]=22176 [11][48]=22176 [11][49]=22176 [11][50]=22176 [11][51]=22176 [11][52]=22176 [11][53]=22176 [11][54]=22176 [11][55]=22176 [11][56]=22176 [11][57]=22176 [11][58]=22176 [11][59]=22176 [11][60]=22176 [11][61]=22176 [11][62]=22176 [11][63]=22176 \n",
            "[12][0]=24192 [12][1]=24192 [12][2]=24192 [12][3]=24192 [12][4]=24192 [12][5]=24192 [12][6]=24192 [12][7]=24192 [12][8]=24192 [12][9]=24192 [12][10]=24192 [12][11]=24192 [12][12]=24192 [12][13]=24192 [12][14]=24192 [12][15]=24192 [12][16]=24192 [12][17]=24192 [12][18]=24192 [12][19]=24192 [12][20]=24192 [12][21]=24192 [12][22]=24192 [12][23]=24192 [12][24]=24192 [12][25]=24192 [12][26]=24192 [12][27]=24192 [12][28]=24192 [12][29]=24192 [12][30]=24192 [12][31]=24192 [12][32]=24192 [12][33]=24192 [12][34]=24192 [12][35]=24192 [12][36]=24192 [12][37]=24192 [12][38]=24192 [12][39]=24192 [12][40]=24192 [12][41]=24192 [12][42]=24192 [12][43]=24192 [12][44]=24192 [12][45]=24192 [12][46]=24192 [12][47]=24192 [12][48]=24192 [12][49]=24192 [12][50]=24192 [12][51]=24192 [12][52]=24192 [12][53]=24192 [12][54]=24192 [12][55]=24192 [12][56]=24192 [12][57]=24192 [12][58]=24192 [12][59]=24192 [12][60]=24192 [12][61]=24192 [12][62]=24192 [12][63]=24192 \n",
            "[13][0]=26208 [13][1]=26208 [13][2]=26208 [13][3]=26208 [13][4]=26208 [13][5]=26208 [13][6]=26208 [13][7]=26208 [13][8]=26208 [13][9]=26208 [13][10]=26208 [13][11]=26208 [13][12]=26208 [13][13]=26208 [13][14]=26208 [13][15]=26208 [13][16]=26208 [13][17]=26208 [13][18]=26208 [13][19]=26208 [13][20]=26208 [13][21]=26208 [13][22]=26208 [13][23]=26208 [13][24]=26208 [13][25]=26208 [13][26]=26208 [13][27]=26208 [13][28]=26208 [13][29]=26208 [13][30]=26208 [13][31]=26208 [13][32]=26208 [13][33]=26208 [13][34]=26208 [13][35]=26208 [13][36]=26208 [13][37]=26208 [13][38]=26208 [13][39]=26208 [13][40]=26208 [13][41]=26208 [13][42]=26208 [13][43]=26208 [13][44]=26208 [13][45]=26208 [13][46]=26208 [13][47]=26208 [13][48]=26208 [13][49]=26208 [13][50]=26208 [13][51]=26208 [13][52]=26208 [13][53]=26208 [13][54]=26208 [13][55]=26208 [13][56]=26208 [13][57]=26208 [13][58]=26208 [13][59]=26208 [13][60]=26208 [13][61]=26208 [13][62]=26208 [13][63]=26208 \n",
            "[14][0]=28224 [14][1]=28224 [14][2]=28224 [14][3]=28224 [14][4]=28224 [14][5]=28224 [14][6]=28224 [14][7]=28224 [14][8]=28224 [14][9]=28224 [14][10]=28224 [14][11]=28224 [14][12]=28224 [14][13]=28224 [14][14]=28224 [14][15]=28224 [14][16]=28224 [14][17]=28224 [14][18]=28224 [14][19]=28224 [14][20]=28224 [14][21]=28224 [14][22]=28224 [14][23]=28224 [14][24]=28224 [14][25]=28224 [14][26]=28224 [14][27]=28224 [14][28]=28224 [14][29]=28224 [14][30]=28224 [14][31]=28224 [14][32]=28224 [14][33]=28224 [14][34]=28224 [14][35]=28224 [14][36]=28224 [14][37]=28224 [14][38]=28224 [14][39]=28224 [14][40]=28224 [14][41]=28224 [14][42]=28224 [14][43]=28224 [14][44]=28224 [14][45]=28224 [14][46]=28224 [14][47]=28224 [14][48]=28224 [14][49]=28224 [14][50]=28224 [14][51]=28224 [14][52]=28224 [14][53]=28224 [14][54]=28224 [14][55]=28224 [14][56]=28224 [14][57]=28224 [14][58]=28224 [14][59]=28224 [14][60]=28224 [14][61]=28224 [14][62]=28224 [14][63]=28224 \n",
            "[15][0]=30240 [15][1]=30240 [15][2]=30240 [15][3]=30240 [15][4]=30240 [15][5]=30240 [15][6]=30240 [15][7]=30240 [15][8]=30240 [15][9]=30240 [15][10]=30240 [15][11]=30240 [15][12]=30240 [15][13]=30240 [15][14]=30240 [15][15]=30240 [15][16]=30240 [15][17]=30240 [15][18]=30240 [15][19]=30240 [15][20]=30240 [15][21]=30240 [15][22]=30240 [15][23]=30240 [15][24]=30240 [15][25]=30240 [15][26]=30240 [15][27]=30240 [15][28]=30240 [15][29]=30240 [15][30]=30240 [15][31]=30240 [15][32]=30240 [15][33]=30240 [15][34]=30240 [15][35]=30240 [15][36]=30240 [15][37]=30240 [15][38]=30240 [15][39]=30240 [15][40]=30240 [15][41]=30240 [15][42]=30240 [15][43]=30240 [15][44]=30240 [15][45]=30240 [15][46]=30240 [15][47]=30240 [15][48]=30240 [15][49]=30240 [15][50]=30240 [15][51]=30240 [15][52]=30240 [15][53]=30240 [15][54]=30240 [15][55]=30240 [15][56]=30240 [15][57]=30240 [15][58]=30240 [15][59]=30240 [15][60]=30240 [15][61]=30240 [15][62]=30240 [15][63]=30240 \n",
            "[16][0]=32256 [16][1]=32256 [16][2]=32256 [16][3]=32256 [16][4]=32256 [16][5]=32256 [16][6]=32256 [16][7]=32256 [16][8]=32256 [16][9]=32256 [16][10]=32256 [16][11]=32256 [16][12]=32256 [16][13]=32256 [16][14]=32256 [16][15]=32256 [16][16]=32256 [16][17]=32256 [16][18]=32256 [16][19]=32256 [16][20]=32256 [16][21]=32256 [16][22]=32256 [16][23]=32256 [16][24]=32256 [16][25]=32256 [16][26]=32256 [16][27]=32256 [16][28]=32256 [16][29]=32256 [16][30]=32256 [16][31]=32256 [16][32]=32256 [16][33]=32256 [16][34]=32256 [16][35]=32256 [16][36]=32256 [16][37]=32256 [16][38]=32256 [16][39]=32256 [16][40]=32256 [16][41]=32256 [16][42]=32256 [16][43]=32256 [16][44]=32256 [16][45]=32256 [16][46]=32256 [16][47]=32256 [16][48]=32256 [16][49]=32256 [16][50]=32256 [16][51]=32256 [16][52]=32256 [16][53]=32256 [16][54]=32256 [16][55]=32256 [16][56]=32256 [16][57]=32256 [16][58]=32256 [16][59]=32256 [16][60]=32256 [16][61]=32256 [16][62]=32256 [16][63]=32256 \n",
            "[17][0]=34272 [17][1]=34272 [17][2]=34272 [17][3]=34272 [17][4]=34272 [17][5]=34272 [17][6]=34272 [17][7]=34272 [17][8]=34272 [17][9]=34272 [17][10]=34272 [17][11]=34272 [17][12]=34272 [17][13]=34272 [17][14]=34272 [17][15]=34272 [17][16]=34272 [17][17]=34272 [17][18]=34272 [17][19]=34272 [17][20]=34272 [17][21]=34272 [17][22]=34272 [17][23]=34272 [17][24]=34272 [17][25]=34272 [17][26]=34272 [17][27]=34272 [17][28]=34272 [17][29]=34272 [17][30]=34272 [17][31]=34272 [17][32]=34272 [17][33]=34272 [17][34]=34272 [17][35]=34272 [17][36]=34272 [17][37]=34272 [17][38]=34272 [17][39]=34272 [17][40]=34272 [17][41]=34272 [17][42]=34272 [17][43]=34272 [17][44]=34272 [17][45]=34272 [17][46]=34272 [17][47]=34272 [17][48]=34272 [17][49]=34272 [17][50]=34272 [17][51]=34272 [17][52]=34272 [17][53]=34272 [17][54]=34272 [17][55]=34272 [17][56]=34272 [17][57]=34272 [17][58]=34272 [17][59]=34272 [17][60]=34272 [17][61]=34272 [17][62]=34272 [17][63]=34272 \n",
            "[18][0]=36288 [18][1]=36288 [18][2]=36288 [18][3]=36288 [18][4]=36288 [18][5]=36288 [18][6]=36288 [18][7]=36288 [18][8]=36288 [18][9]=36288 [18][10]=36288 [18][11]=36288 [18][12]=36288 [18][13]=36288 [18][14]=36288 [18][15]=36288 [18][16]=36288 [18][17]=36288 [18][18]=36288 [18][19]=36288 [18][20]=36288 [18][21]=36288 [18][22]=36288 [18][23]=36288 [18][24]=36288 [18][25]=36288 [18][26]=36288 [18][27]=36288 [18][28]=36288 [18][29]=36288 [18][30]=36288 [18][31]=36288 [18][32]=36288 [18][33]=36288 [18][34]=36288 [18][35]=36288 [18][36]=36288 [18][37]=36288 [18][38]=36288 [18][39]=36288 [18][40]=36288 [18][41]=36288 [18][42]=36288 [18][43]=36288 [18][44]=36288 [18][45]=36288 [18][46]=36288 [18][47]=36288 [18][48]=36288 [18][49]=36288 [18][50]=36288 [18][51]=36288 [18][52]=36288 [18][53]=36288 [18][54]=36288 [18][55]=36288 [18][56]=36288 [18][57]=36288 [18][58]=36288 [18][59]=36288 [18][60]=36288 [18][61]=36288 [18][62]=36288 [18][63]=36288 \n",
            "[19][0]=38304 [19][1]=38304 [19][2]=38304 [19][3]=38304 [19][4]=38304 [19][5]=38304 [19][6]=38304 [19][7]=38304 [19][8]=38304 [19][9]=38304 [19][10]=38304 [19][11]=38304 [19][12]=38304 [19][13]=38304 [19][14]=38304 [19][15]=38304 [19][16]=38304 [19][17]=38304 [19][18]=38304 [19][19]=38304 [19][20]=38304 [19][21]=38304 [19][22]=38304 [19][23]=38304 [19][24]=38304 [19][25]=38304 [19][26]=38304 [19][27]=38304 [19][28]=38304 [19][29]=38304 [19][30]=38304 [19][31]=38304 [19][32]=38304 [19][33]=38304 [19][34]=38304 [19][35]=38304 [19][36]=38304 [19][37]=38304 [19][38]=38304 [19][39]=38304 [19][40]=38304 [19][41]=38304 [19][42]=38304 [19][43]=38304 [19][44]=38304 [19][45]=38304 [19][46]=38304 [19][47]=38304 [19][48]=38304 [19][49]=38304 [19][50]=38304 [19][51]=38304 [19][52]=38304 [19][53]=38304 [19][54]=38304 [19][55]=38304 [19][56]=38304 [19][57]=38304 [19][58]=38304 [19][59]=38304 [19][60]=38304 [19][61]=38304 [19][62]=38304 [19][63]=38304 \n",
            "[20][0]=40320 [20][1]=40320 [20][2]=40320 [20][3]=40320 [20][4]=40320 [20][5]=40320 [20][6]=40320 [20][7]=40320 [20][8]=40320 [20][9]=40320 [20][10]=40320 [20][11]=40320 [20][12]=40320 [20][13]=40320 [20][14]=40320 [20][15]=40320 [20][16]=40320 [20][17]=40320 [20][18]=40320 [20][19]=40320 [20][20]=40320 [20][21]=40320 [20][22]=40320 [20][23]=40320 [20][24]=40320 [20][25]=40320 [20][26]=40320 [20][27]=40320 [20][28]=40320 [20][29]=40320 [20][30]=40320 [20][31]=40320 [20][32]=40320 [20][33]=40320 [20][34]=40320 [20][35]=40320 [20][36]=40320 [20][37]=40320 [20][38]=40320 [20][39]=40320 [20][40]=40320 [20][41]=40320 [20][42]=40320 [20][43]=40320 [20][44]=40320 [20][45]=40320 [20][46]=40320 [20][47]=40320 [20][48]=40320 [20][49]=40320 [20][50]=40320 [20][51]=40320 [20][52]=40320 [20][53]=40320 [20][54]=40320 [20][55]=40320 [20][56]=40320 [20][57]=40320 [20][58]=40320 [20][59]=40320 [20][60]=40320 [20][61]=40320 [20][62]=40320 [20][63]=40320 \n",
            "[21][0]=42336 [21][1]=42336 [21][2]=42336 [21][3]=42336 [21][4]=42336 [21][5]=42336 [21][6]=42336 [21][7]=42336 [21][8]=42336 [21][9]=42336 [21][10]=42336 [21][11]=42336 [21][12]=42336 [21][13]=42336 [21][14]=42336 [21][15]=42336 [21][16]=42336 [21][17]=42336 [21][18]=42336 [21][19]=42336 [21][20]=42336 [21][21]=42336 [21][22]=42336 [21][23]=42336 [21][24]=42336 [21][25]=42336 [21][26]=42336 [21][27]=42336 [21][28]=42336 [21][29]=42336 [21][30]=42336 [21][31]=42336 [21][32]=42336 [21][33]=42336 [21][34]=42336 [21][35]=42336 [21][36]=42336 [21][37]=42336 [21][38]=42336 [21][39]=42336 [21][40]=42336 [21][41]=42336 [21][42]=42336 [21][43]=42336 [21][44]=42336 [21][45]=42336 [21][46]=42336 [21][47]=42336 [21][48]=42336 [21][49]=42336 [21][50]=42336 [21][51]=42336 [21][52]=42336 [21][53]=42336 [21][54]=42336 [21][55]=42336 [21][56]=42336 [21][57]=42336 [21][58]=42336 [21][59]=42336 [21][60]=42336 [21][61]=42336 [21][62]=42336 [21][63]=42336 \n",
            "[22][0]=44352 [22][1]=44352 [22][2]=44352 [22][3]=44352 [22][4]=44352 [22][5]=44352 [22][6]=44352 [22][7]=44352 [22][8]=44352 [22][9]=44352 [22][10]=44352 [22][11]=44352 [22][12]=44352 [22][13]=44352 [22][14]=44352 [22][15]=44352 [22][16]=44352 [22][17]=44352 [22][18]=44352 [22][19]=44352 [22][20]=44352 [22][21]=44352 [22][22]=44352 [22][23]=44352 [22][24]=44352 [22][25]=44352 [22][26]=44352 [22][27]=44352 [22][28]=44352 [22][29]=44352 [22][30]=44352 [22][31]=44352 [22][32]=44352 [22][33]=44352 [22][34]=44352 [22][35]=44352 [22][36]=44352 [22][37]=44352 [22][38]=44352 [22][39]=44352 [22][40]=44352 [22][41]=44352 [22][42]=44352 [22][43]=44352 [22][44]=44352 [22][45]=44352 [22][46]=44352 [22][47]=44352 [22][48]=44352 [22][49]=44352 [22][50]=44352 [22][51]=44352 [22][52]=44352 [22][53]=44352 [22][54]=44352 [22][55]=44352 [22][56]=44352 [22][57]=44352 [22][58]=44352 [22][59]=44352 [22][60]=44352 [22][61]=44352 [22][62]=44352 [22][63]=44352 \n",
            "[23][0]=46368 [23][1]=46368 [23][2]=46368 [23][3]=46368 [23][4]=46368 [23][5]=46368 [23][6]=46368 [23][7]=46368 [23][8]=46368 [23][9]=46368 [23][10]=46368 [23][11]=46368 [23][12]=46368 [23][13]=46368 [23][14]=46368 [23][15]=46368 [23][16]=46368 [23][17]=46368 [23][18]=46368 [23][19]=46368 [23][20]=46368 [23][21]=46368 [23][22]=46368 [23][23]=46368 [23][24]=46368 [23][25]=46368 [23][26]=46368 [23][27]=46368 [23][28]=46368 [23][29]=46368 [23][30]=46368 [23][31]=46368 [23][32]=46368 [23][33]=46368 [23][34]=46368 [23][35]=46368 [23][36]=46368 [23][37]=46368 [23][38]=46368 [23][39]=46368 [23][40]=46368 [23][41]=46368 [23][42]=46368 [23][43]=46368 [23][44]=46368 [23][45]=46368 [23][46]=46368 [23][47]=46368 [23][48]=46368 [23][49]=46368 [23][50]=46368 [23][51]=46368 [23][52]=46368 [23][53]=46368 [23][54]=46368 [23][55]=46368 [23][56]=46368 [23][57]=46368 [23][58]=46368 [23][59]=46368 [23][60]=46368 [23][61]=46368 [23][62]=46368 [23][63]=46368 \n",
            "[24][0]=48384 [24][1]=48384 [24][2]=48384 [24][3]=48384 [24][4]=48384 [24][5]=48384 [24][6]=48384 [24][7]=48384 [24][8]=48384 [24][9]=48384 [24][10]=48384 [24][11]=48384 [24][12]=48384 [24][13]=48384 [24][14]=48384 [24][15]=48384 [24][16]=48384 [24][17]=48384 [24][18]=48384 [24][19]=48384 [24][20]=48384 [24][21]=48384 [24][22]=48384 [24][23]=48384 [24][24]=48384 [24][25]=48384 [24][26]=48384 [24][27]=48384 [24][28]=48384 [24][29]=48384 [24][30]=48384 [24][31]=48384 [24][32]=48384 [24][33]=48384 [24][34]=48384 [24][35]=48384 [24][36]=48384 [24][37]=48384 [24][38]=48384 [24][39]=48384 [24][40]=48384 [24][41]=48384 [24][42]=48384 [24][43]=48384 [24][44]=48384 [24][45]=48384 [24][46]=48384 [24][47]=48384 [24][48]=48384 [24][49]=48384 [24][50]=48384 [24][51]=48384 [24][52]=48384 [24][53]=48384 [24][54]=48384 [24][55]=48384 [24][56]=48384 [24][57]=48384 [24][58]=48384 [24][59]=48384 [24][60]=48384 [24][61]=48384 [24][62]=48384 [24][63]=48384 \n",
            "[25][0]=50400 [25][1]=50400 [25][2]=50400 [25][3]=50400 [25][4]=50400 [25][5]=50400 [25][6]=50400 [25][7]=50400 [25][8]=50400 [25][9]=50400 [25][10]=50400 [25][11]=50400 [25][12]=50400 [25][13]=50400 [25][14]=50400 [25][15]=50400 [25][16]=50400 [25][17]=50400 [25][18]=50400 [25][19]=50400 [25][20]=50400 [25][21]=50400 [25][22]=50400 [25][23]=50400 [25][24]=50400 [25][25]=50400 [25][26]=50400 [25][27]=50400 [25][28]=50400 [25][29]=50400 [25][30]=50400 [25][31]=50400 [25][32]=50400 [25][33]=50400 [25][34]=50400 [25][35]=50400 [25][36]=50400 [25][37]=50400 [25][38]=50400 [25][39]=50400 [25][40]=50400 [25][41]=50400 [25][42]=50400 [25][43]=50400 [25][44]=50400 [25][45]=50400 [25][46]=50400 [25][47]=50400 [25][48]=50400 [25][49]=50400 [25][50]=50400 [25][51]=50400 [25][52]=50400 [25][53]=50400 [25][54]=50400 [25][55]=50400 [25][56]=50400 [25][57]=50400 [25][58]=50400 [25][59]=50400 [25][60]=50400 [25][61]=50400 [25][62]=50400 [25][63]=50400 \n",
            "[26][0]=52416 [26][1]=52416 [26][2]=52416 [26][3]=52416 [26][4]=52416 [26][5]=52416 [26][6]=52416 [26][7]=52416 [26][8]=52416 [26][9]=52416 [26][10]=52416 [26][11]=52416 [26][12]=52416 [26][13]=52416 [26][14]=52416 [26][15]=52416 [26][16]=52416 [26][17]=52416 [26][18]=52416 [26][19]=52416 [26][20]=52416 [26][21]=52416 [26][22]=52416 [26][23]=52416 [26][24]=52416 [26][25]=52416 [26][26]=52416 [26][27]=52416 [26][28]=52416 [26][29]=52416 [26][30]=52416 [26][31]=52416 [26][32]=52416 [26][33]=52416 [26][34]=52416 [26][35]=52416 [26][36]=52416 [26][37]=52416 [26][38]=52416 [26][39]=52416 [26][40]=52416 [26][41]=52416 [26][42]=52416 [26][43]=52416 [26][44]=52416 [26][45]=52416 [26][46]=52416 [26][47]=52416 [26][48]=52416 [26][49]=52416 [26][50]=52416 [26][51]=52416 [26][52]=52416 [26][53]=52416 [26][54]=52416 [26][55]=52416 [26][56]=52416 [26][57]=52416 [26][58]=52416 [26][59]=52416 [26][60]=52416 [26][61]=52416 [26][62]=52416 [26][63]=52416 \n",
            "[27][0]=54432 [27][1]=54432 [27][2]=54432 [27][3]=54432 [27][4]=54432 [27][5]=54432 [27][6]=54432 [27][7]=54432 [27][8]=54432 [27][9]=54432 [27][10]=54432 [27][11]=54432 [27][12]=54432 [27][13]=54432 [27][14]=54432 [27][15]=54432 [27][16]=54432 [27][17]=54432 [27][18]=54432 [27][19]=54432 [27][20]=54432 [27][21]=54432 [27][22]=54432 [27][23]=54432 [27][24]=54432 [27][25]=54432 [27][26]=54432 [27][27]=54432 [27][28]=54432 [27][29]=54432 [27][30]=54432 [27][31]=54432 [27][32]=54432 [27][33]=54432 [27][34]=54432 [27][35]=54432 [27][36]=54432 [27][37]=54432 [27][38]=54432 [27][39]=54432 [27][40]=54432 [27][41]=54432 [27][42]=54432 [27][43]=54432 [27][44]=54432 [27][45]=54432 [27][46]=54432 [27][47]=54432 [27][48]=54432 [27][49]=54432 [27][50]=54432 [27][51]=54432 [27][52]=54432 [27][53]=54432 [27][54]=54432 [27][55]=54432 [27][56]=54432 [27][57]=54432 [27][58]=54432 [27][59]=54432 [27][60]=54432 [27][61]=54432 [27][62]=54432 [27][63]=54432 \n",
            "[28][0]=56448 [28][1]=56448 [28][2]=56448 [28][3]=56448 [28][4]=56448 [28][5]=56448 [28][6]=56448 [28][7]=56448 [28][8]=56448 [28][9]=56448 [28][10]=56448 [28][11]=56448 [28][12]=56448 [28][13]=56448 [28][14]=56448 [28][15]=56448 [28][16]=56448 [28][17]=56448 [28][18]=56448 [28][19]=56448 [28][20]=56448 [28][21]=56448 [28][22]=56448 [28][23]=56448 [28][24]=56448 [28][25]=56448 [28][26]=56448 [28][27]=56448 [28][28]=56448 [28][29]=56448 [28][30]=56448 [28][31]=56448 [28][32]=56448 [28][33]=56448 [28][34]=56448 [28][35]=56448 [28][36]=56448 [28][37]=56448 [28][38]=56448 [28][39]=56448 [28][40]=56448 [28][41]=56448 [28][42]=56448 [28][43]=56448 [28][44]=56448 [28][45]=56448 [28][46]=56448 [28][47]=56448 [28][48]=56448 [28][49]=56448 [28][50]=56448 [28][51]=56448 [28][52]=56448 [28][53]=56448 [28][54]=56448 [28][55]=56448 [28][56]=56448 [28][57]=56448 [28][58]=56448 [28][59]=56448 [28][60]=56448 [28][61]=56448 [28][62]=56448 [28][63]=56448 \n",
            "[29][0]=58464 [29][1]=58464 [29][2]=58464 [29][3]=58464 [29][4]=58464 [29][5]=58464 [29][6]=58464 [29][7]=58464 [29][8]=58464 [29][9]=58464 [29][10]=58464 [29][11]=58464 [29][12]=58464 [29][13]=58464 [29][14]=58464 [29][15]=58464 [29][16]=58464 [29][17]=58464 [29][18]=58464 [29][19]=58464 [29][20]=58464 [29][21]=58464 [29][22]=58464 [29][23]=58464 [29][24]=58464 [29][25]=58464 [29][26]=58464 [29][27]=58464 [29][28]=58464 [29][29]=58464 [29][30]=58464 [29][31]=58464 [29][32]=58464 [29][33]=58464 [29][34]=58464 [29][35]=58464 [29][36]=58464 [29][37]=58464 [29][38]=58464 [29][39]=58464 [29][40]=58464 [29][41]=58464 [29][42]=58464 [29][43]=58464 [29][44]=58464 [29][45]=58464 [29][46]=58464 [29][47]=58464 [29][48]=58464 [29][49]=58464 [29][50]=58464 [29][51]=58464 [29][52]=58464 [29][53]=58464 [29][54]=58464 [29][55]=58464 [29][56]=58464 [29][57]=58464 [29][58]=58464 [29][59]=58464 [29][60]=58464 [29][61]=58464 [29][62]=58464 [29][63]=58464 \n",
            "[30][0]=60480 [30][1]=60480 [30][2]=60480 [30][3]=60480 [30][4]=60480 [30][5]=60480 [30][6]=60480 [30][7]=60480 [30][8]=60480 [30][9]=60480 [30][10]=60480 [30][11]=60480 [30][12]=60480 [30][13]=60480 [30][14]=60480 [30][15]=60480 [30][16]=60480 [30][17]=60480 [30][18]=60480 [30][19]=60480 [30][20]=60480 [30][21]=60480 [30][22]=60480 [30][23]=60480 [30][24]=60480 [30][25]=60480 [30][26]=60480 [30][27]=60480 [30][28]=60480 [30][29]=60480 [30][30]=60480 [30][31]=60480 [30][32]=60480 [30][33]=60480 [30][34]=60480 [30][35]=60480 [30][36]=60480 [30][37]=60480 [30][38]=60480 [30][39]=60480 [30][40]=60480 [30][41]=60480 [30][42]=60480 [30][43]=60480 [30][44]=60480 [30][45]=60480 [30][46]=60480 [30][47]=60480 [30][48]=60480 [30][49]=60480 [30][50]=60480 [30][51]=60480 [30][52]=60480 [30][53]=60480 [30][54]=60480 [30][55]=60480 [30][56]=60480 [30][57]=60480 [30][58]=60480 [30][59]=60480 [30][60]=60480 [30][61]=60480 [30][62]=60480 [30][63]=60480 \n",
            "[31][0]=62496 [31][1]=62496 [31][2]=62496 [31][3]=62496 [31][4]=62496 [31][5]=62496 [31][6]=62496 [31][7]=62496 [31][8]=62496 [31][9]=62496 [31][10]=62496 [31][11]=62496 [31][12]=62496 [31][13]=62496 [31][14]=62496 [31][15]=62496 [31][16]=62496 [31][17]=62496 [31][18]=62496 [31][19]=62496 [31][20]=62496 [31][21]=62496 [31][22]=62496 [31][23]=62496 [31][24]=62496 [31][25]=62496 [31][26]=62496 [31][27]=62496 [31][28]=62496 [31][29]=62496 [31][30]=62496 [31][31]=62496 [31][32]=62496 [31][33]=62496 [31][34]=62496 [31][35]=62496 [31][36]=62496 [31][37]=62496 [31][38]=62496 [31][39]=62496 [31][40]=62496 [31][41]=62496 [31][42]=62496 [31][43]=62496 [31][44]=62496 [31][45]=62496 [31][46]=62496 [31][47]=62496 [31][48]=62496 [31][49]=62496 [31][50]=62496 [31][51]=62496 [31][52]=62496 [31][53]=62496 [31][54]=62496 [31][55]=62496 [31][56]=62496 [31][57]=62496 [31][58]=62496 [31][59]=62496 [31][60]=62496 [31][61]=62496 [31][62]=62496 [31][63]=62496 \n",
            "[32][0]=64512 [32][1]=64512 [32][2]=64512 [32][3]=64512 [32][4]=64512 [32][5]=64512 [32][6]=64512 [32][7]=64512 [32][8]=64512 [32][9]=64512 [32][10]=64512 [32][11]=64512 [32][12]=64512 [32][13]=64512 [32][14]=64512 [32][15]=64512 [32][16]=64512 [32][17]=64512 [32][18]=64512 [32][19]=64512 [32][20]=64512 [32][21]=64512 [32][22]=64512 [32][23]=64512 [32][24]=64512 [32][25]=64512 [32][26]=64512 [32][27]=64512 [32][28]=64512 [32][29]=64512 [32][30]=64512 [32][31]=64512 [32][32]=64512 [32][33]=64512 [32][34]=64512 [32][35]=64512 [32][36]=64512 [32][37]=64512 [32][38]=64512 [32][39]=64512 [32][40]=64512 [32][41]=64512 [32][42]=64512 [32][43]=64512 [32][44]=64512 [32][45]=64512 [32][46]=64512 [32][47]=64512 [32][48]=64512 [32][49]=64512 [32][50]=64512 [32][51]=64512 [32][52]=64512 [32][53]=64512 [32][54]=64512 [32][55]=64512 [32][56]=64512 [32][57]=64512 [32][58]=64512 [32][59]=64512 [32][60]=64512 [32][61]=64512 [32][62]=64512 [32][63]=64512 \n",
            "[33][0]=66528 [33][1]=66528 [33][2]=66528 [33][3]=66528 [33][4]=66528 [33][5]=66528 [33][6]=66528 [33][7]=66528 [33][8]=66528 [33][9]=66528 [33][10]=66528 [33][11]=66528 [33][12]=66528 [33][13]=66528 [33][14]=66528 [33][15]=66528 [33][16]=66528 [33][17]=66528 [33][18]=66528 [33][19]=66528 [33][20]=66528 [33][21]=66528 [33][22]=66528 [33][23]=66528 [33][24]=66528 [33][25]=66528 [33][26]=66528 [33][27]=66528 [33][28]=66528 [33][29]=66528 [33][30]=66528 [33][31]=66528 [33][32]=66528 [33][33]=66528 [33][34]=66528 [33][35]=66528 [33][36]=66528 [33][37]=66528 [33][38]=66528 [33][39]=66528 [33][40]=66528 [33][41]=66528 [33][42]=66528 [33][43]=66528 [33][44]=66528 [33][45]=66528 [33][46]=66528 [33][47]=66528 [33][48]=66528 [33][49]=66528 [33][50]=66528 [33][51]=66528 [33][52]=66528 [33][53]=66528 [33][54]=66528 [33][55]=66528 [33][56]=66528 [33][57]=66528 [33][58]=66528 [33][59]=66528 [33][60]=66528 [33][61]=66528 [33][62]=66528 [33][63]=66528 \n",
            "[34][0]=68544 [34][1]=68544 [34][2]=68544 [34][3]=68544 [34][4]=68544 [34][5]=68544 [34][6]=68544 [34][7]=68544 [34][8]=68544 [34][9]=68544 [34][10]=68544 [34][11]=68544 [34][12]=68544 [34][13]=68544 [34][14]=68544 [34][15]=68544 [34][16]=68544 [34][17]=68544 [34][18]=68544 [34][19]=68544 [34][20]=68544 [34][21]=68544 [34][22]=68544 [34][23]=68544 [34][24]=68544 [34][25]=68544 [34][26]=68544 [34][27]=68544 [34][28]=68544 [34][29]=68544 [34][30]=68544 [34][31]=68544 [34][32]=68544 [34][33]=68544 [34][34]=68544 [34][35]=68544 [34][36]=68544 [34][37]=68544 [34][38]=68544 [34][39]=68544 [34][40]=68544 [34][41]=68544 [34][42]=68544 [34][43]=68544 [34][44]=68544 [34][45]=68544 [34][46]=68544 [34][47]=68544 [34][48]=68544 [34][49]=68544 [34][50]=68544 [34][51]=68544 [34][52]=68544 [34][53]=68544 [34][54]=68544 [34][55]=68544 [34][56]=68544 [34][57]=68544 [34][58]=68544 [34][59]=68544 [34][60]=68544 [34][61]=68544 [34][62]=68544 [34][63]=68544 \n",
            "[35][0]=70560 [35][1]=70560 [35][2]=70560 [35][3]=70560 [35][4]=70560 [35][5]=70560 [35][6]=70560 [35][7]=70560 [35][8]=70560 [35][9]=70560 [35][10]=70560 [35][11]=70560 [35][12]=70560 [35][13]=70560 [35][14]=70560 [35][15]=70560 [35][16]=70560 [35][17]=70560 [35][18]=70560 [35][19]=70560 [35][20]=70560 [35][21]=70560 [35][22]=70560 [35][23]=70560 [35][24]=70560 [35][25]=70560 [35][26]=70560 [35][27]=70560 [35][28]=70560 [35][29]=70560 [35][30]=70560 [35][31]=70560 [35][32]=70560 [35][33]=70560 [35][34]=70560 [35][35]=70560 [35][36]=70560 [35][37]=70560 [35][38]=70560 [35][39]=70560 [35][40]=70560 [35][41]=70560 [35][42]=70560 [35][43]=70560 [35][44]=70560 [35][45]=70560 [35][46]=70560 [35][47]=70560 [35][48]=70560 [35][49]=70560 [35][50]=70560 [35][51]=70560 [35][52]=70560 [35][53]=70560 [35][54]=70560 [35][55]=70560 [35][56]=70560 [35][57]=70560 [35][58]=70560 [35][59]=70560 [35][60]=70560 [35][61]=70560 [35][62]=70560 [35][63]=70560 \n",
            "[36][0]=72576 [36][1]=72576 [36][2]=72576 [36][3]=72576 [36][4]=72576 [36][5]=72576 [36][6]=72576 [36][7]=72576 [36][8]=72576 [36][9]=72576 [36][10]=72576 [36][11]=72576 [36][12]=72576 [36][13]=72576 [36][14]=72576 [36][15]=72576 [36][16]=72576 [36][17]=72576 [36][18]=72576 [36][19]=72576 [36][20]=72576 [36][21]=72576 [36][22]=72576 [36][23]=72576 [36][24]=72576 [36][25]=72576 [36][26]=72576 [36][27]=72576 [36][28]=72576 [36][29]=72576 [36][30]=72576 [36][31]=72576 [36][32]=72576 [36][33]=72576 [36][34]=72576 [36][35]=72576 [36][36]=72576 [36][37]=72576 [36][38]=72576 [36][39]=72576 [36][40]=72576 [36][41]=72576 [36][42]=72576 [36][43]=72576 [36][44]=72576 [36][45]=72576 [36][46]=72576 [36][47]=72576 [36][48]=72576 [36][49]=72576 [36][50]=72576 [36][51]=72576 [36][52]=72576 [36][53]=72576 [36][54]=72576 [36][55]=72576 [36][56]=72576 [36][57]=72576 [36][58]=72576 [36][59]=72576 [36][60]=72576 [36][61]=72576 [36][62]=72576 [36][63]=72576 \n",
            "[37][0]=74592 [37][1]=74592 [37][2]=74592 [37][3]=74592 [37][4]=74592 [37][5]=74592 [37][6]=74592 [37][7]=74592 [37][8]=74592 [37][9]=74592 [37][10]=74592 [37][11]=74592 [37][12]=74592 [37][13]=74592 [37][14]=74592 [37][15]=74592 [37][16]=74592 [37][17]=74592 [37][18]=74592 [37][19]=74592 [37][20]=74592 [37][21]=74592 [37][22]=74592 [37][23]=74592 [37][24]=74592 [37][25]=74592 [37][26]=74592 [37][27]=74592 [37][28]=74592 [37][29]=74592 [37][30]=74592 [37][31]=74592 [37][32]=74592 [37][33]=74592 [37][34]=74592 [37][35]=74592 [37][36]=74592 [37][37]=74592 [37][38]=74592 [37][39]=74592 [37][40]=74592 [37][41]=74592 [37][42]=74592 [37][43]=74592 [37][44]=74592 [37][45]=74592 [37][46]=74592 [37][47]=74592 [37][48]=74592 [37][49]=74592 [37][50]=74592 [37][51]=74592 [37][52]=74592 [37][53]=74592 [37][54]=74592 [37][55]=74592 [37][56]=74592 [37][57]=74592 [37][58]=74592 [37][59]=74592 [37][60]=74592 [37][61]=74592 [37][62]=74592 [37][63]=74592 \n",
            "[38][0]=76608 [38][1]=76608 [38][2]=76608 [38][3]=76608 [38][4]=76608 [38][5]=76608 [38][6]=76608 [38][7]=76608 [38][8]=76608 [38][9]=76608 [38][10]=76608 [38][11]=76608 [38][12]=76608 [38][13]=76608 [38][14]=76608 [38][15]=76608 [38][16]=76608 [38][17]=76608 [38][18]=76608 [38][19]=76608 [38][20]=76608 [38][21]=76608 [38][22]=76608 [38][23]=76608 [38][24]=76608 [38][25]=76608 [38][26]=76608 [38][27]=76608 [38][28]=76608 [38][29]=76608 [38][30]=76608 [38][31]=76608 [38][32]=76608 [38][33]=76608 [38][34]=76608 [38][35]=76608 [38][36]=76608 [38][37]=76608 [38][38]=76608 [38][39]=76608 [38][40]=76608 [38][41]=76608 [38][42]=76608 [38][43]=76608 [38][44]=76608 [38][45]=76608 [38][46]=76608 [38][47]=76608 [38][48]=76608 [38][49]=76608 [38][50]=76608 [38][51]=76608 [38][52]=76608 [38][53]=76608 [38][54]=76608 [38][55]=76608 [38][56]=76608 [38][57]=76608 [38][58]=76608 [38][59]=76608 [38][60]=76608 [38][61]=76608 [38][62]=76608 [38][63]=76608 \n",
            "[39][0]=78624 [39][1]=78624 [39][2]=78624 [39][3]=78624 [39][4]=78624 [39][5]=78624 [39][6]=78624 [39][7]=78624 [39][8]=78624 [39][9]=78624 [39][10]=78624 [39][11]=78624 [39][12]=78624 [39][13]=78624 [39][14]=78624 [39][15]=78624 [39][16]=78624 [39][17]=78624 [39][18]=78624 [39][19]=78624 [39][20]=78624 [39][21]=78624 [39][22]=78624 [39][23]=78624 [39][24]=78624 [39][25]=78624 [39][26]=78624 [39][27]=78624 [39][28]=78624 [39][29]=78624 [39][30]=78624 [39][31]=78624 [39][32]=78624 [39][33]=78624 [39][34]=78624 [39][35]=78624 [39][36]=78624 [39][37]=78624 [39][38]=78624 [39][39]=78624 [39][40]=78624 [39][41]=78624 [39][42]=78624 [39][43]=78624 [39][44]=78624 [39][45]=78624 [39][46]=78624 [39][47]=78624 [39][48]=78624 [39][49]=78624 [39][50]=78624 [39][51]=78624 [39][52]=78624 [39][53]=78624 [39][54]=78624 [39][55]=78624 [39][56]=78624 [39][57]=78624 [39][58]=78624 [39][59]=78624 [39][60]=78624 [39][61]=78624 [39][62]=78624 [39][63]=78624 \n",
            "[40][0]=80640 [40][1]=80640 [40][2]=80640 [40][3]=80640 [40][4]=80640 [40][5]=80640 [40][6]=80640 [40][7]=80640 [40][8]=80640 [40][9]=80640 [40][10]=80640 [40][11]=80640 [40][12]=80640 [40][13]=80640 [40][14]=80640 [40][15]=80640 [40][16]=80640 [40][17]=80640 [40][18]=80640 [40][19]=80640 [40][20]=80640 [40][21]=80640 [40][22]=80640 [40][23]=80640 [40][24]=80640 [40][25]=80640 [40][26]=80640 [40][27]=80640 [40][28]=80640 [40][29]=80640 [40][30]=80640 [40][31]=80640 [40][32]=80640 [40][33]=80640 [40][34]=80640 [40][35]=80640 [40][36]=80640 [40][37]=80640 [40][38]=80640 [40][39]=80640 [40][40]=80640 [40][41]=80640 [40][42]=80640 [40][43]=80640 [40][44]=80640 [40][45]=80640 [40][46]=80640 [40][47]=80640 [40][48]=80640 [40][49]=80640 [40][50]=80640 [40][51]=80640 [40][52]=80640 [40][53]=80640 [40][54]=80640 [40][55]=80640 [40][56]=80640 [40][57]=80640 [40][58]=80640 [40][59]=80640 [40][60]=80640 [40][61]=80640 [40][62]=80640 [40][63]=80640 \n",
            "[41][0]=82656 [41][1]=82656 [41][2]=82656 [41][3]=82656 [41][4]=82656 [41][5]=82656 [41][6]=82656 [41][7]=82656 [41][8]=82656 [41][9]=82656 [41][10]=82656 [41][11]=82656 [41][12]=82656 [41][13]=82656 [41][14]=82656 [41][15]=82656 [41][16]=82656 [41][17]=82656 [41][18]=82656 [41][19]=82656 [41][20]=82656 [41][21]=82656 [41][22]=82656 [41][23]=82656 [41][24]=82656 [41][25]=82656 [41][26]=82656 [41][27]=82656 [41][28]=82656 [41][29]=82656 [41][30]=82656 [41][31]=82656 [41][32]=82656 [41][33]=82656 [41][34]=82656 [41][35]=82656 [41][36]=82656 [41][37]=82656 [41][38]=82656 [41][39]=82656 [41][40]=82656 [41][41]=82656 [41][42]=82656 [41][43]=82656 [41][44]=82656 [41][45]=82656 [41][46]=82656 [41][47]=82656 [41][48]=82656 [41][49]=82656 [41][50]=82656 [41][51]=82656 [41][52]=82656 [41][53]=82656 [41][54]=82656 [41][55]=82656 [41][56]=82656 [41][57]=82656 [41][58]=82656 [41][59]=82656 [41][60]=82656 [41][61]=82656 [41][62]=82656 [41][63]=82656 \n",
            "[42][0]=84672 [42][1]=84672 [42][2]=84672 [42][3]=84672 [42][4]=84672 [42][5]=84672 [42][6]=84672 [42][7]=84672 [42][8]=84672 [42][9]=84672 [42][10]=84672 [42][11]=84672 [42][12]=84672 [42][13]=84672 [42][14]=84672 [42][15]=84672 [42][16]=84672 [42][17]=84672 [42][18]=84672 [42][19]=84672 [42][20]=84672 [42][21]=84672 [42][22]=84672 [42][23]=84672 [42][24]=84672 [42][25]=84672 [42][26]=84672 [42][27]=84672 [42][28]=84672 [42][29]=84672 [42][30]=84672 [42][31]=84672 [42][32]=84672 [42][33]=84672 [42][34]=84672 [42][35]=84672 [42][36]=84672 [42][37]=84672 [42][38]=84672 [42][39]=84672 [42][40]=84672 [42][41]=84672 [42][42]=84672 [42][43]=84672 [42][44]=84672 [42][45]=84672 [42][46]=84672 [42][47]=84672 [42][48]=84672 [42][49]=84672 [42][50]=84672 [42][51]=84672 [42][52]=84672 [42][53]=84672 [42][54]=84672 [42][55]=84672 [42][56]=84672 [42][57]=84672 [42][58]=84672 [42][59]=84672 [42][60]=84672 [42][61]=84672 [42][62]=84672 [42][63]=84672 \n",
            "[43][0]=86688 [43][1]=86688 [43][2]=86688 [43][3]=86688 [43][4]=86688 [43][5]=86688 [43][6]=86688 [43][7]=86688 [43][8]=86688 [43][9]=86688 [43][10]=86688 [43][11]=86688 [43][12]=86688 [43][13]=86688 [43][14]=86688 [43][15]=86688 [43][16]=86688 [43][17]=86688 [43][18]=86688 [43][19]=86688 [43][20]=86688 [43][21]=86688 [43][22]=86688 [43][23]=86688 [43][24]=86688 [43][25]=86688 [43][26]=86688 [43][27]=86688 [43][28]=86688 [43][29]=86688 [43][30]=86688 [43][31]=86688 [43][32]=86688 [43][33]=86688 [43][34]=86688 [43][35]=86688 [43][36]=86688 [43][37]=86688 [43][38]=86688 [43][39]=86688 [43][40]=86688 [43][41]=86688 [43][42]=86688 [43][43]=86688 [43][44]=86688 [43][45]=86688 [43][46]=86688 [43][47]=86688 [43][48]=86688 [43][49]=86688 [43][50]=86688 [43][51]=86688 [43][52]=86688 [43][53]=86688 [43][54]=86688 [43][55]=86688 [43][56]=86688 [43][57]=86688 [43][58]=86688 [43][59]=86688 [43][60]=86688 [43][61]=86688 [43][62]=86688 [43][63]=86688 \n",
            "[44][0]=88704 [44][1]=88704 [44][2]=88704 [44][3]=88704 [44][4]=88704 [44][5]=88704 [44][6]=88704 [44][7]=88704 [44][8]=88704 [44][9]=88704 [44][10]=88704 [44][11]=88704 [44][12]=88704 [44][13]=88704 [44][14]=88704 [44][15]=88704 [44][16]=88704 [44][17]=88704 [44][18]=88704 [44][19]=88704 [44][20]=88704 [44][21]=88704 [44][22]=88704 [44][23]=88704 [44][24]=88704 [44][25]=88704 [44][26]=88704 [44][27]=88704 [44][28]=88704 [44][29]=88704 [44][30]=88704 [44][31]=88704 [44][32]=88704 [44][33]=88704 [44][34]=88704 [44][35]=88704 [44][36]=88704 [44][37]=88704 [44][38]=88704 [44][39]=88704 [44][40]=88704 [44][41]=88704 [44][42]=88704 [44][43]=88704 [44][44]=88704 [44][45]=88704 [44][46]=88704 [44][47]=88704 [44][48]=88704 [44][49]=88704 [44][50]=88704 [44][51]=88704 [44][52]=88704 [44][53]=88704 [44][54]=88704 [44][55]=88704 [44][56]=88704 [44][57]=88704 [44][58]=88704 [44][59]=88704 [44][60]=88704 [44][61]=88704 [44][62]=88704 [44][63]=88704 \n",
            "[45][0]=90720 [45][1]=90720 [45][2]=90720 [45][3]=90720 [45][4]=90720 [45][5]=90720 [45][6]=90720 [45][7]=90720 [45][8]=90720 [45][9]=90720 [45][10]=90720 [45][11]=90720 [45][12]=90720 [45][13]=90720 [45][14]=90720 [45][15]=90720 [45][16]=90720 [45][17]=90720 [45][18]=90720 [45][19]=90720 [45][20]=90720 [45][21]=90720 [45][22]=90720 [45][23]=90720 [45][24]=90720 [45][25]=90720 [45][26]=90720 [45][27]=90720 [45][28]=90720 [45][29]=90720 [45][30]=90720 [45][31]=90720 [45][32]=90720 [45][33]=90720 [45][34]=90720 [45][35]=90720 [45][36]=90720 [45][37]=90720 [45][38]=90720 [45][39]=90720 [45][40]=90720 [45][41]=90720 [45][42]=90720 [45][43]=90720 [45][44]=90720 [45][45]=90720 [45][46]=90720 [45][47]=90720 [45][48]=90720 [45][49]=90720 [45][50]=90720 [45][51]=90720 [45][52]=90720 [45][53]=90720 [45][54]=90720 [45][55]=90720 [45][56]=90720 [45][57]=90720 [45][58]=90720 [45][59]=90720 [45][60]=90720 [45][61]=90720 [45][62]=90720 [45][63]=90720 \n",
            "[46][0]=92736 [46][1]=92736 [46][2]=92736 [46][3]=92736 [46][4]=92736 [46][5]=92736 [46][6]=92736 [46][7]=92736 [46][8]=92736 [46][9]=92736 [46][10]=92736 [46][11]=92736 [46][12]=92736 [46][13]=92736 [46][14]=92736 [46][15]=92736 [46][16]=92736 [46][17]=92736 [46][18]=92736 [46][19]=92736 [46][20]=92736 [46][21]=92736 [46][22]=92736 [46][23]=92736 [46][24]=92736 [46][25]=92736 [46][26]=92736 [46][27]=92736 [46][28]=92736 [46][29]=92736 [46][30]=92736 [46][31]=92736 [46][32]=92736 [46][33]=92736 [46][34]=92736 [46][35]=92736 [46][36]=92736 [46][37]=92736 [46][38]=92736 [46][39]=92736 [46][40]=92736 [46][41]=92736 [46][42]=92736 [46][43]=92736 [46][44]=92736 [46][45]=92736 [46][46]=92736 [46][47]=92736 [46][48]=92736 [46][49]=92736 [46][50]=92736 [46][51]=92736 [46][52]=92736 [46][53]=92736 [46][54]=92736 [46][55]=92736 [46][56]=92736 [46][57]=92736 [46][58]=92736 [46][59]=92736 [46][60]=92736 [46][61]=92736 [46][62]=92736 [46][63]=92736 \n",
            "[47][0]=94752 [47][1]=94752 [47][2]=94752 [47][3]=94752 [47][4]=94752 [47][5]=94752 [47][6]=94752 [47][7]=94752 [47][8]=94752 [47][9]=94752 [47][10]=94752 [47][11]=94752 [47][12]=94752 [47][13]=94752 [47][14]=94752 [47][15]=94752 [47][16]=94752 [47][17]=94752 [47][18]=94752 [47][19]=94752 [47][20]=94752 [47][21]=94752 [47][22]=94752 [47][23]=94752 [47][24]=94752 [47][25]=94752 [47][26]=94752 [47][27]=94752 [47][28]=94752 [47][29]=94752 [47][30]=94752 [47][31]=94752 [47][32]=94752 [47][33]=94752 [47][34]=94752 [47][35]=94752 [47][36]=94752 [47][37]=94752 [47][38]=94752 [47][39]=94752 [47][40]=94752 [47][41]=94752 [47][42]=94752 [47][43]=94752 [47][44]=94752 [47][45]=94752 [47][46]=94752 [47][47]=94752 [47][48]=94752 [47][49]=94752 [47][50]=94752 [47][51]=94752 [47][52]=94752 [47][53]=94752 [47][54]=94752 [47][55]=94752 [47][56]=94752 [47][57]=94752 [47][58]=94752 [47][59]=94752 [47][60]=94752 [47][61]=94752 [47][62]=94752 [47][63]=94752 \n",
            "[48][0]=96768 [48][1]=96768 [48][2]=96768 [48][3]=96768 [48][4]=96768 [48][5]=96768 [48][6]=96768 [48][7]=96768 [48][8]=96768 [48][9]=96768 [48][10]=96768 [48][11]=96768 [48][12]=96768 [48][13]=96768 [48][14]=96768 [48][15]=96768 [48][16]=96768 [48][17]=96768 [48][18]=96768 [48][19]=96768 [48][20]=96768 [48][21]=96768 [48][22]=96768 [48][23]=96768 [48][24]=96768 [48][25]=96768 [48][26]=96768 [48][27]=96768 [48][28]=96768 [48][29]=96768 [48][30]=96768 [48][31]=96768 [48][32]=96768 [48][33]=96768 [48][34]=96768 [48][35]=96768 [48][36]=96768 [48][37]=96768 [48][38]=96768 [48][39]=96768 [48][40]=96768 [48][41]=96768 [48][42]=96768 [48][43]=96768 [48][44]=96768 [48][45]=96768 [48][46]=96768 [48][47]=96768 [48][48]=96768 [48][49]=96768 [48][50]=96768 [48][51]=96768 [48][52]=96768 [48][53]=96768 [48][54]=96768 [48][55]=96768 [48][56]=96768 [48][57]=96768 [48][58]=96768 [48][59]=96768 [48][60]=96768 [48][61]=96768 [48][62]=96768 [48][63]=96768 \n",
            "[49][0]=98784 [49][1]=98784 [49][2]=98784 [49][3]=98784 [49][4]=98784 [49][5]=98784 [49][6]=98784 [49][7]=98784 [49][8]=98784 [49][9]=98784 [49][10]=98784 [49][11]=98784 [49][12]=98784 [49][13]=98784 [49][14]=98784 [49][15]=98784 [49][16]=98784 [49][17]=98784 [49][18]=98784 [49][19]=98784 [49][20]=98784 [49][21]=98784 [49][22]=98784 [49][23]=98784 [49][24]=98784 [49][25]=98784 [49][26]=98784 [49][27]=98784 [49][28]=98784 [49][29]=98784 [49][30]=98784 [49][31]=98784 [49][32]=98784 [49][33]=98784 [49][34]=98784 [49][35]=98784 [49][36]=98784 [49][37]=98784 [49][38]=98784 [49][39]=98784 [49][40]=98784 [49][41]=98784 [49][42]=98784 [49][43]=98784 [49][44]=98784 [49][45]=98784 [49][46]=98784 [49][47]=98784 [49][48]=98784 [49][49]=98784 [49][50]=98784 [49][51]=98784 [49][52]=98784 [49][53]=98784 [49][54]=98784 [49][55]=98784 [49][56]=98784 [49][57]=98784 [49][58]=98784 [49][59]=98784 [49][60]=98784 [49][61]=98784 [49][62]=98784 [49][63]=98784 \n",
            "[50][0]=100800 [50][1]=100800 [50][2]=100800 [50][3]=100800 [50][4]=100800 [50][5]=100800 [50][6]=100800 [50][7]=100800 [50][8]=100800 [50][9]=100800 [50][10]=100800 [50][11]=100800 [50][12]=100800 [50][13]=100800 [50][14]=100800 [50][15]=100800 [50][16]=100800 [50][17]=100800 [50][18]=100800 [50][19]=100800 [50][20]=100800 [50][21]=100800 [50][22]=100800 [50][23]=100800 [50][24]=100800 [50][25]=100800 [50][26]=100800 [50][27]=100800 [50][28]=100800 [50][29]=100800 [50][30]=100800 [50][31]=100800 [50][32]=100800 [50][33]=100800 [50][34]=100800 [50][35]=100800 [50][36]=100800 [50][37]=100800 [50][38]=100800 [50][39]=100800 [50][40]=100800 [50][41]=100800 [50][42]=100800 [50][43]=100800 [50][44]=100800 [50][45]=100800 [50][46]=100800 [50][47]=100800 [50][48]=100800 [50][49]=100800 [50][50]=100800 [50][51]=100800 [50][52]=100800 [50][53]=100800 [50][54]=100800 [50][55]=100800 [50][56]=100800 [50][57]=100800 [50][58]=100800 [50][59]=100800 [50][60]=100800 [50][61]=100800 [50][62]=100800 [50][63]=100800 \n",
            "[51][0]=102816 [51][1]=102816 [51][2]=102816 [51][3]=102816 [51][4]=102816 [51][5]=102816 [51][6]=102816 [51][7]=102816 [51][8]=102816 [51][9]=102816 [51][10]=102816 [51][11]=102816 [51][12]=102816 [51][13]=102816 [51][14]=102816 [51][15]=102816 [51][16]=102816 [51][17]=102816 [51][18]=102816 [51][19]=102816 [51][20]=102816 [51][21]=102816 [51][22]=102816 [51][23]=102816 [51][24]=102816 [51][25]=102816 [51][26]=102816 [51][27]=102816 [51][28]=102816 [51][29]=102816 [51][30]=102816 [51][31]=102816 [51][32]=102816 [51][33]=102816 [51][34]=102816 [51][35]=102816 [51][36]=102816 [51][37]=102816 [51][38]=102816 [51][39]=102816 [51][40]=102816 [51][41]=102816 [51][42]=102816 [51][43]=102816 [51][44]=102816 [51][45]=102816 [51][46]=102816 [51][47]=102816 [51][48]=102816 [51][49]=102816 [51][50]=102816 [51][51]=102816 [51][52]=102816 [51][53]=102816 [51][54]=102816 [51][55]=102816 [51][56]=102816 [51][57]=102816 [51][58]=102816 [51][59]=102816 [51][60]=102816 [51][61]=102816 [51][62]=102816 [51][63]=102816 \n",
            "[52][0]=104832 [52][1]=104832 [52][2]=104832 [52][3]=104832 [52][4]=104832 [52][5]=104832 [52][6]=104832 [52][7]=104832 [52][8]=104832 [52][9]=104832 [52][10]=104832 [52][11]=104832 [52][12]=104832 [52][13]=104832 [52][14]=104832 [52][15]=104832 [52][16]=104832 [52][17]=104832 [52][18]=104832 [52][19]=104832 [52][20]=104832 [52][21]=104832 [52][22]=104832 [52][23]=104832 [52][24]=104832 [52][25]=104832 [52][26]=104832 [52][27]=104832 [52][28]=104832 [52][29]=104832 [52][30]=104832 [52][31]=104832 [52][32]=104832 [52][33]=104832 [52][34]=104832 [52][35]=104832 [52][36]=104832 [52][37]=104832 [52][38]=104832 [52][39]=104832 [52][40]=104832 [52][41]=104832 [52][42]=104832 [52][43]=104832 [52][44]=104832 [52][45]=104832 [52][46]=104832 [52][47]=104832 [52][48]=104832 [52][49]=104832 [52][50]=104832 [52][51]=104832 [52][52]=104832 [52][53]=104832 [52][54]=104832 [52][55]=104832 [52][56]=104832 [52][57]=104832 [52][58]=104832 [52][59]=104832 [52][60]=104832 [52][61]=104832 [52][62]=104832 [52][63]=104832 \n",
            "[53][0]=106848 [53][1]=106848 [53][2]=106848 [53][3]=106848 [53][4]=106848 [53][5]=106848 [53][6]=106848 [53][7]=106848 [53][8]=106848 [53][9]=106848 [53][10]=106848 [53][11]=106848 [53][12]=106848 [53][13]=106848 [53][14]=106848 [53][15]=106848 [53][16]=106848 [53][17]=106848 [53][18]=106848 [53][19]=106848 [53][20]=106848 [53][21]=106848 [53][22]=106848 [53][23]=106848 [53][24]=106848 [53][25]=106848 [53][26]=106848 [53][27]=106848 [53][28]=106848 [53][29]=106848 [53][30]=106848 [53][31]=106848 [53][32]=106848 [53][33]=106848 [53][34]=106848 [53][35]=106848 [53][36]=106848 [53][37]=106848 [53][38]=106848 [53][39]=106848 [53][40]=106848 [53][41]=106848 [53][42]=106848 [53][43]=106848 [53][44]=106848 [53][45]=106848 [53][46]=106848 [53][47]=106848 [53][48]=106848 [53][49]=106848 [53][50]=106848 [53][51]=106848 [53][52]=106848 [53][53]=106848 [53][54]=106848 [53][55]=106848 [53][56]=106848 [53][57]=106848 [53][58]=106848 [53][59]=106848 [53][60]=106848 [53][61]=106848 [53][62]=106848 [53][63]=106848 \n",
            "[54][0]=108864 [54][1]=108864 [54][2]=108864 [54][3]=108864 [54][4]=108864 [54][5]=108864 [54][6]=108864 [54][7]=108864 [54][8]=108864 [54][9]=108864 [54][10]=108864 [54][11]=108864 [54][12]=108864 [54][13]=108864 [54][14]=108864 [54][15]=108864 [54][16]=108864 [54][17]=108864 [54][18]=108864 [54][19]=108864 [54][20]=108864 [54][21]=108864 [54][22]=108864 [54][23]=108864 [54][24]=108864 [54][25]=108864 [54][26]=108864 [54][27]=108864 [54][28]=108864 [54][29]=108864 [54][30]=108864 [54][31]=108864 [54][32]=108864 [54][33]=108864 [54][34]=108864 [54][35]=108864 [54][36]=108864 [54][37]=108864 [54][38]=108864 [54][39]=108864 [54][40]=108864 [54][41]=108864 [54][42]=108864 [54][43]=108864 [54][44]=108864 [54][45]=108864 [54][46]=108864 [54][47]=108864 [54][48]=108864 [54][49]=108864 [54][50]=108864 [54][51]=108864 [54][52]=108864 [54][53]=108864 [54][54]=108864 [54][55]=108864 [54][56]=108864 [54][57]=108864 [54][58]=108864 [54][59]=108864 [54][60]=108864 [54][61]=108864 [54][62]=108864 [54][63]=108864 \n",
            "[55][0]=110880 [55][1]=110880 [55][2]=110880 [55][3]=110880 [55][4]=110880 [55][5]=110880 [55][6]=110880 [55][7]=110880 [55][8]=110880 [55][9]=110880 [55][10]=110880 [55][11]=110880 [55][12]=110880 [55][13]=110880 [55][14]=110880 [55][15]=110880 [55][16]=110880 [55][17]=110880 [55][18]=110880 [55][19]=110880 [55][20]=110880 [55][21]=110880 [55][22]=110880 [55][23]=110880 [55][24]=110880 [55][25]=110880 [55][26]=110880 [55][27]=110880 [55][28]=110880 [55][29]=110880 [55][30]=110880 [55][31]=110880 [55][32]=110880 [55][33]=110880 [55][34]=110880 [55][35]=110880 [55][36]=110880 [55][37]=110880 [55][38]=110880 [55][39]=110880 [55][40]=110880 [55][41]=110880 [55][42]=110880 [55][43]=110880 [55][44]=110880 [55][45]=110880 [55][46]=110880 [55][47]=110880 [55][48]=110880 [55][49]=110880 [55][50]=110880 [55][51]=110880 [55][52]=110880 [55][53]=110880 [55][54]=110880 [55][55]=110880 [55][56]=110880 [55][57]=110880 [55][58]=110880 [55][59]=110880 [55][60]=110880 [55][61]=110880 [55][62]=110880 [55][63]=110880 \n",
            "[56][0]=112896 [56][1]=112896 [56][2]=112896 [56][3]=112896 [56][4]=112896 [56][5]=112896 [56][6]=112896 [56][7]=112896 [56][8]=112896 [56][9]=112896 [56][10]=112896 [56][11]=112896 [56][12]=112896 [56][13]=112896 [56][14]=112896 [56][15]=112896 [56][16]=112896 [56][17]=112896 [56][18]=112896 [56][19]=112896 [56][20]=112896 [56][21]=112896 [56][22]=112896 [56][23]=112896 [56][24]=112896 [56][25]=112896 [56][26]=112896 [56][27]=112896 [56][28]=112896 [56][29]=112896 [56][30]=112896 [56][31]=112896 [56][32]=112896 [56][33]=112896 [56][34]=112896 [56][35]=112896 [56][36]=112896 [56][37]=112896 [56][38]=112896 [56][39]=112896 [56][40]=112896 [56][41]=112896 [56][42]=112896 [56][43]=112896 [56][44]=112896 [56][45]=112896 [56][46]=112896 [56][47]=112896 [56][48]=112896 [56][49]=112896 [56][50]=112896 [56][51]=112896 [56][52]=112896 [56][53]=112896 [56][54]=112896 [56][55]=112896 [56][56]=112896 [56][57]=112896 [56][58]=112896 [56][59]=112896 [56][60]=112896 [56][61]=112896 [56][62]=112896 [56][63]=112896 \n",
            "[57][0]=114912 [57][1]=114912 [57][2]=114912 [57][3]=114912 [57][4]=114912 [57][5]=114912 [57][6]=114912 [57][7]=114912 [57][8]=114912 [57][9]=114912 [57][10]=114912 [57][11]=114912 [57][12]=114912 [57][13]=114912 [57][14]=114912 [57][15]=114912 [57][16]=114912 [57][17]=114912 [57][18]=114912 [57][19]=114912 [57][20]=114912 [57][21]=114912 [57][22]=114912 [57][23]=114912 [57][24]=114912 [57][25]=114912 [57][26]=114912 [57][27]=114912 [57][28]=114912 [57][29]=114912 [57][30]=114912 [57][31]=114912 [57][32]=114912 [57][33]=114912 [57][34]=114912 [57][35]=114912 [57][36]=114912 [57][37]=114912 [57][38]=114912 [57][39]=114912 [57][40]=114912 [57][41]=114912 [57][42]=114912 [57][43]=114912 [57][44]=114912 [57][45]=114912 [57][46]=114912 [57][47]=114912 [57][48]=114912 [57][49]=114912 [57][50]=114912 [57][51]=114912 [57][52]=114912 [57][53]=114912 [57][54]=114912 [57][55]=114912 [57][56]=114912 [57][57]=114912 [57][58]=114912 [57][59]=114912 [57][60]=114912 [57][61]=114912 [57][62]=114912 [57][63]=114912 \n",
            "[58][0]=116928 [58][1]=116928 [58][2]=116928 [58][3]=116928 [58][4]=116928 [58][5]=116928 [58][6]=116928 [58][7]=116928 [58][8]=116928 [58][9]=116928 [58][10]=116928 [58][11]=116928 [58][12]=116928 [58][13]=116928 [58][14]=116928 [58][15]=116928 [58][16]=116928 [58][17]=116928 [58][18]=116928 [58][19]=116928 [58][20]=116928 [58][21]=116928 [58][22]=116928 [58][23]=116928 [58][24]=116928 [58][25]=116928 [58][26]=116928 [58][27]=116928 [58][28]=116928 [58][29]=116928 [58][30]=116928 [58][31]=116928 [58][32]=116928 [58][33]=116928 [58][34]=116928 [58][35]=116928 [58][36]=116928 [58][37]=116928 [58][38]=116928 [58][39]=116928 [58][40]=116928 [58][41]=116928 [58][42]=116928 [58][43]=116928 [58][44]=116928 [58][45]=116928 [58][46]=116928 [58][47]=116928 [58][48]=116928 [58][49]=116928 [58][50]=116928 [58][51]=116928 [58][52]=116928 [58][53]=116928 [58][54]=116928 [58][55]=116928 [58][56]=116928 [58][57]=116928 [58][58]=116928 [58][59]=116928 [58][60]=116928 [58][61]=116928 [58][62]=116928 [58][63]=116928 \n",
            "[59][0]=118944 [59][1]=118944 [59][2]=118944 [59][3]=118944 [59][4]=118944 [59][5]=118944 [59][6]=118944 [59][7]=118944 [59][8]=118944 [59][9]=118944 [59][10]=118944 [59][11]=118944 [59][12]=118944 [59][13]=118944 [59][14]=118944 [59][15]=118944 [59][16]=118944 [59][17]=118944 [59][18]=118944 [59][19]=118944 [59][20]=118944 [59][21]=118944 [59][22]=118944 [59][23]=118944 [59][24]=118944 [59][25]=118944 [59][26]=118944 [59][27]=118944 [59][28]=118944 [59][29]=118944 [59][30]=118944 [59][31]=118944 [59][32]=118944 [59][33]=118944 [59][34]=118944 [59][35]=118944 [59][36]=118944 [59][37]=118944 [59][38]=118944 [59][39]=118944 [59][40]=118944 [59][41]=118944 [59][42]=118944 [59][43]=118944 [59][44]=118944 [59][45]=118944 [59][46]=118944 [59][47]=118944 [59][48]=118944 [59][49]=118944 [59][50]=118944 [59][51]=118944 [59][52]=118944 [59][53]=118944 [59][54]=118944 [59][55]=118944 [59][56]=118944 [59][57]=118944 [59][58]=118944 [59][59]=118944 [59][60]=118944 [59][61]=118944 [59][62]=118944 [59][63]=118944 \n",
            "[60][0]=120960 [60][1]=120960 [60][2]=120960 [60][3]=120960 [60][4]=120960 [60][5]=120960 [60][6]=120960 [60][7]=120960 [60][8]=120960 [60][9]=120960 [60][10]=120960 [60][11]=120960 [60][12]=120960 [60][13]=120960 [60][14]=120960 [60][15]=120960 [60][16]=120960 [60][17]=120960 [60][18]=120960 [60][19]=120960 [60][20]=120960 [60][21]=120960 [60][22]=120960 [60][23]=120960 [60][24]=120960 [60][25]=120960 [60][26]=120960 [60][27]=120960 [60][28]=120960 [60][29]=120960 [60][30]=120960 [60][31]=120960 [60][32]=120960 [60][33]=120960 [60][34]=120960 [60][35]=120960 [60][36]=120960 [60][37]=120960 [60][38]=120960 [60][39]=120960 [60][40]=120960 [60][41]=120960 [60][42]=120960 [60][43]=120960 [60][44]=120960 [60][45]=120960 [60][46]=120960 [60][47]=120960 [60][48]=120960 [60][49]=120960 [60][50]=120960 [60][51]=120960 [60][52]=120960 [60][53]=120960 [60][54]=120960 [60][55]=120960 [60][56]=120960 [60][57]=120960 [60][58]=120960 [60][59]=120960 [60][60]=120960 [60][61]=120960 [60][62]=120960 [60][63]=120960 \n",
            "[61][0]=122976 [61][1]=122976 [61][2]=122976 [61][3]=122976 [61][4]=122976 [61][5]=122976 [61][6]=122976 [61][7]=122976 [61][8]=122976 [61][9]=122976 [61][10]=122976 [61][11]=122976 [61][12]=122976 [61][13]=122976 [61][14]=122976 [61][15]=122976 [61][16]=122976 [61][17]=122976 [61][18]=122976 [61][19]=122976 [61][20]=122976 [61][21]=122976 [61][22]=122976 [61][23]=122976 [61][24]=122976 [61][25]=122976 [61][26]=122976 [61][27]=122976 [61][28]=122976 [61][29]=122976 [61][30]=122976 [61][31]=122976 [61][32]=122976 [61][33]=122976 [61][34]=122976 [61][35]=122976 [61][36]=122976 [61][37]=122976 [61][38]=122976 [61][39]=122976 [61][40]=122976 [61][41]=122976 [61][42]=122976 [61][43]=122976 [61][44]=122976 [61][45]=122976 [61][46]=122976 [61][47]=122976 [61][48]=122976 [61][49]=122976 [61][50]=122976 [61][51]=122976 [61][52]=122976 [61][53]=122976 [61][54]=122976 [61][55]=122976 [61][56]=122976 [61][57]=122976 [61][58]=122976 [61][59]=122976 [61][60]=122976 [61][61]=122976 [61][62]=122976 [61][63]=122976 \n",
            "[62][0]=124992 [62][1]=124992 [62][2]=124992 [62][3]=124992 [62][4]=124992 [62][5]=124992 [62][6]=124992 [62][7]=124992 [62][8]=124992 [62][9]=124992 [62][10]=124992 [62][11]=124992 [62][12]=124992 [62][13]=124992 [62][14]=124992 [62][15]=124992 [62][16]=124992 [62][17]=124992 [62][18]=124992 [62][19]=124992 [62][20]=124992 [62][21]=124992 [62][22]=124992 [62][23]=124992 [62][24]=124992 [62][25]=124992 [62][26]=124992 [62][27]=124992 [62][28]=124992 [62][29]=124992 [62][30]=124992 [62][31]=124992 [62][32]=124992 [62][33]=124992 [62][34]=124992 [62][35]=124992 [62][36]=124992 [62][37]=124992 [62][38]=124992 [62][39]=124992 [62][40]=124992 [62][41]=124992 [62][42]=124992 [62][43]=124992 [62][44]=124992 [62][45]=124992 [62][46]=124992 [62][47]=124992 [62][48]=124992 [62][49]=124992 [62][50]=124992 [62][51]=124992 [62][52]=124992 [62][53]=124992 [62][54]=124992 [62][55]=124992 [62][56]=124992 [62][57]=124992 [62][58]=124992 [62][59]=124992 [62][60]=124992 [62][61]=124992 [62][62]=124992 [62][63]=124992 \n",
            "[63][0]=127008 [63][1]=127008 [63][2]=127008 [63][3]=127008 [63][4]=127008 [63][5]=127008 [63][6]=127008 [63][7]=127008 [63][8]=127008 [63][9]=127008 [63][10]=127008 [63][11]=127008 [63][12]=127008 [63][13]=127008 [63][14]=127008 [63][15]=127008 [63][16]=127008 [63][17]=127008 [63][18]=127008 [63][19]=127008 [63][20]=127008 [63][21]=127008 [63][22]=127008 [63][23]=127008 [63][24]=127008 [63][25]=127008 [63][26]=127008 [63][27]=127008 [63][28]=127008 [63][29]=127008 [63][30]=127008 [63][31]=127008 [63][32]=127008 [63][33]=127008 [63][34]=127008 [63][35]=127008 [63][36]=127008 [63][37]=127008 [63][38]=127008 [63][39]=127008 [63][40]=127008 [63][41]=127008 [63][42]=127008 [63][43]=127008 [63][44]=127008 [63][45]=127008 [63][46]=127008 [63][47]=127008 [63][48]=127008 [63][49]=127008 [63][50]=127008 [63][51]=127008 [63][52]=127008 [63][53]=127008 [63][54]=127008 [63][55]=127008 [63][56]=127008 [63][57]=127008 [63][58]=127008 [63][59]=127008 [63][60]=127008 [63][61]=127008 [63][62]=127008 [63][63]=127008 \n",
            "GigaFlops: 25.435622==1944== Profiling application: ./matrixMul_gpu3\n",
            "==1944== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.90%  15.879ms      1000  15.879us  15.584us  17.824us  matrixMultGPU(int*, int*, int*, int)\n",
            "                    0.07%  10.528us         2  5.2640us  5.1840us  5.3440us  [CUDA memcpy HtoD]\n",
            "                    0.03%  4.6720us         1  4.6720us  4.6720us  4.6720us  [CUDA memcpy DtoH]\n",
            "      API calls:   90.76%  207.41ms         2  103.70ms     977ns  207.40ms  cudaEventCreate\n",
            "                    4.52%  10.331ms      1000  10.330us  3.9120us  2.3549ms  cudaLaunchKernel\n",
            "                    4.19%  9.5685ms         1  9.5685ms  9.5685ms  9.5685ms  cudaEventSynchronize\n",
            "                    0.22%  502.98us         1  502.98us  502.98us  502.98us  cuDeviceTotalMem\n",
            "                    0.09%  207.34us       101  2.0520us     153ns  85.277us  cuDeviceGetAttribute\n",
            "                    0.08%  180.67us         3  60.222us  2.1490us  174.29us  cudaMalloc\n",
            "                    0.07%  161.79us         3  53.931us  3.1330us  148.45us  cudaFree\n",
            "                    0.05%  106.12us         3  35.373us  22.206us  42.533us  cudaMemcpy\n",
            "                    0.01%  24.281us         1  24.281us  24.281us  24.281us  cuDeviceGetName\n",
            "                    0.01%  17.282us         2  8.6410us  4.5110us  12.771us  cudaEventRecord\n",
            "                    0.00%  6.1750us         1  6.1750us  6.1750us  6.1750us  cuDeviceGetPCIBusId\n",
            "                    0.00%  3.1360us         1  3.1360us  3.1360us  3.1360us  cudaEventElapsedTime\n",
            "                    0.00%  2.0000us         3     666ns     190ns     977ns  cuDeviceGetCount\n",
            "                    0.00%  1.7840us         2     892ns     342ns  1.4420us  cuDeviceGet\n",
            "                    0.00%     327ns         1     327ns     327ns     327ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agd10FrQB2vQ"
      },
      "source": [
        "### Resultados de la ejecución de multiplicación de matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5Kk6Y4wB--U"
      },
      "source": [
        "Sin memoria compartida:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPZSCOh2B2H7",
        "outputId": "26a87f13-dd63-4638-af82-9101365ff847"
      },
      "source": [
        "!for i in 16 32 64 128 512; do sed -i \"/#define N/c\\#define N $i\" matrixMul_gpu2.cu && /usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true matrixMul_gpu2.cu -o matrixMul_gpu2 -lcudadevrt && nvprof ./matrixMul_gpu2; done"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==2364== NVPROF is profiling process 2364, command: ./matrixMul_gpu2\n",
            "==2364== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "GigaFlops: 0.765514==2364== Profiling application: ./matrixMul_gpu2\n",
            "==2364== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.87%  5.2725ms      1000  5.2720us  5.0880us  12.480us  matrixMultGPU(int*, int*, int*)\n",
            "                    0.08%  4.1280us         2  2.0640us  1.7280us  2.4000us  [CUDA memcpy HtoD]\n",
            "                    0.05%  2.5280us         1  2.5280us  2.5280us  2.5280us  [CUDA memcpy DtoH]\n",
            "      API calls:   94.66%  202.69ms         2  101.34ms  1.0730us  202.69ms  cudaEventCreate\n",
            "                    3.36%  7.1910ms      1000  7.1900us  4.1410us  411.39us  cudaLaunchKernel\n",
            "                    1.40%  3.0006ms         1  3.0006ms  3.0006ms  3.0006ms  cudaEventSynchronize\n",
            "                    0.23%  493.61us         1  493.61us  493.61us  493.61us  cuDeviceTotalMem\n",
            "                    0.12%  253.91us       101  2.5130us     141ns  114.58us  cuDeviceGetAttribute\n",
            "                    0.09%  182.68us         3  60.892us  2.0190us  176.79us  cudaMalloc\n",
            "                    0.07%  151.28us         3  50.426us  2.4060us  141.19us  cudaFree\n",
            "                    0.04%  88.090us         3  29.363us  20.289us  39.432us  cudaMemcpy\n",
            "                    0.02%  33.715us         1  33.715us  33.715us  33.715us  cuDeviceGetName\n",
            "                    0.00%  10.704us         3  3.5680us     240ns  9.4290us  cuDeviceGetCount\n",
            "                    0.00%  8.0110us         2  4.0050us  3.8290us  4.1820us  cudaEventRecord\n",
            "                    0.00%  7.0100us         1  7.0100us  7.0100us  7.0100us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.3340us         1  2.3340us  2.3340us  2.3340us  cudaEventElapsedTime\n",
            "                    0.00%  1.6160us         2     808ns     330ns  1.2860us  cuDeviceGet\n",
            "                    0.00%     283ns         1     283ns     283ns     283ns  cuDeviceGetUuid\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==2409== NVPROF is profiling process 2409, command: ./matrixMul_gpu2\n",
            "==2409== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "GigaFlops: 4.966956==2409== Profiling application: ./matrixMul_gpu2\n",
            "==2409== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.91%  8.9627ms      1000  8.9620us  8.7990us  12.384us  matrixMultGPU(int*, int*, int*)\n",
            "                    0.06%  5.2480us         2  2.6240us  2.4640us  2.7840us  [CUDA memcpy HtoD]\n",
            "                    0.03%  2.9440us         1  2.9440us  2.9440us  2.9440us  [CUDA memcpy DtoH]\n",
            "      API calls:   93.00%  185.08ms         2  92.541ms     901ns  185.08ms  cudaEventCreate\n",
            "                    4.16%  8.2803ms         1  8.2803ms  8.2803ms  8.2803ms  cudaEventSynchronize\n",
            "                    2.29%  4.5481ms      1000  4.5480us  3.4360us  61.847us  cudaLaunchKernel\n",
            "                    0.26%  521.65us         1  521.65us  521.65us  521.65us  cuDeviceTotalMem\n",
            "                    0.10%  204.26us       101  2.0220us     149ns  74.283us  cuDeviceGetAttribute\n",
            "                    0.07%  144.73us         3  48.243us  2.0750us  139.45us  cudaMalloc\n",
            "                    0.06%  120.11us         3  40.035us  2.6630us  110.21us  cudaFree\n",
            "                    0.03%  62.163us         3  20.721us  13.857us  28.280us  cudaMemcpy\n",
            "                    0.01%  22.806us         1  22.806us  22.806us  22.806us  cuDeviceGetName\n",
            "                    0.00%  7.0240us         2  3.5120us  3.4200us  3.6040us  cudaEventRecord\n",
            "                    0.00%  5.7200us         1  5.7200us  5.7200us  5.7200us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.5790us         1  2.5790us  2.5790us  2.5790us  cudaEventElapsedTime\n",
            "                    0.00%  1.9500us         3     650ns     218ns  1.1210us  cuDeviceGetCount\n",
            "                    0.00%  1.3290us         2     664ns     351ns     978ns  cuDeviceGet\n",
            "                    0.00%     346ns         1     346ns     346ns     346ns  cuDeviceGetUuid\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==2454== NVPROF is profiling process 2454, command: ./matrixMul_gpu2\n",
            "==2454== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "GigaFlops: 21.185859==2454== Profiling application: ./matrixMul_gpu2\n",
            "==2454== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.92%  19.783ms      1000  19.782us  19.072us  20.736us  matrixMultGPU(int*, int*, int*)\n",
            "                    0.05%  10.496us         2  5.2480us  5.1520us  5.3440us  [CUDA memcpy HtoD]\n",
            "                    0.02%  4.6720us         1  4.6720us  4.6720us  4.6720us  [CUDA memcpy DtoH]\n",
            "      API calls:   87.29%  175.80ms         2  87.898ms     813ns  175.79ms  cudaEventCreate\n",
            "                    9.81%  19.754ms         1  19.754ms  19.754ms  19.754ms  cudaEventSynchronize\n",
            "                    2.28%  4.5865ms      1000  4.5860us  3.3930us  65.873us  cudaLaunchKernel\n",
            "                    0.25%  510.19us         1  510.19us  510.19us  510.19us  cuDeviceTotalMem\n",
            "                    0.10%  195.94us         3  65.314us  4.6870us  178.79us  cudaFree\n",
            "                    0.10%  191.85us       101  1.8990us     158ns  82.067us  cuDeviceGetAttribute\n",
            "                    0.08%  168.98us         3  56.327us  1.8980us  163.82us  cudaMalloc\n",
            "                    0.08%  151.18us         3  50.394us  19.377us  75.078us  cudaMemcpy\n",
            "                    0.01%  20.560us         1  20.560us  20.560us  20.560us  cuDeviceGetName\n",
            "                    0.00%  8.4340us         2  4.2170us  4.0790us  4.3550us  cudaEventRecord\n",
            "                    0.00%  6.5600us         1  6.5600us  6.5600us  6.5600us  cuDeviceGetPCIBusId\n",
            "                    0.00%  4.5720us         1  4.5720us  4.5720us  4.5720us  cudaEventElapsedTime\n",
            "                    0.00%  1.7170us         3     572ns     190ns     960ns  cuDeviceGetCount\n",
            "                    0.00%  1.2710us         2     635ns     293ns     978ns  cuDeviceGet\n",
            "                    0.00%     335ns         1     335ns     335ns     335ns  cuDeviceGetUuid\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==2499== NVPROF is profiling process 2499, command: ./matrixMul_gpu2\n",
            "==2499== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "GigaFlops: 35.091985==2499== Profiling application: ./matrixMul_gpu2\n",
            "==2499== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.96%  114.63ms      1000  114.63us  112.00us  123.23us  matrixMultGPU(int*, int*, int*)\n",
            "                    0.03%  32.384us         2  16.192us  16.128us  16.256us  [CUDA memcpy HtoD]\n",
            "                    0.01%  11.360us         1  11.360us  11.360us  11.360us  [CUDA memcpy DtoH]\n",
            "      API calls:   60.84%  187.05ms         2  93.526ms  1.6520us  187.05ms  cudaEventCreate\n",
            "                   36.66%  112.70ms         1  112.70ms  112.70ms  112.70ms  cudaEventSynchronize\n",
            "                    2.07%  6.3652ms      1000  6.3650us  3.9120us  624.84us  cudaLaunchKernel\n",
            "                    0.15%  456.94us         1  456.94us  456.94us  456.94us  cuDeviceTotalMem\n",
            "                    0.08%  244.32us         3  81.441us  3.4500us  209.51us  cudaFree\n",
            "                    0.07%  205.55us         3  68.516us  3.3260us  196.36us  cudaMalloc\n",
            "                    0.07%  204.20us         3  68.065us  42.141us  119.00us  cudaMemcpy\n",
            "                    0.05%  164.05us       101  1.6240us     143ns  66.043us  cuDeviceGetAttribute\n",
            "                    0.01%  22.081us         1  22.081us  22.081us  22.081us  cuDeviceGetName\n",
            "                    0.00%  11.090us         2  5.5450us  4.2870us  6.8030us  cudaEventRecord\n",
            "                    0.00%  6.3970us         1  6.3970us  6.3970us  6.3970us  cuDeviceGetPCIBusId\n",
            "                    0.00%  3.7700us         1  3.7700us  3.7700us  3.7700us  cudaEventElapsedTime\n",
            "                    0.00%  1.6760us         3     558ns     161ns     919ns  cuDeviceGetCount\n",
            "                    0.00%  1.2050us         2     602ns     223ns     982ns  cuDeviceGet\n",
            "                    0.00%     311ns         1     311ns     311ns     311ns  cuDeviceGetUuid\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==2544== NVPROF is profiling process 2544, command: ./matrixMul_gpu2\n",
            "==2544== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "GigaFlops: 61.677500==2544== Profiling application: ./matrixMul_gpu2\n",
            "==2544== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.99%  4.34782s      1000  4.3478ms  4.1553ms  6.5073ms  matrixMultGPU(int*, int*, int*)\n",
            "                    0.01%  280.00us         2  140.00us  135.30us  144.70us  [CUDA memcpy HtoD]\n",
            "                    0.00%  120.41us         1  120.41us  120.41us  120.41us  [CUDA memcpy DtoH]\n",
            "      API calls:   95.69%  4.34767s         1  4.34767s  4.34767s  4.34767s  cudaEventSynchronize\n",
            "                    4.15%  188.35ms         2  94.175ms  1.1470us  188.35ms  cudaEventCreate\n",
            "                    0.10%  4.3676ms      1000  4.3670us  3.4360us  32.291us  cudaLaunchKernel\n",
            "                    0.03%  1.2964ms         3  432.14us  96.558us  966.72us  cudaMemcpy\n",
            "                    0.01%  542.28us         3  180.76us  155.54us  219.94us  cudaFree\n",
            "                    0.01%  431.98us         3  143.99us  123.10us  174.08us  cudaMalloc\n",
            "                    0.01%  430.69us         1  430.69us  430.69us  430.69us  cuDeviceTotalMem\n",
            "                    0.00%  188.05us       101  1.8610us     143ns  94.263us  cuDeviceGetAttribute\n",
            "                    0.00%  22.172us         1  22.172us  22.172us  22.172us  cuDeviceGetName\n",
            "                    0.00%  7.0040us         2  3.5020us  2.8550us  4.1490us  cudaEventRecord\n",
            "                    0.00%  6.9410us         1  6.9410us  6.9410us  6.9410us  cuDeviceGetPCIBusId\n",
            "                    0.00%  6.5450us         1  6.5450us  6.5450us  6.5450us  cudaEventElapsedTime\n",
            "                    0.00%  2.0170us         3     672ns     215ns     972ns  cuDeviceGetCount\n",
            "                    0.00%  1.6060us         2     803ns     407ns  1.1990us  cuDeviceGet\n",
            "                    0.00%     344ns         1     344ns     344ns     344ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w591hNMTCROP"
      },
      "source": [
        "<style type=\"text/css\">\n",
        ".tg  {border-collapse:collapse;border-spacing:0;}\n",
        ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
        "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
        ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
        "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
        ".tg .tg-n2k3{background-color:#FFF;color:#000000;font-family:\"Palatino Linotype\", \"Book Antiqua\", Palatino, serif !important;;\n",
        "  text-align:center;vertical-align:top}\n",
        ".tg .tg-6kwm{background-color:#FFF;border-color:inherit;color:#000000;\n",
        "  font-family:\"Palatino Linotype\", \"Book Antiqua\", Palatino, serif !important;;text-align:center;vertical-align:top}\n",
        ".tg .tg-bifh{background-color:#FFF;color:#000000;font-family:\"Palatino Linotype\", \"Book Antiqua\", Palatino, serif !important;;\n",
        "  text-align:center;vertical-align:middle}\n",
        ".tg .tg-h01i{background-color:#FFF;border-color:inherit;color:#000000;\n",
        "  font-family:\"Palatino Linotype\", \"Book Antiqua\", Palatino, serif !important;;text-align:center;vertical-align:middle}\n",
        "</style>\n",
        "<table class=\"tg\">\n",
        "<thead>\n",
        "  <tr>\n",
        "    <th class=\"tg-6kwm\">Sin usar memoria compartida</th>\n",
        "    <th class=\"tg-6kwm\" colspan=\"3\">Tiempo de Ejecución (msec)</th>\n",
        "    <th class=\"tg-h01i\"></th>\n",
        "    <th class=\"tg-bifh\"></th>\n",
        "  </tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "  <tr>\n",
        "    <td class=\"tg-6kwm\">Tamaño de la matriz</td>\n",
        "    <td class=\"tg-6kwm\">CPU- &gt;GPU</td>\n",
        "    <td class=\"tg-6kwm\">GPU- &gt;CPU</td>\n",
        "    <td class=\"tg-6kwm\">Ejecución kernel</td>\n",
        "    <td class=\"tg-6kwm\">Ratio comparado con 128x128</td>\n",
        "    <td class=\"tg-n2k3\">GFLOPs</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-6kwm\">16x16</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-6kwm\">32x32</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-n2k3\">64x64</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-n2k3\">128x128</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-n2k3\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-n2k3\">512x512</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "  </tr>\n",
        "</tbody>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaB6NAsHCDhd"
      },
      "source": [
        "Con memoria compartida:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6ce9CdTBZAL",
        "outputId": "9a1b451f-6766-4fed-f0c9-65de4b4b75bf"
      },
      "source": [
        "!for i in 16 32 64 128 512; do sed -i \"/#define N/c\\#define N $i\" matrixMul_gpu3.cu && /usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true matrixMul_gpu3.cu -o matrixMul_gpu3 -lcudadevrt && nvprof ./matrixMul_gpu3; done"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==2658== NVPROF is profiling process 2658, command: ./matrixMul_gpu3\n",
            "==2658== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "==2658== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
            "GigaFlops: 0.788080==2658== Profiling application: ./matrixMul_gpu3\n",
            "==2658== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.87%  5.2328ms      1000  5.2320us  5.0880us  11.040us  matrixMultGPU(int*, int*, int*, int)\n",
            "                    0.08%  4.2560us         2  2.1280us  1.8560us  2.4000us  [CUDA memcpy HtoD]\n",
            "                    0.05%  2.4960us         1  2.4960us  2.4960us  2.4960us  [CUDA memcpy DtoH]\n",
            "      API calls:   93.82%  191.08ms         2  95.542ms  16.625us  191.07ms  cudaEventCreate\n",
            "                    2.59%  5.2775ms         1  5.2775ms  5.2775ms  5.2775ms  cudaEventSynchronize\n",
            "                    2.34%  4.7629ms      1000  4.7620us  3.5850us  117.66us  cudaLaunchKernel\n",
            "                    0.56%  1.1345ms         1  1.1345ms  1.1345ms  1.1345ms  cuDeviceTotalMem\n",
            "                    0.48%  972.31us       101  9.6260us     214ns  585.35us  cuDeviceGetAttribute\n",
            "                    0.10%  199.91us         3  66.635us  2.0380us  194.03us  cudaMalloc\n",
            "                    0.06%  127.36us         3  42.451us  2.5610us  116.94us  cudaFree\n",
            "                    0.03%  62.573us         3  20.857us  11.910us  27.798us  cudaMemcpy\n",
            "                    0.01%  25.864us         1  25.864us  25.864us  25.864us  cuDeviceGetName\n",
            "                    0.00%  6.7000us         2  3.3500us  3.1120us  3.5880us  cudaEventRecord\n",
            "                    0.00%  6.4450us         1  6.4450us  6.4450us  6.4450us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.6630us         1  2.6630us  2.6630us  2.6630us  cudaEventElapsedTime\n",
            "                    0.00%  2.0940us         3     698ns     364ns  1.1780us  cuDeviceGetCount\n",
            "                    0.00%  1.5080us         2     754ns     275ns  1.2330us  cuDeviceGet\n",
            "                    0.00%     371ns         1     371ns     371ns     371ns  cuDeviceGetUuid\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==2703== NVPROF is profiling process 2703, command: ./matrixMul_gpu3\n",
            "==2703== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "==2703== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
            "GigaFlops: 4.888073==2703== Profiling application: ./matrixMul_gpu3\n",
            "==2703== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.90%  7.8823ms      1000  7.8820us  7.7760us  11.840us  matrixMultGPU(int*, int*, int*, int)\n",
            "                    0.07%  5.1520us         2  2.5760us  2.4000us  2.7520us  [CUDA memcpy HtoD]\n",
            "                    0.04%  2.9120us         1  2.9120us  2.9120us  2.9120us  [CUDA memcpy DtoH]\n",
            "      API calls:   93.12%  193.71ms         2  96.857ms     949ns  193.71ms  cudaEventCreate\n",
            "                    3.27%  6.7928ms      1000  6.7920us  3.6860us  145.09us  cudaLaunchKernel\n",
            "                    2.96%  6.1536ms         1  6.1536ms  6.1536ms  6.1536ms  cudaEventSynchronize\n",
            "                    0.29%  595.99us         1  595.99us  595.99us  595.99us  cuDeviceTotalMem\n",
            "                    0.14%  294.46us       101  2.9150us     152ns  133.62us  cuDeviceGetAttribute\n",
            "                    0.09%  187.26us         3  62.419us  2.5210us  166.55us  cudaMalloc\n",
            "                    0.07%  145.99us         3  48.663us  2.5440us  135.09us  cudaFree\n",
            "                    0.05%  97.674us         3  32.558us  26.771us  37.312us  cudaMemcpy\n",
            "                    0.01%  23.918us         1  23.918us  23.918us  23.918us  cuDeviceGetName\n",
            "                    0.00%  8.9180us         2  4.4590us  4.0490us  4.8690us  cudaEventRecord\n",
            "                    0.00%  7.8250us         1  7.8250us  7.8250us  7.8250us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.6190us         1  2.6190us  2.6190us  2.6190us  cudaEventElapsedTime\n",
            "                    0.00%  2.0520us         3     684ns     222ns  1.1760us  cuDeviceGetCount\n",
            "                    0.00%  1.3090us         2     654ns     247ns  1.0620us  cuDeviceGet\n",
            "                    0.00%     367ns         1     367ns     367ns     367ns  cuDeviceGetUuid\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==2748== NVPROF is profiling process 2748, command: ./matrixMul_gpu3\n",
            "==2748== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "==2748== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
            "GigaFlops: 25.294333==2748== Profiling application: ./matrixMul_gpu3\n",
            "==2748== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.85%  15.881ms      1000  15.880us  15.520us  18.080us  matrixMultGPU(int*, int*, int*, int)\n",
            "                    0.12%  19.488us         2  9.7440us  5.1840us  14.304us  [CUDA memcpy HtoD]\n",
            "                    0.03%  4.6720us         1  4.6720us  4.6720us  4.6720us  [CUDA memcpy DtoH]\n",
            "      API calls:   90.28%  199.47ms         2  99.737ms  1.4230us  199.47ms  cudaEventCreate\n",
            "                    6.81%  15.052ms         1  15.052ms  15.052ms  15.052ms  cudaEventSynchronize\n",
            "                    2.37%  5.2415ms      1000  5.2410us  3.5810us  47.604us  cudaLaunchKernel\n",
            "                    0.25%  548.04us         1  548.04us  548.04us  548.04us  cuDeviceTotalMem\n",
            "                    0.09%  192.04us         3  64.013us  3.2800us  183.43us  cudaMalloc\n",
            "                    0.07%  161.16us       101  1.5950us     147ns  66.376us  cuDeviceGetAttribute\n",
            "                    0.06%  130.12us         3  43.372us  2.8630us  118.97us  cudaFree\n",
            "                    0.04%  95.457us         3  31.819us  27.820us  38.140us  cudaMemcpy\n",
            "                    0.01%  25.474us         2  12.737us  6.3850us  19.089us  cudaEventRecord\n",
            "                    0.01%  23.108us         1  23.108us  23.108us  23.108us  cuDeviceGetName\n",
            "                    0.00%  6.1000us         1  6.1000us  6.1000us  6.1000us  cuDeviceGetPCIBusId\n",
            "                    0.00%  3.0300us         1  3.0300us  3.0300us  3.0300us  cudaEventElapsedTime\n",
            "                    0.00%  1.8700us         3     623ns     166ns     948ns  cuDeviceGetCount\n",
            "                    0.00%  1.4560us         2     728ns     408ns  1.0480us  cuDeviceGet\n",
            "                    0.00%     264ns         1     264ns     264ns     264ns  cuDeviceGetUuid\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==2793== NVPROF is profiling process 2793, command: ./matrixMul_gpu3\n",
            "==2793== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "==2793== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
            "GigaFlops: 51.459022==2793== Profiling application: ./matrixMul_gpu3\n",
            "==2793== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.94%  76.586ms      1000  76.586us  73.599us  80.608us  matrixMultGPU(int*, int*, int*, int)\n",
            "                    0.04%  32.416us         2  16.208us  16.128us  16.288us  [CUDA memcpy HtoD]\n",
            "                    0.01%  11.360us         1  11.360us  11.360us  11.360us  [CUDA memcpy DtoH]\n",
            "      API calls:   70.39%  195.87ms         2  97.934ms  1.0810us  195.87ms  cudaEventCreate\n",
            "                   27.39%  76.215ms         1  76.215ms  76.215ms  76.215ms  cudaEventSynchronize\n",
            "                    1.79%  4.9809ms      1000  4.9800us  3.7910us  55.766us  cudaLaunchKernel\n",
            "                    0.17%  481.56us         1  481.56us  481.56us  481.56us  cuDeviceTotalMem\n",
            "                    0.08%  222.46us         3  74.154us  2.2990us  215.76us  cudaMalloc\n",
            "                    0.06%  163.38us       101  1.6170us     147ns  67.006us  cuDeviceGetAttribute\n",
            "                    0.06%  158.06us         3  52.686us  32.678us  85.985us  cudaMemcpy\n",
            "                    0.04%  122.14us         3  40.713us  2.3940us  111.81us  cudaFree\n",
            "                    0.01%  23.354us         1  23.354us  23.354us  23.354us  cuDeviceGetName\n",
            "                    0.00%  6.9300us         2  3.4650us  2.8930us  4.0370us  cudaEventRecord\n",
            "                    0.00%  6.6890us         1  6.6890us  6.6890us  6.6890us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.7860us         1  2.7860us  2.7860us  2.7860us  cudaEventElapsedTime\n",
            "                    0.00%  1.5050us         3     501ns     192ns     817ns  cuDeviceGetCount\n",
            "                    0.00%  1.3870us         2     693ns     217ns  1.1700us  cuDeviceGet\n",
            "                    0.00%     267ns         1     267ns     267ns     267ns  cuDeviceGetUuid\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==2838== NVPROF is profiling process 2838, command: ./matrixMul_gpu3\n",
            "==2838== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "==2838== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
            "GigaFlops: 96.417440==2838== Profiling application: ./matrixMul_gpu3\n",
            "==2838== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.99%  2.77962s      1000  2.7796ms  2.5974ms  4.0380ms  matrixMultGPU(int*, int*, int*, int)\n",
            "                    0.01%  272.54us         2  136.27us  135.42us  137.12us  [CUDA memcpy HtoD]\n",
            "                    0.00%  88.254us         1  88.254us  88.254us  88.254us  [CUDA memcpy DtoH]\n",
            "      API calls:   93.67%  2.77848s         1  2.77848s  2.77848s  2.77848s  cudaEventSynchronize\n",
            "                    6.05%  179.47ms         2  89.737ms     948ns  179.47ms  cudaEventCreate\n",
            "                    0.18%  5.3526ms      1000  5.3520us  3.6060us  58.239us  cudaLaunchKernel\n",
            "                    0.04%  1.1839ms         3  394.63us  118.87us  833.87us  cudaMemcpy\n",
            "                    0.02%  536.32us         3  178.77us  172.49us  189.76us  cudaMalloc\n",
            "                    0.02%  473.56us         1  473.56us  473.56us  473.56us  cuDeviceTotalMem\n",
            "                    0.02%  457.46us         3  152.49us  130.45us  195.31us  cudaFree\n",
            "                    0.01%  201.58us       101  1.9950us     143ns  86.325us  cuDeviceGetAttribute\n",
            "                    0.00%  26.445us         1  26.445us  26.445us  26.445us  cuDeviceGetName\n",
            "                    0.00%  8.2540us         2  4.1270us  3.8470us  4.4070us  cudaEventRecord\n",
            "                    0.00%  5.8900us         1  5.8900us  5.8900us  5.8900us  cuDeviceGetPCIBusId\n",
            "                    0.00%  5.2260us         1  5.2260us  5.2260us  5.2260us  cudaEventElapsedTime\n",
            "                    0.00%  2.0420us         3     680ns     185ns  1.1190us  cuDeviceGetCount\n",
            "                    0.00%  1.8220us         2     911ns     457ns  1.3650us  cuDeviceGet\n",
            "                    0.00%     294ns         1     294ns     294ns     294ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApWeMA7SGZDN"
      },
      "source": [
        "<style type=\"text/css\">\n",
        ".tg  {border-collapse:collapse;border-spacing:0;}\n",
        ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
        "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
        ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
        "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
        ".tg .tg-n2k3{background-color:#FFF;color:#000000;font-family:\"Palatino Linotype\", \"Book Antiqua\", Palatino, serif !important;;\n",
        "  text-align:center;vertical-align:top}\n",
        ".tg .tg-6kwm{background-color:#FFF;border-color:inherit;color:#000000;\n",
        "  font-family:\"Palatino Linotype\", \"Book Antiqua\", Palatino, serif !important;;text-align:center;vertical-align:top}\n",
        ".tg .tg-bifh{background-color:#FFF;color:#000000;font-family:\"Palatino Linotype\", \"Book Antiqua\", Palatino, serif !important;;\n",
        "  text-align:center;vertical-align:middle}\n",
        ".tg .tg-h01i{background-color:#FFF;border-color:inherit;color:#000000;\n",
        "  font-family:\"Palatino Linotype\", \"Book Antiqua\", Palatino, serif !important;;text-align:center;vertical-align:middle}\n",
        "</style>\n",
        "<table class=\"tg\">\n",
        "<thead>\n",
        "  <tr>\n",
        "    <th class=\"tg-6kwm\">Con memoria compartida</th>\n",
        "    <th class=\"tg-6kwm\" colspan=\"3\">Tiempo de Ejecución (msec)</th>\n",
        "    <th class=\"tg-h01i\"></th>\n",
        "    <th class=\"tg-bifh\"></th>\n",
        "  </tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "  <tr>\n",
        "    <td class=\"tg-6kwm\">Tamaño de la matriz</td>\n",
        "    <td class=\"tg-6kwm\">CPU- &gt;GPU</td>\n",
        "    <td class=\"tg-6kwm\">GPU- &gt;CPU</td>\n",
        "    <td class=\"tg-6kwm\">Ejecución kernel</td>\n",
        "    <td class=\"tg-6kwm\">Ratio comparado con 128x128</td>\n",
        "    <td class=\"tg-n2k3\">GFLOPs</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-6kwm\">16x16</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-6kwm\">32x32</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-n2k3\">64x64</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-n2k3\">128x128</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-n2k3\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-n2k3\">512x512</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "  </tr>\n",
        "</tbody>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv0tV9ofCG62"
      },
      "source": [
        "## Revisión de los ejemplos instalados con CUDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mq9bEvZ6JUpq",
        "outputId": "0d64d385-af25-411a-b668-486bc0f8dc3e"
      },
      "source": [
        "%cd /usr/local/cuda/samples\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/cuda-11.1/samples\n",
            "\u001b[0m\u001b[01;34m0_Simple\u001b[0m/     \u001b[01;34m2_Graphics\u001b[0m/  \u001b[01;34m4_Finance\u001b[0m/      \u001b[01;34m6_Advanced\u001b[0m/       \u001b[01;34mbin\u001b[0m/     EULA.txt\n",
            "\u001b[01;34m1_Utilities\u001b[0m/  \u001b[01;34m3_Imaging\u001b[0m/   \u001b[01;34m5_Simulations\u001b[0m/  \u001b[01;34m7_CUDALibraries\u001b[0m/  \u001b[01;34mcommon\u001b[0m/  Makefile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tR1tfxYzJeVc",
        "outputId": "780a0d0a-4688-44f2-bf34-fd79f5b85ef0"
      },
      "source": [
        "%cd  0_Simple/matrixMul/\n",
        "%ls\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/cuda-11.1/samples/0_Simple/matrixMul\n",
            "Makefile      NsightEclipse.xml  \u001b[0m\u001b[01;32mstencil1d_base\u001b[0m*    \u001b[01;32mstenciltest_sm_2\u001b[0m*\n",
            "matrixMul.cu  readme.txt         stencil1d_base.cu  stenciltest_sm_2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "m485XaWJJ43M",
        "outputId": "be43c40d-ca88-437e-d3fb-1902823f70b0"
      },
      "source": [
        "%pwd\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/usr/local/cuda-11.1/samples/0_Simple/matrixMul'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2AAxHSoKX_K",
        "outputId": "a682299b-d367-471d-b0d1-709242bc9a94"
      },
      "source": [
        "!cat matrixMul.cu\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/**\n",
            " * Copyright 1993-2015 NVIDIA Corporation.  All rights reserved.\n",
            " *\n",
            " * Please refer to the NVIDIA end user license agreement (EULA) associated\n",
            " * with this source code for terms and conditions that govern your use of\n",
            " * this software. Any use, reproduction, disclosure, or distribution of\n",
            " * this software and related documentation outside the terms of the EULA\n",
            " * is strictly prohibited.\n",
            " *\n",
            " */\n",
            "\n",
            "/**\n",
            " * Matrix multiplication: C = A * B.\n",
            " * Host code.\n",
            " *\n",
            " * This sample implements matrix multiplication which makes use of shared memory\n",
            " * to ensure data reuse, the matrix multiplication is done using tiling approach.\n",
            " * It has been written for clarity of exposition to illustrate various CUDA programming\n",
            " * principles, not with the goal of providing the most performant generic kernel for matrix multiplication.\n",
            " * See also:\n",
            " * V. Volkov and J. Demmel, \"Benchmarking GPUs to tune dense linear algebra,\"\n",
            " * in Proc. 2008 ACM/IEEE Conf. on Supercomputing (SC '08),\n",
            " * Piscataway, NJ: IEEE Press, 2008, pp. Art. 31:1-11.\n",
            " */\n",
            "\n",
            "// System includes\n",
            "#include <stdio.h>\n",
            "#include <assert.h>\n",
            "\n",
            "// CUDA runtime\n",
            "#include <cuda_runtime.h>\n",
            "\n",
            "// Helper functions and utilities to work with CUDA\n",
            "#include <helper_functions.h>\n",
            "#include <helper_cuda.h>\n",
            "\n",
            "/**\n",
            " * Matrix multiplication (CUDA Kernel) on the device: C = A * B\n",
            " * wA is A's width and wB is B's width\n",
            " */\n",
            "template <int BLOCK_SIZE> __global__ void MatrixMulCUDA(float *C, float *A,\n",
            "                                                        float *B, int wA,\n",
            "                                                        int wB) {\n",
            "    // Block index\n",
            "    int bx = blockIdx.x;\n",
            "    int by = blockIdx.y;\n",
            "\n",
            "    // Thread index\n",
            "    int tx = threadIdx.x;\n",
            "    int ty = threadIdx.y;\n",
            "\n",
            "    // Index of the first sub-matrix of A processed by the block\n",
            "    int aBegin = wA * BLOCK_SIZE * by;\n",
            "\n",
            "    // Index of the last sub-matrix of A processed by the block\n",
            "    int aEnd   = aBegin + wA - 1;\n",
            "\n",
            "    // Step size used to iterate through the sub-matrices of A\n",
            "    int aStep  = BLOCK_SIZE;\n",
            "\n",
            "    // Index of the first sub-matrix of B processed by the block\n",
            "    int bBegin = BLOCK_SIZE * bx;\n",
            "\n",
            "    // Step size used to iterate through the sub-matrices of B\n",
            "    int bStep  = BLOCK_SIZE * wB;\n",
            "\n",
            "    // Csub is used to store the element of the block sub-matrix\n",
            "    // that is computed by the thread\n",
            "    float Csub = 0;\n",
            "\n",
            "    // Loop over all the sub-matrices of A and B\n",
            "    // required to compute the block sub-matrix\n",
            "    for (int a = aBegin, b = bBegin;\n",
            "            a <= aEnd;\n",
            "            a += aStep, b += bStep) {\n",
            "        // Declaration of the shared memory array As used to\n",
            "        // store the sub-matrix of A\n",
            "        __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];\n",
            "\n",
            "        // Declaration of the shared memory array Bs used to\n",
            "        // store the sub-matrix of B\n",
            "        __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n",
            "\n",
            "        // Load the matrices from device memory\n",
            "        // to shared memory; each thread loads\n",
            "        // one element of each matrix\n",
            "        As[ty][tx] = A[a + wA * ty + tx];\n",
            "        Bs[ty][tx] = B[b + wB * ty + tx];\n",
            "\n",
            "        // Synchronize to make sure the matrices are loaded\n",
            "        __syncthreads();\n",
            "\n",
            "        // Multiply the two matrices together;\n",
            "        // each thread computes one element\n",
            "        // of the block sub-matrix\n",
            "#pragma unroll\n",
            "\n",
            "        for (int k = 0; k < BLOCK_SIZE; ++k) {\n",
            "            Csub += As[ty][k] * Bs[k][tx];\n",
            "        }\n",
            "\n",
            "        // Synchronize to make sure that the preceding\n",
            "        // computation is done before loading two new\n",
            "        // sub-matrices of A and B in the next iteration\n",
            "        __syncthreads();\n",
            "    }\n",
            "\n",
            "    // Write the block sub-matrix to device memory;\n",
            "    // each thread writes one element\n",
            "    int c = wB * BLOCK_SIZE * by + BLOCK_SIZE * bx;\n",
            "    C[c + wB * ty + tx] = Csub;\n",
            "}\n",
            "\n",
            "void ConstantInit(float *data, int size, float val) {\n",
            "    for (int i = 0; i < size; ++i) {\n",
            "        data[i] = val;\n",
            "    }\n",
            "}\n",
            "\n",
            "/**\n",
            " * Run a simple test of matrix multiplication using CUDA\n",
            " */\n",
            "int MatrixMultiply(int argc, char **argv,\n",
            "                   int block_size, const dim3 &dimsA,\n",
            "                   const dim3 &dimsB) {\n",
            "    // Allocate host memory for matrices A and B\n",
            "    unsigned int size_A = dimsA.x * dimsA.y;\n",
            "    unsigned int mem_size_A = sizeof(float) * size_A;\n",
            "    float *h_A;\n",
            "    checkCudaErrors(cudaMallocHost(&h_A, mem_size_A));\n",
            "    unsigned int size_B = dimsB.x * dimsB.y;\n",
            "    unsigned int mem_size_B = sizeof(float) * size_B;\n",
            "    float *h_B;\n",
            "    checkCudaErrors(cudaMallocHost(&h_B, mem_size_B));\n",
            "    cudaStream_t stream;\n",
            "\n",
            "    // Initialize host memory\n",
            "    const float valB = 0.01f;\n",
            "    ConstantInit(h_A, size_A, 1.0f);\n",
            "    ConstantInit(h_B, size_B, valB);\n",
            "\n",
            "    // Allocate device memory\n",
            "    float *d_A, *d_B, *d_C;\n",
            "\n",
            "    // Allocate host matrix C\n",
            "    dim3 dimsC(dimsB.x, dimsA.y, 1);\n",
            "    unsigned int mem_size_C = dimsC.x * dimsC.y * sizeof(float);\n",
            "    float *h_C;\n",
            "    checkCudaErrors(cudaMallocHost(&h_C, mem_size_C));\n",
            "\n",
            "    if (h_C == NULL) {\n",
            "        fprintf(stderr, \"Failed to allocate host matrix C!\\n\");\n",
            "        exit(EXIT_FAILURE);\n",
            "    }\n",
            "\n",
            "    checkCudaErrors(cudaMalloc(reinterpret_cast<void **>(&d_A), mem_size_A));\n",
            "    checkCudaErrors(cudaMalloc(reinterpret_cast<void **>(&d_B), mem_size_B));\n",
            "    checkCudaErrors(cudaMalloc(reinterpret_cast<void **>(&d_C), mem_size_C));\n",
            "    // Allocate CUDA events that we'll use for timing\n",
            "    cudaEvent_t start, stop;\n",
            "    checkCudaErrors(cudaEventCreate(&start));\n",
            "    checkCudaErrors(cudaEventCreate(&stop));\n",
            "\n",
            "    checkCudaErrors(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));\n",
            "\n",
            "    // copy host memory to device\n",
            "    checkCudaErrors(cudaMemcpyAsync(d_A, h_A, mem_size_A, cudaMemcpyHostToDevice, stream));\n",
            "    checkCudaErrors(cudaMemcpyAsync(d_B, h_B, mem_size_B, cudaMemcpyHostToDevice, stream));\n",
            "\n",
            "    // Setup execution parameters\n",
            "    dim3 threads(block_size, block_size);\n",
            "    dim3 grid(dimsB.x / threads.x, dimsA.y / threads.y);\n",
            "\n",
            "    // Create and start timer\n",
            "    printf(\"Computing result using CUDA Kernel...\\n\");\n",
            "\n",
            "    // Performs warmup operation using matrixMul CUDA kernel\n",
            "    if (block_size == 16) {\n",
            "        MatrixMulCUDA<16> <<< grid, threads, 0, stream>>>(d_C, d_A, d_B,\n",
            "                                                dimsA.x, dimsB.x);\n",
            "    } else {\n",
            "        MatrixMulCUDA<32> <<< grid, threads, 0, stream>>>(d_C, d_A, d_B,\n",
            "                                                dimsA.x, dimsB.x);\n",
            "    }\n",
            "\n",
            "    printf(\"done\\n\");\n",
            "    checkCudaErrors(cudaStreamSynchronize(stream));\n",
            "\n",
            "    // Record the start event\n",
            "    checkCudaErrors(cudaEventRecord(start, stream));\n",
            "\n",
            "    // Execute the kernel\n",
            "    int nIter = 300;\n",
            "\n",
            "    for (int j = 0; j < nIter; j++) {\n",
            "        if (block_size == 16) {\n",
            "            MatrixMulCUDA<16> <<<grid, threads, 0, stream>>>(d_C, d_A, d_B,\n",
            "                                                    dimsA.x, dimsB.x);\n",
            "        } else {\n",
            "            MatrixMulCUDA<32> <<<grid, threads, 0, stream>>>(d_C, d_A, d_B,\n",
            "                                                    dimsA.x, dimsB.x);\n",
            "        }\n",
            "    }\n",
            "\n",
            "    // Record the stop event\n",
            "    checkCudaErrors(cudaEventRecord(stop, stream));\n",
            "\n",
            "    // Wait for the stop event to complete\n",
            "    checkCudaErrors(cudaEventSynchronize(stop));\n",
            "\n",
            "    float msecTotal = 0.0f;\n",
            "    checkCudaErrors(cudaEventElapsedTime(&msecTotal, start, stop));\n",
            "\n",
            "    // Compute and print the performance\n",
            "    float msecPerMatrixMul = msecTotal / nIter;\n",
            "    double flopsPerMatrixMul = 2.0 * static_cast<double>(dimsA.x) *\n",
            "                               static_cast<double>(dimsA.y) *\n",
            "                               static_cast<double>(dimsB.x);\n",
            "    double gigaFlops = (flopsPerMatrixMul * 1.0e-9f) /\n",
            "                       (msecPerMatrixMul / 1000.0f);\n",
            "    printf(\n",
            "        \"Performance= %.2f GFlop/s, Time= %.3f msec, Size= %.0f Ops,\" \\\n",
            "        \" WorkgroupSize= %u threads/block\\n\",\n",
            "        gigaFlops,\n",
            "        msecPerMatrixMul,\n",
            "        flopsPerMatrixMul,\n",
            "        threads.x * threads.y);\n",
            "\n",
            "    // Copy result from device to host\n",
            "    checkCudaErrors(cudaMemcpyAsync(h_C, d_C, mem_size_C, cudaMemcpyDeviceToHost, stream));\n",
            "    checkCudaErrors(cudaStreamSynchronize(stream));\n",
            "\n",
            "    printf(\"Checking computed result for correctness: \");\n",
            "    bool correct = true;\n",
            "\n",
            "    // test relative error by the formula\n",
            "    //     |<x, y>_cpu - <x,y>_gpu|/<|x|, |y|>  < eps\n",
            "    double eps = 1.e-6;  // machine zero\n",
            "\n",
            "    for (int i = 0; i < static_cast<int>(dimsC.x * dimsC.y); i++) {\n",
            "        double abs_err = fabs(h_C[i] - (dimsA.x * valB));\n",
            "        double dot_length = dimsA.x;\n",
            "        double abs_val = fabs(h_C[i]);\n",
            "        double rel_err = abs_err / abs_val / dot_length;\n",
            "\n",
            "        if (rel_err > eps) {\n",
            "            printf(\"Error! Matrix[%05d]=%.8f, ref=%.8f error term is > %E\\n\",\n",
            "                   i, h_C[i], dimsA.x * valB, eps);\n",
            "            correct = false;\n",
            "        }\n",
            "    }\n",
            "\n",
            "    printf(\"%s\\n\", correct ? \"Result = PASS\" : \"Result = FAIL\");\n",
            "\n",
            "    // Clean up memory\n",
            "    checkCudaErrors(cudaFreeHost(h_A));\n",
            "    checkCudaErrors(cudaFreeHost(h_B));\n",
            "    checkCudaErrors(cudaFreeHost(h_C));\n",
            "    checkCudaErrors(cudaFree(d_A));\n",
            "    checkCudaErrors(cudaFree(d_B));\n",
            "    checkCudaErrors(cudaFree(d_C));\n",
            "    checkCudaErrors(cudaEventDestroy(start));\n",
            "    checkCudaErrors(cudaEventDestroy(stop));\n",
            "    printf(\"\\nNOTE: The CUDA Samples are not meant for performance\"\\\n",
            "           \"measurements. Results may vary when GPU Boost is enabled.\\n\");\n",
            "\n",
            "    if (correct) {\n",
            "        return EXIT_SUCCESS;\n",
            "    } else {\n",
            "        return EXIT_FAILURE;\n",
            "    }\n",
            "}\n",
            "\n",
            "\n",
            "/**\n",
            " * Program main\n",
            " */\n",
            "int main(int argc, char **argv) {\n",
            "    printf(\"[Matrix Multiply Using CUDA] - Starting...\\n\");\n",
            "\n",
            "    if (checkCmdLineFlag(argc, (const char **)argv, \"help\") ||\n",
            "            checkCmdLineFlag(argc, (const char **)argv, \"?\")) {\n",
            "        printf(\"Usage -device=n (n >= 0 for deviceID)\\n\");\n",
            "        printf(\"      -wA=WidthA -hA=HeightA (Width x Height of Matrix A)\\n\");\n",
            "        printf(\"      -wB=WidthB -hB=HeightB (Width x Height of Matrix B)\\n\");\n",
            "        printf(\"  Note: Outer matrix dimensions of A & B matrices\" \\\n",
            "               \" must be equal.\\n\");\n",
            "\n",
            "        exit(EXIT_SUCCESS);\n",
            "    }\n",
            "\n",
            "    // This will pick the best possible CUDA capable device, otherwise\n",
            "    // override the device ID based on input provided at the command line\n",
            "    int dev = findCudaDevice(argc, (const char **)argv);\n",
            "\n",
            "    int block_size = 32;\n",
            "\n",
            "    dim3 dimsA(5 * 2 * block_size, 5 * 2 * block_size, 1);\n",
            "    dim3 dimsB(5 * 4 * block_size, 5 * 2 * block_size, 1);\n",
            "\n",
            "    // width of Matrix A\n",
            "    if (checkCmdLineFlag(argc, (const char **)argv, \"wA\")) {\n",
            "        dimsA.x = getCmdLineArgumentInt(argc, (const char **)argv, \"wA\");\n",
            "    }\n",
            "\n",
            "    // height of Matrix A\n",
            "    if (checkCmdLineFlag(argc, (const char **)argv, \"hA\")) {\n",
            "        dimsA.y = getCmdLineArgumentInt(argc, (const char **)argv, \"hA\");\n",
            "    }\n",
            "\n",
            "    // width of Matrix B\n",
            "    if (checkCmdLineFlag(argc, (const char **)argv, \"wB\")) {\n",
            "        dimsB.x = getCmdLineArgumentInt(argc, (const char **)argv, \"wB\");\n",
            "    }\n",
            "\n",
            "    // height of Matrix B\n",
            "    if (checkCmdLineFlag(argc, (const char **)argv, \"hB\")) {\n",
            "        dimsB.y = getCmdLineArgumentInt(argc, (const char **)argv, \"hB\");\n",
            "    }\n",
            "\n",
            "    if (dimsA.x != dimsB.y) {\n",
            "        printf(\"Error: outer matrix dimensions must be equal. (%d != %d)\\n\",\n",
            "               dimsA.x, dimsB.y);\n",
            "        exit(EXIT_FAILURE);\n",
            "    }\n",
            "\n",
            "    printf(\"MatrixA(%d,%d), MatrixB(%d,%d)\\n\", dimsA.x, dimsA.y,\n",
            "                                               dimsB.x, dimsB.y);\n",
            "\n",
            "    int matrix_result = MatrixMultiply(argc, argv, block_size, dimsA, dimsB);\n",
            "\n",
            "    exit(matrix_result);\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFPG0ssAl_lY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}