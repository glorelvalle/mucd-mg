{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "P2CUDAbarrosovalle_tutorial+ejercicios.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H83eQ91UUo4c"
      },
      "source": [
        "# Práctica 2: Programación GPU y Computación Cuántica\n",
        "\n",
        "María Barroso Honrubia\n",
        "\n",
        "Gloria del Valle Cano"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbHv-FMrvZI8"
      },
      "source": [
        "## Parte 1: Tutorial\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y44sGKPivjB6"
      },
      "source": [
        "### Introducción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP1qHupkVOHt"
      },
      "source": [
        "Primero comprobamos las características que nos ofrece Google Colab con $\\texttt{lscpu}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPsAm64NUPjz",
        "outputId": "8e8f50e4-4891-4bd4-b7c8-de8680d0e9ea"
      },
      "source": [
        "!lscpu"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture:        x86_64\n",
            "CPU op-mode(s):      32-bit, 64-bit\n",
            "Byte Order:          Little Endian\n",
            "CPU(s):              2\n",
            "On-line CPU(s) list: 0,1\n",
            "Thread(s) per core:  2\n",
            "Core(s) per socket:  1\n",
            "Socket(s):           1\n",
            "NUMA node(s):        1\n",
            "Vendor ID:           GenuineIntel\n",
            "CPU family:          6\n",
            "Model:               63\n",
            "Model name:          Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "Stepping:            0\n",
            "CPU MHz:             2299.998\n",
            "BogoMIPS:            4599.99\n",
            "Hypervisor vendor:   KVM\n",
            "Virtualization type: full\n",
            "L1d cache:           32K\n",
            "L1i cache:           32K\n",
            "L2 cache:            256K\n",
            "L3 cache:            46080K\n",
            "NUMA node0 CPU(s):   0,1\n",
            "Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt arat md_clear arch_capabilities\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQCR4SR1VSj-"
      },
      "source": [
        "Comprobamos la memoria física disponible y swap del sistema con $\\texttt{free}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jipoTT7VXPv",
        "outputId": "afe1909a-8a85-4d70-976a-e2e084a31a27"
      },
      "source": [
        "!free -kh"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:            12G        572M        9.9G        1.2M        2.2G         11G\n",
            "Swap:            0B          0B          0B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsMVLr4R39eD"
      },
      "source": [
        "Observamos con más profundidad información del sistema como memoria total RAM, memoria usada para la caché o número total de swaps:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3ZEIoaAVe8o",
        "outputId": "c5c01ee6-8882-4906-ec7b-f841c44b0493"
      },
      "source": [
        "!cat /proc/meminfo"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MemTotal:       13302924 kB\n",
            "MemFree:        10361308 kB\n",
            "MemAvailable:   12449604 kB\n",
            "Buffers:          131244 kB\n",
            "Cached:          2080512 kB\n",
            "SwapCached:            0 kB\n",
            "Active:          1059708 kB\n",
            "Inactive:        1599484 kB\n",
            "Active(anon):     400672 kB\n",
            "Inactive(anon):      440 kB\n",
            "Active(file):     659036 kB\n",
            "Inactive(file):  1599044 kB\n",
            "Unevictable:           0 kB\n",
            "Mlocked:               0 kB\n",
            "SwapTotal:             0 kB\n",
            "SwapFree:              0 kB\n",
            "Dirty:               940 kB\n",
            "Writeback:             0 kB\n",
            "AnonPages:        447444 kB\n",
            "Mapped:           239832 kB\n",
            "Shmem:              1188 kB\n",
            "KReclaimable:     143384 kB\n",
            "Slab:             193976 kB\n",
            "SReclaimable:     143384 kB\n",
            "SUnreclaim:        50592 kB\n",
            "KernelStack:        4796 kB\n",
            "PageTables:         6180 kB\n",
            "NFS_Unstable:          0 kB\n",
            "Bounce:                0 kB\n",
            "WritebackTmp:          0 kB\n",
            "CommitLimit:     6651460 kB\n",
            "Committed_AS:    3114684 kB\n",
            "VmallocTotal:   34359738367 kB\n",
            "VmallocUsed:       44772 kB\n",
            "VmallocChunk:          0 kB\n",
            "Percpu:             1568 kB\n",
            "AnonHugePages:         0 kB\n",
            "ShmemHugePages:        0 kB\n",
            "ShmemPmdMapped:        0 kB\n",
            "FileHugePages:         0 kB\n",
            "FilePmdMapped:         0 kB\n",
            "CmaTotal:              0 kB\n",
            "CmaFree:               0 kB\n",
            "HugePages_Total:       0\n",
            "HugePages_Free:        0\n",
            "HugePages_Rsvd:        0\n",
            "HugePages_Surp:        0\n",
            "Hugepagesize:       2048 kB\n",
            "Hugetlb:               0 kB\n",
            "DirectMap4k:      150336 kB\n",
            "DirectMap2M:     5089280 kB\n",
            "DirectMap1G:    10485760 kB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4wJQ_pYXXRZ"
      },
      "source": [
        "Observamos el funcionamiento de diferentes comandos útiles:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkZaPypuXaSs"
      },
      "source": [
        "Listar el contenido:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suXeGTH4Xh5n",
        "outputId": "11da79ab-0703-4cb2-8a13-3f5cd7b49c30"
      },
      "source": [
        "!ls -la"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 16\n",
            "drwxr-xr-x 1 root root 4096 Nov  1 13:35 .\n",
            "drwxr-xr-x 1 root root 4096 Nov  7 13:42 ..\n",
            "drwxr-xr-x 4 root root 4096 Nov  1 13:34 .config\n",
            "drwxr-xr-x 1 root root 4096 Nov  1 13:35 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tqmQjJRXq0N"
      },
      "source": [
        "Vemos el contenido del directorio /usr/local  donde está instalado CUDA:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pSvEcb0X6m2",
        "outputId": "956d5cbc-254c-448c-ded0-f764f95975d9"
      },
      "source": [
        "!ls -la /usr/local"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 84\n",
            "drwxr-xr-x  1 root root 4096 Nov  1 13:57 .\n",
            "drwxr-xr-x  1 root root 4096 Nov  7 13:42 ..\n",
            "drwxr-xr-x  1 root root 4096 Nov  4 13:12 bin\n",
            "lrwxrwxrwx  1 root root   22 Nov  1 13:27 cuda -> /etc/alternatives/cuda\n",
            "drwxr-xr-x 16 root root 4096 Nov  1 13:19 cuda-10.0\n",
            "drwxr-xr-x 15 root root 4096 Nov  1 13:21 cuda-10.1\n",
            "lrwxrwxrwx  1 root root   25 Nov  1 13:27 cuda-11 -> /etc/alternatives/cuda-11\n",
            "drwxr-xr-x 15 root root 4096 Nov  1 13:24 cuda-11.0\n",
            "drwxr-xr-x  1 root root 4096 Nov  1 13:26 cuda-11.1\n",
            "drwxr-xr-x  1 root root 4096 Nov  1 13:36 etc\n",
            "drwxr-xr-x  2 root root 4096 Nov 19  2020 games\n",
            "drwxr-xr-x  2 root root 4096 Nov  1 13:47 _gcs_config_ops.so\n",
            "drwxr-xr-x  1 root root 4096 Nov  1 13:57 include\n",
            "drwxr-xr-x  1 root root 4096 Nov  1 13:58 lib\n",
            "-rw-r--r--  1 root root 1636 Nov  1 13:52 LICENSE.txt\n",
            "drwxr-xr-x  3 root root 4096 Nov  1 13:47 licensing\n",
            "lrwxrwxrwx  1 root root    9 Nov 19  2020 man -> share/man\n",
            "drwxr-xr-x  2 root root 4096 Nov 19  2020 sbin\n",
            "-rw-r--r--  1 root root 7291 Nov  1 13:52 setup.cfg\n",
            "drwxr-xr-x  1 root root 4096 Nov  1 13:45 share\n",
            "drwxr-xr-x  2 root root 4096 Nov 19  2020 src\n",
            "drwxr-xr-x  2 root root 4096 Nov  1 13:59 xgboost\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsU2aQ3NZks5",
        "outputId": "fae9b10c-73e4-4f95-f22f-98f1825fbdb2"
      },
      "source": [
        "!ls -la /usr/local/cuda-11.1/bin"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 80220\n",
            "drwxr-xr-x 1 root root     4096 Nov  1 13:26 .\n",
            "drwxr-xr-x 1 root root     4096 Nov  1 13:26 ..\n",
            "-rwxr-xr-x 1 root root    80608 Oct 13  2020 bin2c\n",
            "lrwxrwxrwx 1 root root        4 Oct 13  2020 computeprof -> nvvp\n",
            "-rwxr-xr-x 1 root root       97 Oct 13  2020 compute-sanitizer\n",
            "drwxr-xr-x 2 root root     4096 Dec 14  2020 crt\n",
            "-rwxr-xr-x 1 root root  5190640 Oct 13  2020 cudafe++\n",
            "-rwxr-xr-x 1 root root 12035840 Oct 13  2020 cuda-gdb\n",
            "-rwxr-xr-x 1 root root   753776 Oct 13  2020 cuda-gdbserver\n",
            "-rwxr-xr-x 1 root root      800 Oct 13  2020 cuda-install-samples-11.1.sh\n",
            "-rwxr-xr-x 1 root root   353752 Oct 13  2020 cuda-memcheck\n",
            "-rwxr-xr-x 1 root root   232752 Sep 16  2020 cuobjdump\n",
            "-rwxr-xr-x 1 root root   269392 Oct 13  2020 fatbinary\n",
            "-rwxr-xr-x 1 root root     2974 Oct 16  2020 ncu\n",
            "-rwxr-xr-x 1 root root     2577 Oct 16  2020 ncu-ui\n",
            "-rwxr-xr-x 1 root root     1580 Oct 13  2020 nsight_ee_plugins_manage.sh\n",
            "-rwxr-xr-x 1 root root      745 Oct 16  2020 nsight-sys\n",
            "-rwxr-xr-x 1 root root      746 Oct 16  2020 nsys\n",
            "-rwxr-xr-x 1 root root      104 Oct 16  2020 nsys-exporter\n",
            "-rwxr-xr-x 1 root root       85 Oct 16  2020 nsys-ui\n",
            "-rwxr-xr-x 1 root root  5120848 Oct 13  2020 nvcc\n",
            "-rw-r--r-- 1 root root      417 Oct 13  2020 nvcc.profile\n",
            "-rwxr-xr-x 1 root root 33704440 Sep 16  2020 nvdisasm\n",
            "-rwxr-xr-x 1 root root  9391256 Oct 13  2020 nvlink\n",
            "lrwxrwxrwx 1 root root        6 Oct 16  2020 nv-nsight-cu -> ncu-ui\n",
            "lrwxrwxrwx 1 root root        3 Oct 16  2020 nv-nsight-cu-cli -> ncu\n",
            "-rwxr-xr-x 1 root root  5592904 Oct 13  2020 nvprof\n",
            "-rwxr-xr-x 1 root root   101296 Sep 16  2020 nvprune\n",
            "-rwxr-xr-x 1 root root      285 Oct 13  2020 nvvp\n",
            "-rwxr-xr-x 1 root root  9234464 Oct 13  2020 ptxas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvgsLG0saJ9l",
        "outputId": "7ec87474-a8a9-40c0-9179-0da7e8d1145b"
      },
      "source": [
        "!ls -la /usr/local/cuda-11.1/samples\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 108\n",
            "drwxr-xr-x 11 root root  4096 Nov  1 13:26 .\n",
            "drwxr-xr-x  1 root root  4096 Nov  1 13:26 ..\n",
            "drwxr-xr-x 63 root root  4096 Nov  1 13:26 0_Simple\n",
            "drwxr-xr-x  8 root root  4096 Nov  1 13:26 1_Utilities\n",
            "drwxr-xr-x 14 root root  4096 Nov  1 13:26 2_Graphics\n",
            "drwxr-xr-x 24 root root  4096 Nov  1 13:26 3_Imaging\n",
            "drwxr-xr-x 10 root root  4096 Nov  1 13:26 4_Finance\n",
            "drwxr-xr-x 10 root root  4096 Nov  1 13:26 5_Simulations\n",
            "drwxr-xr-x 36 root root  4096 Nov  1 13:26 6_Advanced\n",
            "drwxr-xr-x 37 root root  4096 Nov  1 13:26 7_CUDALibraries\n",
            "drwxr-xr-x  6 root root  4096 Nov  1 13:26 common\n",
            "-rw-r--r--  1 root root 60537 Oct 13  2020 EULA.txt\n",
            "-rw-r--r--  1 root root  2606 Oct 13  2020 Makefile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv4vB-OuYP_k"
      },
      "source": [
        "Observamos la versión de CUDA instalada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOWMprbqYylZ",
        "outputId": "d321d307-7549-4ec0-b194-99298d8167dc"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-IeY9zi7uFc"
      },
      "source": [
        "Con la interfaz de configuración de NVIDIA observamos el estado de la GPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIE_XcCoZF0J",
        "outputId": "98e5e6fc-0d0a-4622-fc3b-590d8f533a0b"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Nov  7 13:51:58 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqSdqOYDZR9f"
      },
      "source": [
        "Listamos los ejemplos proporcionados para el tutorial:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8amHLuTAZUoX",
        "outputId": "b3884267-fd74-47ac-8d42-4cd81fc81cc9"
      },
      "source": [
        "!ls -la /usr/local/cuda/samples/"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 108\n",
            "drwxr-xr-x 11 root root  4096 Nov  1 13:26 .\n",
            "drwxr-xr-x  1 root root  4096 Nov  1 13:26 ..\n",
            "drwxr-xr-x 63 root root  4096 Nov  1 13:26 0_Simple\n",
            "drwxr-xr-x  8 root root  4096 Nov  1 13:26 1_Utilities\n",
            "drwxr-xr-x 14 root root  4096 Nov  1 13:26 2_Graphics\n",
            "drwxr-xr-x 24 root root  4096 Nov  1 13:26 3_Imaging\n",
            "drwxr-xr-x 10 root root  4096 Nov  1 13:26 4_Finance\n",
            "drwxr-xr-x 10 root root  4096 Nov  1 13:26 5_Simulations\n",
            "drwxr-xr-x 36 root root  4096 Nov  1 13:26 6_Advanced\n",
            "drwxr-xr-x 37 root root  4096 Nov  1 13:26 7_CUDALibraries\n",
            "drwxr-xr-x  6 root root  4096 Nov  1 13:26 common\n",
            "-rw-r--r--  1 root root 60537 Oct 13  2020 EULA.txt\n",
            "-rw-r--r--  1 root root  2606 Oct 13  2020 Makefile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iwlJSZqbxEw"
      },
      "source": [
        "Compilación de uno de los ejemplos proporcionados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECcKq7X4bwun",
        "outputId": "b916e3e3-f635-406e-c272-76866a5b76b4"
      },
      "source": [
        "%cd /usr/local/cuda/samples/1_Utilities/deviceQuery/\n",
        "%ls"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/cuda-11.1/samples/1_Utilities/deviceQuery\n",
            "deviceQuery.cpp  Makefile  NsightEclipse.xml  readme.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZpOrjkgb8Mf",
        "outputId": "5e6acd05-8ed7-4a7e-bdd2-47f9e1201621"
      },
      "source": [
        "!make"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/cuda-11.1/bin/nvcc -ccbin g++ -I../../common/inc  -m64    -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_86,code=compute_86 -o deviceQuery.o -c deviceQuery.cpp\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "/usr/local/cuda-11.1/bin/nvcc -ccbin g++   -m64      -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_86,code=compute_86 -o deviceQuery deviceQuery.o \n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "mkdir -p ../../bin/x86_64/linux/release\n",
            "cp deviceQuery ../../bin/x86_64/linux/release\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w79U1y11cDst",
        "outputId": "9aa23cc1-108c-486f-90d2-618078e02ab0"
      },
      "source": [
        "!ls -la"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 776\n",
            "drwxr-xr-x 1 root root   4096 Nov  7 13:52 .\n",
            "drwxr-xr-x 1 root root   4096 Nov  1 13:26 ..\n",
            "-rwxr-xr-x 1 root root 722704 Nov  7 13:52 deviceQuery\n",
            "-rw-r--r-- 1 root root  12720 Oct 13  2020 deviceQuery.cpp\n",
            "-rw-r--r-- 1 root root  16752 Nov  7 13:52 deviceQuery.o\n",
            "-rw-r--r-- 1 root root  12165 Oct 13  2020 Makefile\n",
            "-rw-r--r-- 1 root root   1815 Oct 13  2020 NsightEclipse.xml\n",
            "-rw-r--r-- 1 root root    168 Oct 13  2020 readme.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUg5wJQIcPUM"
      },
      "source": [
        "Corremos el ejecutable que nos ofrece información relevante acerca de CUDA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGu6lqTicOs0",
        "outputId": "a74f74d2-b74a-48d6-c41d-5b861c2f3275"
      },
      "source": [
        "!./deviceQuery"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./deviceQuery Starting...\n",
            "\n",
            " CUDA Device Query (Runtime API) version (CUDART static linking)\n",
            "\n",
            "Detected 1 CUDA Capable device(s)\n",
            "\n",
            "Device 0: \"Tesla K80\"\n",
            "  CUDA Driver Version / Runtime Version          11.2 / 11.1\n",
            "  CUDA Capability Major/Minor version number:    3.7\n",
            "  Total amount of global memory:                 11441 MBytes (11996954624 bytes)\n",
            "  (13) Multiprocessors, (192) CUDA Cores/MP:     2496 CUDA Cores\n",
            "  GPU Max Clock rate:                            824 MHz (0.82 GHz)\n",
            "  Memory Clock rate:                             2505 Mhz\n",
            "  Memory Bus Width:                              384-bit\n",
            "  L2 Cache Size:                                 1572864 bytes\n",
            "  Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)\n",
            "  Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers\n",
            "  Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers\n",
            "  Total amount of constant memory:               65536 bytes\n",
            "  Total amount of shared memory per block:       49152 bytes\n",
            "  Total shared memory per multiprocessor:        114688 bytes\n",
            "  Total number of registers available per block: 65536\n",
            "  Warp size:                                     32\n",
            "  Maximum number of threads per multiprocessor:  2048\n",
            "  Maximum number of threads per block:           1024\n",
            "  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n",
            "  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n",
            "  Maximum memory pitch:                          2147483647 bytes\n",
            "  Texture alignment:                             512 bytes\n",
            "  Concurrent copy and kernel execution:          Yes with 2 copy engine(s)\n",
            "  Run time limit on kernels:                     No\n",
            "  Integrated GPU sharing Host Memory:            No\n",
            "  Support host page-locked memory mapping:       Yes\n",
            "  Alignment requirement for Surfaces:            Yes\n",
            "  Device has ECC support:                        Enabled\n",
            "  Device supports Unified Addressing (UVA):      Yes\n",
            "  Device supports Managed Memory:                Yes\n",
            "  Device supports Compute Preemption:            No\n",
            "  Supports Cooperative Kernel Launch:            No\n",
            "  Supports MultiDevice Co-op Kernel Launch:      No\n",
            "  Device PCI Domain ID / Bus ID / location ID:   0 / 0 / 4\n",
            "  Compute Mode:\n",
            "     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n",
            "\n",
            "deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.2, CUDA Runtime Version = 11.1, NumDevs = 1\n",
            "Result = PASS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXva9zmVvp7B"
      },
      "source": [
        "Aprovechamos los resultados para responder a las preguntas que se nos ofrecen más abajo:\n",
        "* **Pruebe a lanzar diferente número de threads (con un solo 1 bloque)\n",
        "¿Cuáles son los valores máximos y mínimos de número de threads por bloque en esta GPU?**\n",
        "\n",
        "    Además de verlo en las posteriores ejecuciones, podemos ver ya que por definición el mínimo es 1 thread y como máximo se pueden tener 1024 threads por cada bloque.\n",
        "\n",
        "* **Pruebe a lanzar diferente número de bloques (con un solo thread) ¿Cuáles son los valores máximos y mínimos de número de bloques en esta GPU?**\n",
        "\n",
        "    En este caso el máximo número de bloques es 65535, siendo 1 el mínimo. Si se desea ver más información ver [especificaciones técnicas de CUDA](http://blog.cuvilib.com/2010/06/09/nvidia-cuda-difference-between-fermi-and-previous-architectures/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzeXoGFjdFpb"
      },
      "source": [
        "Compilar en CUDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tiuxb8ZcMg1",
        "outputId": "67b01811-1ffb-46c7-d484-77bf1f931cec"
      },
      "source": [
        "%cd /content/"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmIeBtHB3kKf",
        "outputId": "b36e98f3-a6a8-476d-c56d-f6ab15a8d8de"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq2Zkdb3dRbg"
      },
      "source": [
        "Hacer un directorio de trabajo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFIrKoxRdVKU"
      },
      "source": [
        "!mkdir workcuda"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJnzSXYDdah0",
        "outputId": "4afa9942-c1a6-4814-e833-6145b308b956"
      },
      "source": [
        "cd workcuda/\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/workcuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fFsAUW9dgVJ",
        "outputId": "876f6147-550a-40c8-8b67-7569d2b02f20"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/workcuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH1Xtcend3Bf"
      },
      "source": [
        "### Ejercicio 1: suma de vectores\n",
        "A continuación se discuten diferentes versiones de un programa que suma los elementos de un vector en CUDA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89b1I4R29vaA"
      },
      "source": [
        "#### Ejercicio base en CUDA: 1 bloque/1 thread\n",
        "Nota: este ejercicio aportado no aprovecha los recursos de la GPU, es decir, se ejecuta en GPU pero en serie."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwAyqNfAd95Z",
        "outputId": "31ab8fd8-5113-47d4-d627-fad3908a4f31"
      },
      "source": [
        "%%writefile suma1d.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "__global__ void add(int *a, int *b, int *c) {\n",
        "*c = *a + *b;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "int a, b, c;\n",
        "\n",
        "// host copies of variables a, b & c\n",
        "int *d_a, *d_b, *d_c;\n",
        "// device copies of variables a, b & c\n",
        "int size = sizeof(int);\n",
        "\n",
        "// Allocate space for device copies of a, b, c\n",
        "cudaMalloc((void **)&d_a, size);\n",
        "cudaMalloc((void **)&d_b, size);\n",
        "cudaMalloc((void **)&d_c, size);\n",
        "\n",
        "// Setup input values  \n",
        "c = 0;\n",
        "a = 3;\n",
        "b = 5;\n",
        "\n",
        "// Copy inputs to device\n",
        "cudaMemcpy(d_a, &a, size, cudaMemcpyHostToDevice);\n",
        "cudaMemcpy(d_b, &b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "// Launch add() kernel on GPU\n",
        "add<<<1,1>>>(d_a, d_b, d_c);\n",
        "\n",
        "// Copy result back to host\n",
        "cudaError err = cudaMemcpy(&c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "  if(err!=cudaSuccess) {\n",
        "      printf(\"CUDA error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "  }\n",
        "printf(\"result is %d\\n\",c);\n",
        "\n",
        "// Cleanup\n",
        "cudaFree(d_a);\n",
        "cudaFree(d_b);\n",
        "cudaFree(d_c);\n",
        "\n",
        "return 0;\n",
        "}"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing suma1d.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e5lNkDHfZGl"
      },
      "source": [
        "Compilamos y ejecutamos el ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-Wx8K-ffJOS",
        "outputId": "d1f3ff72-5fcd-4f2d-eb1f-e4ffe6c6fb0f"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true suma1d.cu -o suma1d -lcudadevrt\n",
        "!./suma1d"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "result is 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BjD-FD6fjHM"
      },
      "source": [
        "Comprobamos el perfil de ejecución"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq6V0drHBzsH"
      },
      "source": [
        "Vemos que el resultado es correcto, pero estamos haciendo un mal uso de la GPU al no estar haciendo ninguna división. Por consiguiente esto puede afectar al tiempo de ejecución, que se puede ver fácilmente con el $\\texttt{profiler}$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpSV8mdMfomL",
        "outputId": "092a95c0-8647-4d74-9b97-f0cb1a78777c"
      },
      "source": [
        "!nvprof ./suma1d"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==351== NVPROF is profiling process 351, command: ./suma1d\n",
            "==351== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "result is 8\n",
            "==351== Profiling application: ./suma1d\n",
            "==351== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   38.56%  3.9360us         2  1.9680us  1.5680us  2.3680us  [CUDA memcpy HtoD]\n",
            "                   36.36%  3.7120us         1  3.7120us  3.7120us  3.7120us  add(int*, int*, int*)\n",
            "                   25.08%  2.5600us         1  2.5600us  2.5600us  2.5600us  [CUDA memcpy DtoH]\n",
            "      API calls:   99.59%  259.42ms         3  86.473ms  2.7710us  259.41ms  cudaMalloc\n",
            "                    0.21%  550.84us         1  550.84us  550.84us  550.84us  cuDeviceTotalMem\n",
            "                    0.09%  222.48us       101  2.2020us     152ns  78.051us  cuDeviceGetAttribute\n",
            "                    0.06%  150.04us         3  50.014us  6.0570us  128.40us  cudaFree\n",
            "                    0.02%  62.940us         3  20.980us  12.622us  31.024us  cudaMemcpy\n",
            "                    0.02%  40.660us         1  40.660us  40.660us  40.660us  cuDeviceGetName\n",
            "                    0.01%  25.738us         1  25.738us  25.738us  25.738us  cudaLaunchKernel\n",
            "                    0.00%  9.5190us         1  9.5190us  9.5190us  9.5190us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.5360us         3     845ns     229ns  1.2810us  cuDeviceGetCount\n",
            "                    0.00%  1.6360us         2     818ns     389ns  1.2470us  cuDeviceGet\n",
            "                    0.00%     302ns         1     302ns     302ns     302ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FqMcRGNHCqu"
      },
      "source": [
        "#### Ejercicio N bloques/1 thread\n",
        "Probamos haciendo paralelismo de 512 bloques de 1 thread."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDULMwPfgxuU",
        "outputId": "6b6fbe00-e885-4773-8de2-2402e9e9ae98"
      },
      "source": [
        "%%writefile suma2dvector.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "__global__ void add(int *a, int *b, int *c) {\n",
        "//*c = *a + *b;\n",
        "c[blockIdx.x] = a[blockIdx.x] + b[blockIdx.x]; \n",
        "}\n",
        "\n",
        "#define N 512\n",
        "\n",
        "int main(void) {\n",
        "int *a, *b, *c;        // host copies of variables a, b & c\n",
        "int *d_a, *d_b, *d_c;  // device copies of variables a, b & c\n",
        "int size = N * sizeof(int);\n",
        "\n",
        "// Allocate space for host copies of a, b, c   Setup input values  \n",
        "a =  (int *) malloc(size); \n",
        "b =  (int *) malloc(size); \n",
        "c =  (int *) malloc(size); \n",
        "\n",
        "\n",
        "// Setup input values  \n",
        "\n",
        "for( int i = 0; i < N; i++ )\n",
        "\t{\n",
        "\t\ta[i] = i;\n",
        "    b[i] = N-i;\n",
        "\t\tc[i] = 0;\n",
        "\t}\n",
        "\n",
        "\n",
        "// Allocate space for device copies of a, b, c\n",
        "cudaMalloc((void **)&d_a, size);\n",
        "cudaMalloc((void **)&d_b, size);\n",
        "cudaMalloc((void **)&d_c, size);\n",
        "\n",
        "\n",
        "// Copy inputs to device\n",
        "cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n",
        "cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "// Launch add() kernel on GPU  Se lanzan N bloques de 1 Thread.\n",
        "add<<<N,1>>>(d_a, d_b, d_c);\n",
        "\n",
        "// Copy result back to host\n",
        "cudaError err = cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "  if(err!=cudaSuccess) {\n",
        "      printf(\"CUDA error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "  }\n",
        "printf(\" valor a[0] es %d\\n\",a[0]);\n",
        "printf(\" valor b[0] es %d\\n\",b[0]);\n",
        "printf(\"resultado c[0] es %d\\n\",c[0]);\n",
        "printf(\" valor a[2] es %d\\n\",a[2]);\n",
        "printf(\" valor b[2] es %d\\n\",b[2]);\n",
        "printf(\"resultado c[2] es %d\\n\",c[2]);\n",
        "printf(\" valor a[15] es %d\\n\",a[15]);\n",
        "printf(\" valor b[15] es %d\\n\",b[15]);\n",
        "printf(\"resultado c[15] es %d\\n\",c[15]);\n",
        "\n",
        "// Cleanup\n",
        "\n",
        "free(a); free(b);free(c); \n",
        "\n",
        "cudaFree(d_a);\n",
        "cudaFree(d_b);\n",
        "cudaFree(d_c);\n",
        "\n",
        "return 0;\n",
        "}"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing suma2dvector.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEcMF9ZODP3r"
      },
      "source": [
        "Compilamos y ejecutamos y vemos que el resultado es correcto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-w1xk4RFDZ9t",
        "outputId": "976210bd-a1ca-4a76-c696-f4bead8276ea"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true suma2dvector.cu -o suma2dvector -lcudadevrt\n",
        "!./suma2dvector"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            " valor a[0] es 0\n",
            " valor b[0] es 512\n",
            "resultado c[0] es 512\n",
            " valor a[2] es 2\n",
            " valor b[2] es 510\n",
            "resultado c[2] es 512\n",
            " valor a[15] es 15\n",
            " valor b[15] es 497\n",
            "resultado c[15] es 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lUTdjJuFTnR"
      },
      "source": [
        "Comprobamos el perfil de ejecución y vemos que tarda más que en el caso anterior. Si bien al haber solo un thread por bloque, quiere decir que solo se ejecuta un warp por bloque y no maximiza el aprovechamiento de la GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPBo6tWUFWyb",
        "outputId": "a75b80ef-5258-4924-d1ef-19aca3c3c2b5"
      },
      "source": [
        "!nvprof ./suma2dvector"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==398== NVPROF is profiling process 398, command: ./suma2dvector\n",
            "==398== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            " valor a[0] es 0\n",
            " valor b[0] es 512\n",
            "resultado c[0] es 512\n",
            " valor a[2] es 2\n",
            " valor b[2] es 510\n",
            "resultado c[2] es 512\n",
            " valor a[15] es 15\n",
            " valor b[15] es 497\n",
            "resultado c[15] es 512\n",
            "==398== Profiling application: ./suma2dvector\n",
            "==398== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   41.19%  5.0880us         1  5.0880us  5.0880us  5.0880us  add(int*, int*, int*)\n",
            "                   37.31%  4.6080us         2  2.3040us  1.9840us  2.6240us  [CUDA memcpy HtoD]\n",
            "                   21.50%  2.6560us         1  2.6560us  2.6560us  2.6560us  [CUDA memcpy DtoH]\n",
            "      API calls:   99.55%  201.47ms         3  67.156ms  2.5850us  201.46ms  cudaMalloc\n",
            "                    0.22%  450.62us         1  450.62us  450.62us  450.62us  cuDeviceTotalMem\n",
            "                    0.08%  163.84us       101  1.6220us     145ns  67.927us  cuDeviceGetAttribute\n",
            "                    0.07%  144.92us         3  48.307us  4.3430us  125.53us  cudaFree\n",
            "                    0.03%  66.646us         3  22.215us  13.612us  30.114us  cudaMemcpy\n",
            "                    0.02%  36.182us         1  36.182us  36.182us  36.182us  cudaLaunchKernel\n",
            "                    0.02%  31.116us         1  31.116us  31.116us  31.116us  cuDeviceGetName\n",
            "                    0.00%  7.8750us         1  7.8750us  7.8750us  7.8750us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.2540us         3     751ns     222ns  1.0640us  cuDeviceGetCount\n",
            "                    0.00%  1.6930us         2     846ns     489ns  1.2040us  cuDeviceGet\n",
            "                    0.00%     298ns         1     298ns     298ns     298ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0FbrAaUHUQI"
      },
      "source": [
        "#### Ejercicio N bloques/1 thread\n",
        "Probamos haciendo paralelismo de 1 bloque de 1024 threads.\n",
        "\n",
        "Nota: en el tutorial se determinaba N como 1028, sin embargo esto supera el máximo número de threads permitidos por bloque, por lo que el resultado es incorrecto, con el fin de comparar los resultados con diferentes opciones, determinamos N como 1024 threads para este caso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7ujd3TwHcE0",
        "outputId": "04c73fd2-8958-4eec-db10-449f7da85184"
      },
      "source": [
        "%%writefile suma2dvectorNthreads.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "__global__ void add(int *a, int *b, int *c) {\n",
        "c[threadIdx.x] = a[threadIdx.x] + b[threadIdx.x]; \n",
        "}\n",
        "\n",
        "#define N 1024\n",
        "\n",
        "int main(void) {\n",
        "int *a, *b, *c;        // host copies of variables a, b & c\n",
        "int *d_a, *d_b, *d_c;  // device copies of variables a, b & c\n",
        "int size = N * sizeof(int);\n",
        "\n",
        "// Allocate space for host copies of a, b, c   Setup input values  \n",
        "a =  (int *) malloc(size); \n",
        "b =  (int *) malloc(size); \n",
        "c =  (int *) malloc(size); \n",
        "\n",
        "\n",
        "// Setup input values  \n",
        "\n",
        "for( int i = 0; i < N; i++ )\n",
        "\t{\n",
        "\t\ta[i] = i;\n",
        "    b[i] = N-i;\n",
        "\t\tc[i] = 0;\n",
        "\t}\n",
        "\n",
        "\n",
        "// Allocate space for device copies of a, b, c\n",
        "cudaMalloc((void **)&d_a, size);\n",
        "cudaMalloc((void **)&d_b, size);\n",
        "cudaMalloc((void **)&d_c, size);\n",
        "\n",
        "\n",
        "// Copy inputs to device\n",
        "cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n",
        "cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "// Launch add() kernel on GPU  Se lanzan 1 bloques de N Threads.\n",
        "add<<<1,N>>>(d_a, d_b, d_c);\n",
        "\n",
        "// Copy result back to host\n",
        "cudaError err = cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "  if(err!=cudaSuccess) {\n",
        "      printf(\"CUDA error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "  }\n",
        "printf(\" valor a[10] es %d\\n\",a[10]);\n",
        "printf(\" valor b[10] es %d\\n\",b[10]);\n",
        "printf(\"resultado c[10] es %d\\n\",c[10]);\n",
        "printf(\" valor a[0] es %d\\n\",a[0]);\n",
        "printf(\" valor b[0] es %d\\n\",b[0]);\n",
        "printf(\"resultado c[0] es %d\\n\",c[0]);\n",
        "printf(\" valor a[15] es %d\\n\",a[15]);\n",
        "printf(\" valor b[15] es %d\\n\",b[15]);\n",
        "printf(\"resultado c[15] es %d\\n\",c[15]);\n",
        "\n",
        "// Cleanup\n",
        "\n",
        "free(a); free(b);free(c); \n",
        "\n",
        "cudaFree(d_a);\n",
        "cudaFree(d_b);\n",
        "cudaFree(d_c);\n",
        "\n",
        "return 0;\n",
        "}"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing suma2dvectorNthreads.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E0HiMy6IU1L"
      },
      "source": [
        "Ejecutamos este ejemplo y vemos que nuevamente el resultado es correcto.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW4YFVPkIYHr",
        "outputId": "8206f78d-5bba-408e-9f79-4f0f3dfc9905"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true suma2dvectorNthreads.cu -o suma2dvectorNthreads -lcudadevrt\n",
        "!./suma2dvectorNthreads"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            " valor a[10] es 10\n",
            " valor b[10] es 1014\n",
            "resultado c[10] es 1024\n",
            " valor a[0] es 0\n",
            " valor b[0] es 1024\n",
            "resultado c[0] es 1024\n",
            " valor a[15] es 15\n",
            " valor b[15] es 1009\n",
            "resultado c[15] es 1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2uXYiF_sDAu"
      },
      "source": [
        "En este caso vemos que la suma tarda bastante y tampoco se aprovechan los recursos de la GPU de manera eficiente. Esto es porque estamos realizando 1024 operaciones que acceden a memoria global en un único bloque."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhGdz2EMw5W3",
        "outputId": "ced55768-ddbd-46ff-89a9-69b9fe35ce07"
      },
      "source": [
        "!nvprof ./suma2dvectorNthreads"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==445== NVPROF is profiling process 445, command: ./suma2dvectorNthreads\n",
            "==445== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            " valor a[10] es 10\n",
            " valor b[10] es 1014\n",
            "resultado c[10] es 1024\n",
            " valor a[0] es 0\n",
            " valor b[0] es 1024\n",
            "resultado c[0] es 1024\n",
            " valor a[15] es 15\n",
            " valor b[15] es 1009\n",
            "resultado c[15] es 1024\n",
            "==445== Profiling application: ./suma2dvectorNthreads\n",
            "==445== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   45.48%  5.3120us         2  2.6560us  2.4960us  2.8160us  [CUDA memcpy HtoD]\n",
            "                   29.59%  3.4560us         1  3.4560us  3.4560us  3.4560us  add(int*, int*, int*)\n",
            "                   24.93%  2.9120us         1  2.9120us  2.9120us  2.9120us  [CUDA memcpy DtoH]\n",
            "      API calls:   99.49%  181.44ms         3  60.481ms  2.7960us  181.44ms  cudaMalloc\n",
            "                    0.25%  460.50us         1  460.50us  460.50us  460.50us  cuDeviceTotalMem\n",
            "                    0.10%  174.09us         3  58.031us  5.5570us  153.61us  cudaFree\n",
            "                    0.09%  173.03us       101  1.7130us     145ns  70.368us  cuDeviceGetAttribute\n",
            "                    0.04%  66.011us         3  22.003us  14.488us  27.966us  cudaMemcpy\n",
            "                    0.01%  26.933us         1  26.933us  26.933us  26.933us  cudaLaunchKernel\n",
            "                    0.01%  23.622us         1  23.622us  23.622us  23.622us  cuDeviceGetName\n",
            "                    0.00%  6.9050us         1  6.9050us  6.9050us  6.9050us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.0210us         3     673ns     195ns  1.0740us  cuDeviceGetCount\n",
            "                    0.00%  1.4380us         2     719ns     263ns  1.1750us  cuDeviceGet\n",
            "                    0.00%     294ns         1     294ns     294ns     294ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX_QYtIPxjBb"
      },
      "source": [
        "#### Ejercicio N bloques/N threads\n",
        "A continuación se aprovechan los recursos que nos ofrece CUDA combinando bloques y threads para procesar el programa de forma paralela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Spb5jhxKyGz1",
        "outputId": "06895f54-a011-48da-92dd-d4324f0136d4"
      },
      "source": [
        "%%writefile suma2dvectorNBloqxNthreads.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "__global__ void add(int *a, int *b, int *c) {\n",
        "int index = threadIdx.x + blockIdx.x * blockDim.x;\n",
        " c[index] = a[index] + b[index]; \n",
        "}\n",
        "\n",
        "#define N (1024*1024)\n",
        "#define THREADS_PER_BLOCK 512 \n",
        "\n",
        "int main(void) {\n",
        "int *a, *b, *c;        // host copies of variables a, b & c\n",
        "int *d_a, *d_b, *d_c;  // device copies of variables a, b & c\n",
        "int size = N*sizeof(int);\n",
        "\n",
        "// Allocate space for host copies of a, b, c   Setup input values  \n",
        "a =  (int *) malloc(size); \n",
        "b =  (int *) malloc(size); \n",
        "c =  (int *) malloc(size); \n",
        "\n",
        "\n",
        "// Setup input values  \n",
        "\n",
        "for( int i = 0; i < N; i++ )\n",
        "\t{\n",
        "\t\ta[i] = i;\n",
        "    b[i] = N-i;\n",
        "\t\tc[i] = 0;\n",
        "\t}\n",
        "\n",
        "\n",
        "// Allocate space for device copies of a, b, c\n",
        "cudaMalloc((void **)&d_a, size);\n",
        "cudaMalloc((void **)&d_b, size);\n",
        "cudaMalloc((void **)&d_c, size);\n",
        "\n",
        "\n",
        "// Copy inputs to device\n",
        "cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n",
        "cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "// Launch add() kernel on GPU  Se lanzan NuBloq=N/THREADS_PER_BLOCK bloques de \n",
        "// THREADS_PER_BLOCK Threads.\n",
        "add<<<N/THREADS_PER_BLOCK,THREADS_PER_BLOCK>>>(d_a, d_b, d_c);\n",
        "\n",
        "// Copy result back to host\n",
        "cudaError err = cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "  if(err!=cudaSuccess) {\n",
        "      printf(\"CUDA error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "  }\n",
        "printf(\" valor a[10] es %d\\n\",a[10]);\n",
        "printf(\" valor b[10] es %d\\n\",b[10]);\n",
        "printf(\"resultado c[10] es %d\\n\",c[10]);\n",
        "printf(\" valor a[0] es %d\\n\",a[0]);\n",
        "printf(\" valor b[0] es %d\\n\",b[0]);\n",
        "printf(\"resultado c[0] es %d\\n\",c[0]);\n",
        "printf(\" valor a[15] es %d\\n\",a[15]);\n",
        "printf(\" valor b[15] es %d\\n\",b[15]);\n",
        "printf(\"resultado c[15] es %d\\n\",c[15]);\n",
        "\n",
        "// Cleanup\n",
        "\n",
        "free(a); free(b);free(c); \n",
        "\n",
        "cudaFree(d_a);\n",
        "cudaFree(d_b);\n",
        "cudaFree(d_c);\n",
        "\n",
        "return 0;\n",
        "}"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing suma2dvectorNBloqxNthreads.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTeZiHnfxi1E"
      },
      "source": [
        "Comprobamos que el resultado es el mismo y es correcto:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxIvXWKCzBxw",
        "outputId": "a8fb18cb-c7bd-44d0-ead2-34dbb3d60b9c"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true suma2dvectorNBloqxNthreads.cu -o suma2dvectorNBloqxNthreads -lcudadevrt\n",
        "!./suma2dvectorNBloqxNthreads"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            " valor a[10] es 10\n",
            " valor b[10] es 1048566\n",
            "resultado c[10] es 1048576\n",
            " valor a[0] es 0\n",
            " valor b[0] es 1048576\n",
            "resultado c[0] es 1048576\n",
            " valor a[15] es 15\n",
            " valor b[15] es 1048561\n",
            "resultado c[15] es 1048576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXqdKI-X2Tcr"
      },
      "source": [
        "Comprobamos con el perfil de ejecución que se realiza el mejor uso de la GPU hasta ahora."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H08bppmr2UKm",
        "outputId": "0698c7bc-be68-48d0-8e1a-21461d779851"
      },
      "source": [
        "!nvprof ./suma2dvectorNBloqxNthreads"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==494== NVPROF is profiling process 494, command: ./suma2dvectorNBloqxNthreads\n",
            "==494== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            " valor a[10] es 10\n",
            " valor b[10] es 1048566\n",
            "resultado c[10] es 1048576\n",
            " valor a[0] es 0\n",
            " valor b[0] es 1048576\n",
            "resultado c[0] es 1048576\n",
            " valor a[15] es 15\n",
            " valor b[15] es 1048561\n",
            "resultado c[15] es 1048576\n",
            "==494== Profiling application: ./suma2dvectorNBloqxNthreads\n",
            "==494== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   62.56%  1.0927ms         2  546.35us  543.90us  548.80us  [CUDA memcpy HtoD]\n",
            "                   31.37%  547.97us         1  547.97us  547.97us  547.97us  [CUDA memcpy DtoH]\n",
            "                    6.06%  105.86us         1  105.86us  105.86us  105.86us  add(int*, int*, int*)\n",
            "      API calls:   97.80%  194.36ms         3  64.786ms  116.97us  194.12ms  cudaMalloc\n",
            "                    1.11%  2.2100ms         3  736.68us  612.10us  915.57us  cudaMemcpy\n",
            "                    0.72%  1.4310ms         3  477.00us  249.43us  592.37us  cudaFree\n",
            "                    0.25%  501.16us         1  501.16us  501.16us  501.16us  cuDeviceTotalMem\n",
            "                    0.09%  174.91us       101  1.7310us     143ns  69.759us  cuDeviceGetAttribute\n",
            "                    0.01%  28.014us         1  28.014us  28.014us  28.014us  cudaLaunchKernel\n",
            "                    0.01%  25.142us         1  25.142us  25.142us  25.142us  cuDeviceGetName\n",
            "                    0.00%  5.6510us         1  5.6510us  5.6510us  5.6510us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.0740us         3     691ns     297ns  1.1750us  cuDeviceGetCount\n",
            "                    0.00%  1.5960us         2     798ns     286ns  1.3100us  cuDeviceGet\n",
            "                    0.00%     288ns         1     288ns     288ns     288ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Plee2mF3HSI"
      },
      "source": [
        "**Pregunta ¿Qué sucede si incrementa o disminuye el valor de N?**\n",
        "Para responder a esta pregunta, probamos a ejecutar con diferentes tamaños de N para threads."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rrQ_VSzyyWe",
        "outputId": "ccdaeb6c-d5b3-4e4f-d2a6-ce371ed4b80a"
      },
      "source": [
        "!sed -i '/#define N/c\\#define N (1024*1024)' suma2dvectorNBloqxNthreads.cu\n",
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true suma2dvectorNBloqxNthreads.cu -o suma2dvectorNBloqxNthreads -lcudadevrt\n",
        "!nvprof ./suma2dvectorNBloqxNthreads"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==539== NVPROF is profiling process 539, command: ./suma2dvectorNBloqxNthreads\n",
            "==539== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            " valor a[10] es 10\n",
            " valor b[10] es 1048566\n",
            "resultado c[10] es 1048576\n",
            " valor a[0] es 0\n",
            " valor b[0] es 1048576\n",
            "resultado c[0] es 1048576\n",
            " valor a[15] es 15\n",
            " valor b[15] es 1048561\n",
            "resultado c[15] es 1048576\n",
            "==539== Profiling application: ./suma2dvectorNBloqxNthreads\n",
            "==539== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   63.60%  1.1262ms         2  563.10us  547.29us  578.91us  [CUDA memcpy HtoD]\n",
            "                   30.49%  539.84us         1  539.84us  539.84us  539.84us  [CUDA memcpy DtoH]\n",
            "                    5.91%  104.61us         1  104.61us  104.61us  104.61us  add(int*, int*, int*)\n",
            "      API calls:   97.85%  196.54ms         3  65.512ms  134.58us  196.26ms  cudaMalloc\n",
            "                    1.09%  2.1818ms         3  727.28us  605.78us  898.92us  cudaMemcpy\n",
            "                    0.67%  1.3436ms         3  447.86us  139.49us  627.39us  cudaFree\n",
            "                    0.25%  495.09us         1  495.09us  495.09us  495.09us  cuDeviceTotalMem\n",
            "                    0.10%  209.02us       101  2.0690us     153ns  110.93us  cuDeviceGetAttribute\n",
            "                    0.02%  37.142us         1  37.142us  37.142us  37.142us  cudaLaunchKernel\n",
            "                    0.01%  29.941us         1  29.941us  29.941us  29.941us  cuDeviceGetName\n",
            "                    0.00%  7.4920us         1  7.4920us  7.4920us  7.4920us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.3610us         3     787ns     183ns  1.1600us  cuDeviceGetCount\n",
            "                    0.00%  1.9100us         2     955ns     513ns  1.3970us  cuDeviceGet\n",
            "                    0.00%     283ns         1     283ns     283ns     283ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUIhc1ZLzN_F"
      },
      "source": [
        "En este caso hemos probado para un número menor de threads y vemos que el rendimiento es menor. Si lo incrementamos vemos que el resultado de la aplicación sigue siendo correcto pero porque ocupa el máximo número de threads permitidos, si bien el tamaño produce errores de memoria."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgS0a4mYzY8w",
        "outputId": "af5a56d4-6868-419f-a576-5a35b206154b"
      },
      "source": [
        "!sed -i '/#define N/c\\#define N (1024*1024*500)' suma2dvectorNBloqxNthreads.cu\n",
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true suma2dvectorNBloqxNthreads.cu -o suma2dvectorNBloqxNthreads -lcudadevrt\n",
        "!nvprof ./suma2dvectorNBloqxNthreads"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "tcmalloc: large alloc 2097152000 bytes == 0x563ef8f86000 @  0x7f1dc8ad71e7 0x563ef8254272 0x7f1dc7b08bf7 0x563ef8253fda\n",
            "tcmalloc: large alloc 2097152000 bytes == 0x563f75f86000 @  0x7f1dc8ad71e7 0x563ef8254283 0x7f1dc7b08bf7 0x563ef8253fda\n",
            "tcmalloc: large alloc 2097152000 bytes == 0x563ff2f86000 @  0x7f1dc8ad71e7 0x563ef8254294 0x7f1dc7b08bf7 0x563ef8253fda\n",
            "==584== NVPROF is profiling process 584, command: ./suma2dvectorNBloqxNthreads\n",
            "==584== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            " valor a[10] es 10\n",
            " valor b[10] es 524287990\n",
            "resultado c[10] es 524288000\n",
            " valor a[0] es 0\n",
            " valor b[0] es 524288000\n",
            "resultado c[0] es 524288000\n",
            " valor a[15] es 15\n",
            " valor b[15] es 524287985\n",
            "resultado c[15] es 524288000\n",
            "==584== Profiling application: ./suma2dvectorNBloqxNthreads\n",
            "==584== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   65.25%  568.40ms         2  284.20ms  276.02ms  292.37ms  [CUDA memcpy HtoD]\n",
            "                   29.90%  260.47ms         1  260.47ms  260.47ms  260.47ms  [CUDA memcpy DtoH]\n",
            "                    4.84%  42.188ms         1  42.188ms  42.188ms  42.188ms  add(int*, int*, int*)\n",
            "      API calls:   60.79%  871.53ms         3  290.51ms  276.19ms  302.88ms  cudaMemcpy\n",
            "                   24.53%  351.66ms         3  117.22ms  2.1975ms  177.89ms  cudaFree\n",
            "                   14.63%  209.69ms         3  69.896ms  2.7958ms  203.80ms  cudaMalloc\n",
            "                    0.04%  543.47us         1  543.47us  543.47us  543.47us  cuDeviceTotalMem\n",
            "                    0.02%  245.55us       101  2.4310us     145ns  114.94us  cuDeviceGetAttribute\n",
            "                    0.00%  44.677us         1  44.677us  44.677us  44.677us  cudaLaunchKernel\n",
            "                    0.00%  44.484us         1  44.484us  44.484us  44.484us  cuDeviceGetName\n",
            "                    0.00%  5.0450us         1  5.0450us  5.0450us  5.0450us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.1330us         3     711ns     270ns  1.0750us  cuDeviceGetCount\n",
            "                    0.00%  1.5640us         2     782ns     288ns  1.2760us  cuDeviceGet\n",
            "                    0.00%     305ns         1     305ns     305ns     305ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUWn0-_Kvfrj"
      },
      "source": [
        "## Parte 2: Ejercicios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhpaNyib0t-b"
      },
      "source": [
        "### Ejercicio: suma de matrices cuadradas de tamaño N\n",
        "En este caso se realiza la suma de matrices para cualquier *N*, sin tener que definir números fijos y precalcular el número de bloques dado un número de threads. Para ello se ha definido el número de threads por bloque *MAX* como $\\frac{N + MAX - 1}{MAX}$. Asimismo, fijando $MAX=8$ se obtiene un tamaño de grid de $8*8 = 64$, lo que en otras palabras significa que podemos llegar al máximo número de threads sin violar ninguna restricción de CUDA. Además, se ha debido cambiar el *kernel* de la función *add* a una estructura bidimensional para que realice la suma $a_{i,j}+b_{i,j}$ y se guarde el resultado en $c_{i,j}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIb86yWWqN8T",
        "outputId": "49d7752f-0d90-4a58-8b70-4fc4c5e47549"
      },
      "source": [
        "%%writefile sumamatrix_NxN.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define N 1024\n",
        "#define MAX 8\n",
        "\n",
        "__global__ void add(float *a, float *b, float *c, int n) {\n",
        "  int j = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "  int i = threadIdx.y + blockIdx.y * blockDim.y;\n",
        "  int index = i*n + j;\n",
        "  \n",
        "  if (i < n && j < n)\n",
        "    c[index] = a[index] + b[index];\n",
        "}\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "  float *a, *b, *c;\n",
        "  float *d_a, *d_b, *d_c;\n",
        "  int size = N*N*sizeof(float);\n",
        "\n",
        "  a = (float *) malloc(size);\n",
        "  b = (float *) malloc(size);\n",
        "  c = (float *) malloc(size);\n",
        "\n",
        "  // Inicializamos los datos\n",
        "  for (int i = 0; i < N; i++ ){\n",
        "      for (int j = 0; j < N; j++){\n",
        "          a[i*N + j] = 1.0f;\n",
        "          b[i*N + j] = 2.0f;\n",
        "      }\n",
        "  }\n",
        "\n",
        "  cudaMalloc((void **)&d_a, size);\n",
        "  cudaMalloc((void **)&d_b, size);\n",
        "  cudaMalloc((void **)&d_c, size);\n",
        "\n",
        "  cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "  // grid size\n",
        "  dim3 dimGrid((N + MAX - 1)/MAX , (N + MAX - 1)/MAX, 1);\n",
        "  \n",
        "  // block size\n",
        "  dim3 dimBlock(MAX, MAX, 1);\n",
        "\n",
        "  // Launch add() kernel on GPU\n",
        "\n",
        "  add<<<dimGrid, dimBlock>>>(d_a, d_b, d_c, N);\n",
        "\n",
        "    // Copy result back to host\n",
        "  cudaError err = cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "    if(err!=cudaSuccess) {\n",
        "        printf(\"CUDA error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "    }\n",
        "  printf(\" valor a[10] es %f\\n\",a[10]);\n",
        "  printf(\" valor b[10] es %f\\n\",b[10]);\n",
        "  printf(\"resultado c[10] es %f\\n\",c[10]);\n",
        "  printf(\" valor a[0] es %f\\n\",a[0]);\n",
        "  printf(\" valor b[0] es %f\\n\",b[0]);\n",
        "  printf(\"resultado c[0] es %f\\n\",c[0]);\n",
        "  printf(\" valor a[15] es %f\\n\",a[0]);\n",
        "  printf(\" valor b[15] es %f\\n\",b[0]);\n",
        "  printf(\"resultado c[15] es %f\\n\",c[0]);\n",
        "\n",
        "  // Cleanup\n",
        "\n",
        "  free(a); free(b); free(c); \n",
        "\n",
        "  cudaFree(d_a);\n",
        "  cudaFree(d_b);\n",
        "  cudaFree(d_c);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing sumamatrix_NxN.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow545eeb41Ke"
      },
      "source": [
        "Vemos que la ejecución es correcta:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5IXknL8u5m6",
        "outputId": "13b9340a-0b87-43d9-be8a-aff34e6e9eb0"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true sumamatrix_NxN.cu -o sumamatrix_NxN -lcudadevrt\n",
        "!./sumamatrix_NxN"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            " valor a[10] es 1.000000\n",
            " valor b[10] es 2.000000\n",
            "resultado c[10] es 3.000000\n",
            " valor a[0] es 1.000000\n",
            " valor b[0] es 2.000000\n",
            "resultado c[0] es 3.000000\n",
            " valor a[15] es 1.000000\n",
            " valor b[15] es 2.000000\n",
            "resultado c[15] es 3.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYFb5Wxt45RY"
      },
      "source": [
        "Además vemos que se ejecuta en un tiempo razonable (1.7 ms)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j38xyYx5qPar",
        "outputId": "46fb4ea3-0013-4660-9795-8ad3258014f9"
      },
      "source": [
        "!nvprof ./sumamatrix_NxN"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==634== NVPROF is profiling process 634, command: ./sumamatrix_NxN\n",
            "==634== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            " valor a[10] es 1.000000\n",
            " valor b[10] es 2.000000\n",
            "resultado c[10] es 3.000000\n",
            " valor a[0] es 1.000000\n",
            " valor b[0] es 2.000000\n",
            "resultado c[0] es 3.000000\n",
            " valor a[15] es 1.000000\n",
            " valor b[15] es 2.000000\n",
            "resultado c[15] es 3.000000\n",
            "==634== Profiling application: ./sumamatrix_NxN\n",
            "==634== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   56.47%  1.4002ms         1  1.4002ms  1.4002ms  1.4002ms  [CUDA memcpy DtoH]\n",
            "                   39.05%  968.32us         2  484.16us  451.36us  516.96us  [CUDA memcpy HtoD]\n",
            "                    4.48%  110.98us         1  110.98us  110.98us  110.98us  add(float*, float*, float*, int)\n",
            "      API calls:   97.06%  193.37ms         3  64.455ms  131.88us  193.08ms  cudaMalloc\n",
            "                    1.97%  3.9274ms         3  1.3091ms  506.09us  2.7748ms  cudaMemcpy\n",
            "                    0.47%  942.70us         3  314.23us  158.01us  393.22us  cudaFree\n",
            "                    0.32%  644.16us         1  644.16us  644.16us  644.16us  cuDeviceTotalMem\n",
            "                    0.13%  260.20us       101  2.5760us     141ns  102.19us  cuDeviceGetAttribute\n",
            "                    0.02%  34.873us         1  34.873us  34.873us  34.873us  cudaLaunchKernel\n",
            "                    0.02%  30.491us         1  30.491us  30.491us  30.491us  cuDeviceGetName\n",
            "                    0.00%  5.1820us         1  5.1820us  5.1820us  5.1820us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.8040us         3     601ns     220ns     928ns  cuDeviceGetCount\n",
            "                    0.00%  1.4440us         2     722ns     338ns  1.1060us  cuDeviceGet\n",
            "                    0.00%     294ns         1     294ns     294ns     294ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZJjBuKx3VYo"
      },
      "source": [
        "### Ejercicio: ejecución de Stencil1D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uKzTtEeISQw"
      },
      "source": [
        "En este ejercicio vamos a estudiar el efecto de la memoria compartida en una GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13R0IYfr599H"
      },
      "source": [
        "\n",
        "#### Sin memoria compartida\n",
        "Primero realizamos una versión sin memoria compartida.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85-QjdtR5sDK",
        "outputId": "f6680988-35d1-48cd-859c-e17821c64702"
      },
      "source": [
        "%%writefile stencil1d_base.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "#define RADIUS        3\n",
        "#define BLOCK_SIZE    256\n",
        "#define NUM_ELEMENTS  (4096*2)\n",
        "\n",
        "// CUDA API error checking macro\n",
        "#define cudaCheck(error) \\\n",
        "  if (error != cudaSuccess) { \\\n",
        "    printf(\"Fatal error: %s at %s:%d\\n\", \\\n",
        "      cudaGetErrorString(error), \\\n",
        "      __FILE__, __LINE__); \\\n",
        "    exit(1); \\\n",
        "  }\n",
        "\n",
        "__global__ void stencil_1d(int *in, int *out) \n",
        "{\n",
        "    // Just one global index \n",
        "    int index = threadIdx.x + (blockIdx.x * blockDim.x) + RADIUS;\n",
        "\n",
        "    // Apply the stencil\n",
        "    int result = 0;\n",
        "    for (int offset = -RADIUS ; offset <= RADIUS ; offset++)\n",
        "        result += in[index + offset];\n",
        "\n",
        "    // Store the result\n",
        "    out[index-RADIUS] = result;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  unsigned int i;\n",
        "  int h_in[NUM_ELEMENTS + 2 * RADIUS], h_out[NUM_ELEMENTS];\n",
        "  int *d_in, *d_out;\n",
        "\n",
        "  // Initialize host data\n",
        "  for( i = 0; i < (NUM_ELEMENTS + 2*RADIUS); ++i )\n",
        "    h_in[i] = 1; // With a value of 1 and RADIUS of 3, all output values should be 7\n",
        "\n",
        "  // Allocate space on the device\n",
        "  cudaCheck( cudaMalloc( &d_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int)) );\n",
        "  cudaCheck( cudaMalloc( &d_out, NUM_ELEMENTS * sizeof(int)) );\n",
        "\n",
        "  // Copy input data to device\n",
        "  cudaCheck( cudaMemcpy( d_in, h_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int), cudaMemcpyHostToDevice) );\n",
        "\n",
        "  stencil_1d<<< (NUM_ELEMENTS + BLOCK_SIZE - 1)/BLOCK_SIZE, BLOCK_SIZE >>> (d_in, d_out);\n",
        "\n",
        "  cudaCheck( cudaMemcpy( h_out, d_out, NUM_ELEMENTS * sizeof(int), cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // Verify every out value is 7\n",
        "  for( i = 0; i < NUM_ELEMENTS; ++i )\n",
        "    if (h_out[i] != 7)\n",
        "    {\n",
        "      printf(\"Element h_out[%d] == %d != 7\\n\", i, h_out[i]);\n",
        "      break;\n",
        "    }\n",
        "\n",
        "  if (i == NUM_ELEMENTS)\n",
        "    printf(\"SUCCESS!\\n\");\n",
        "\n",
        "  // Free out memory\n",
        "  cudaFree(d_in);\n",
        "  cudaFree(d_out);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing stencil1d_base.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fC5zcttL7KO_",
        "outputId": "0c11c5c9-a015-47c7-82d7-906da8c2f872"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true stencil1d_base.cu -o stencil1d_base -lcudadevrt"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0QKT55-7J-L",
        "outputId": "5207430d-fe6f-4246-e78f-e80aacf78c52"
      },
      "source": [
        "for i in range(100):\n",
        "  !./stencil1d_base"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDyxs9Tg8BHy"
      },
      "source": [
        "Ejecutamos el código 100 veces y vemos que no ha ocurrido problema. Comprobamos la ejecución con $\\texttt{nvprof}$ y vemos que se dedica un tiempo considerable a la copia de elementos, por lo que podemos mejorar en problema con un uso eficiente de memoria compartida."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9olPv5qi8NVi",
        "outputId": "33047c99-398f-4d74-8108-9a8040d2eaab"
      },
      "source": [
        "!nvprof ./stencil1d_base "
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==982== NVPROF is profiling process 982, command: ./stencil1d_base\n",
            "==982== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "SUCCESS!\n",
            "==982== Profiling application: ./stencil1d_base\n",
            "==982== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   44.68%  6.1760us         1  6.1760us  6.1760us  6.1760us  [CUDA memcpy HtoD]\n",
            "                   34.26%  4.7360us         1  4.7360us  4.7360us  4.7360us  [CUDA memcpy DtoH]\n",
            "                   21.06%  2.9120us         1  2.9120us  2.9120us  2.9120us  stencil_1d(int*, int*)\n",
            "      API calls:   99.47%  195.58ms         2  97.788ms  6.5570us  195.57ms  cudaMalloc\n",
            "                    0.27%  530.07us         1  530.07us  530.07us  530.07us  cuDeviceTotalMem\n",
            "                    0.11%  219.43us       101  2.1720us     160ns  98.401us  cuDeviceGetAttribute\n",
            "                    0.07%  142.49us         2  71.245us  13.312us  129.18us  cudaFree\n",
            "                    0.04%  76.836us         2  38.418us  28.971us  47.865us  cudaMemcpy\n",
            "                    0.02%  29.603us         1  29.603us  29.603us  29.603us  cuDeviceGetName\n",
            "                    0.01%  25.142us         1  25.142us  25.142us  25.142us  cudaLaunchKernel\n",
            "                    0.00%  7.2780us         1  7.2780us  7.2780us  7.2780us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.2350us         3     745ns     286ns  1.1210us  cuDeviceGetCount\n",
            "                    0.00%  1.5390us         2     769ns     328ns  1.2110us  cuDeviceGet\n",
            "                    0.00%     296ns         1     296ns     296ns     296ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6tDS3q28yAc"
      },
      "source": [
        "#### Versión con memoria compartida, pero sin sincronización de threads\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGaG1nrF3hKZ",
        "outputId": "7292da44-ae9c-465d-86dc-25683a35cfb8"
      },
      "source": [
        "%%writefile stenciltest_sm_1.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "#define RADIUS        3\n",
        "#define BLOCK_SIZE    256\n",
        "#define NUM_ELEMENTS  (4096*2)\n",
        "\n",
        "// CUDA API error checking macro\n",
        "#define cudaCheck(error) \\\n",
        "  if (error != cudaSuccess) { \\\n",
        "    printf(\"Fatal error: %s at %s:%d\\n\", \\\n",
        "      cudaGetErrorString(error), \\\n",
        "      __FILE__, __LINE__); \\\n",
        "    exit(1); \\\n",
        "  }\n",
        "\n",
        "__global__ void stencil_1d(int *in, int *out) \n",
        "{\n",
        "    __shared__ int temp[BLOCK_SIZE + 2 * RADIUS];\n",
        "    int gindex = threadIdx.x + (blockIdx.x * blockDim.x) + RADIUS;\n",
        "    int lindex = threadIdx.x + RADIUS;\n",
        "\n",
        "    // Read input elements into shared memory\n",
        "    temp[lindex] = in[gindex];\n",
        "    if (threadIdx.x < RADIUS) \n",
        "    {\n",
        "        temp[lindex - RADIUS] = in[gindex - RADIUS];\n",
        "        temp[lindex + BLOCK_SIZE] = in[gindex + BLOCK_SIZE];\n",
        "    }\n",
        "\n",
        "    // Make sure all threads get to this point before proceeding!\n",
        "    //__syncthreads();\n",
        "\n",
        "    // Apply the stencil\n",
        "    int result = 0;\n",
        "    for (int offset = -RADIUS ; offset <= RADIUS ; offset++)\n",
        "        result += temp[lindex + offset];\n",
        "\n",
        "    // Store the result\n",
        "    out[gindex-RADIUS] = result;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  unsigned int i;\n",
        "  int h_in[NUM_ELEMENTS + 2 * RADIUS], h_out[NUM_ELEMENTS];\n",
        "  int *d_in, *d_out;\n",
        "\n",
        "  // Initialize host data\n",
        "  for( i = 0; i < (NUM_ELEMENTS + 2*RADIUS); ++i )\n",
        "    h_in[i] = 1; // With a value of 1 and RADIUS of 3, all output values should be 7\n",
        "\n",
        "  // Allocate space on the device\n",
        "  cudaCheck( cudaMalloc( &d_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int)) );\n",
        "  cudaCheck( cudaMalloc( &d_out, NUM_ELEMENTS * sizeof(int)) );\n",
        "\n",
        "  // Copy input data to device\n",
        "  cudaCheck( cudaMemcpy( d_in, h_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int), cudaMemcpyHostToDevice) );\n",
        "\n",
        "  stencil_1d<<< (NUM_ELEMENTS + BLOCK_SIZE - 1)/BLOCK_SIZE, BLOCK_SIZE >>> (d_in, d_out);\n",
        "\n",
        "  cudaCheck( cudaMemcpy( h_out, d_out, NUM_ELEMENTS * sizeof(int), cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // Verify every out value is 7\n",
        "  for( i = 0; i < NUM_ELEMENTS; ++i )\n",
        "    if (h_out[i] != 7)\n",
        "    {\n",
        "      printf(\"Element h_out[%d] == %d != 7\\n\", i, h_out[i]);\n",
        "      break;\n",
        "    }\n",
        "\n",
        "  if (i == NUM_ELEMENTS)\n",
        "    printf(\"SUCCESS!\\n\");\n",
        "\n",
        "  // Free out memory\n",
        "  cudaFree(d_in);\n",
        "  cudaFree(d_out);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing stenciltest_sm_1.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXahjwTKG4Xy",
        "outputId": "b34f560f-f13e-4ab3-db88-0c09661315fd"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true stenciltest_sm_1.cu -o ./stenciltest_sm_1 -lcudadevrt"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4t7fl5s2IIMw",
        "outputId": "9e20e5d6-4ea3-44cf-f02e-cb2cba3b5fd7"
      },
      "source": [
        "for i in range(100):\n",
        "  !./stenciltest_sm_1"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Element h_out[0] == 2 != 7\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_VaWf_3EF-C",
        "outputId": "31942ea8-f70c-446c-fb3a-0f01bb064659"
      },
      "source": [
        "!nvprof ./stenciltest_sm_1"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==1330== NVPROF is profiling process 1330, command: ./stenciltest_sm_1\n",
            "==1330== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "SUCCESS!\n",
            "==1330== Profiling application: ./stenciltest_sm_1\n",
            "==1330== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   59.65%  15.136us         1  15.136us  15.136us  15.136us  [CUDA memcpy HtoD]\n",
            "                   27.11%  6.8800us         1  6.8800us  6.8800us  6.8800us  [CUDA memcpy DtoH]\n",
            "                   13.24%  3.3600us         1  3.3600us  3.3600us  3.3600us  stencil_1d(int*, int*)\n",
            "      API calls:   99.47%  195.52ms         2  97.759ms  5.0560us  195.51ms  cudaMalloc\n",
            "                    0.25%  494.18us         1  494.18us  494.18us  494.18us  cuDeviceTotalMem\n",
            "                    0.11%  225.18us       101  2.2290us     153ns  87.800us  cuDeviceGetAttribute\n",
            "                    0.08%  149.45us         2  74.726us  12.626us  136.83us  cudaFree\n",
            "                    0.05%  98.131us         2  49.065us  37.031us  61.100us  cudaMemcpy\n",
            "                    0.02%  36.221us         1  36.221us  36.221us  36.221us  cudaLaunchKernel\n",
            "                    0.02%  29.732us         1  29.732us  29.732us  29.732us  cuDeviceGetName\n",
            "                    0.00%  8.9110us         1  8.9110us  8.9110us  8.9110us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.4320us         3     810ns     250ns  1.2670us  cuDeviceGetCount\n",
            "                    0.00%  1.8570us         2     928ns     508ns  1.3490us  cuDeviceGet\n",
            "                    0.00%     320ns         1     320ns     320ns     320ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piM0l12_9z8f"
      },
      "source": [
        "En este caso vemos que existen errores ya que no se utiliza la función $\\texttt{__syncthreads()}$ para hacer que no exista la condición de carrera con los threads. Por ello procedemos a utilizar la función y conseguir que el efecto de la memoria compartida sea seguro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SEy0nUw-Rie"
      },
      "source": [
        "#### Versión con memoria compartida y sincronía de threads."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76ur4eykFTkP",
        "outputId": "c21c6270-4a30-4b8a-de42-058bb26af618"
      },
      "source": [
        "%%writefile stenciltest_sm_2.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "#define RADIUS        3\n",
        "#define BLOCK_SIZE    256\n",
        "#define NUM_ELEMENTS  (4096*2)\n",
        "\n",
        "// CUDA API error checking macro\n",
        "#define cudaCheck(error) \\\n",
        "  if (error != cudaSuccess) { \\\n",
        "    printf(\"Fatal error: %s at %s:%d\\n\", \\\n",
        "      cudaGetErrorString(error), \\\n",
        "      __FILE__, __LINE__); \\\n",
        "    exit(1); \\\n",
        "  }\n",
        "\n",
        "__global__ void stencil_1d(int *in, int *out) \n",
        "{\n",
        "    __shared__ int temp[BLOCK_SIZE + 2 * RADIUS];\n",
        "    int gindex = threadIdx.x + (blockIdx.x * blockDim.x) + RADIUS;\n",
        "    int lindex = threadIdx.x + RADIUS;\n",
        "\n",
        "    // Read input elements into shared memory\n",
        "    temp[lindex] = in[gindex];\n",
        "    if (threadIdx.x < RADIUS) \n",
        "    {\n",
        "        temp[lindex - RADIUS] = in[gindex - RADIUS];\n",
        "        temp[lindex + BLOCK_SIZE] = in[gindex + BLOCK_SIZE];\n",
        "    }\n",
        "\n",
        "    // Make sure all threads get to this point before proceeding!\n",
        "    __syncthreads();\n",
        "\n",
        "    // Apply the stencil\n",
        "    int result = 0;\n",
        "    for (int offset = -RADIUS ; offset <= RADIUS ; offset++)\n",
        "        result += temp[lindex + offset];\n",
        "\n",
        "    // Store the result\n",
        "    out[gindex-RADIUS] = result;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  unsigned int i;\n",
        "  int h_in[NUM_ELEMENTS + 2 * RADIUS], h_out[NUM_ELEMENTS];\n",
        "  int *d_in, *d_out;\n",
        "\n",
        "  // Initialize host data\n",
        "  for( i = 0; i < (NUM_ELEMENTS + 2*RADIUS); ++i )\n",
        "    h_in[i] = 1; // With a value of 1 and RADIUS of 3, all output values should be 7\n",
        "\n",
        "  // Allocate space on the device\n",
        "  cudaCheck( cudaMalloc( &d_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int)) );\n",
        "  cudaCheck( cudaMalloc( &d_out, NUM_ELEMENTS * sizeof(int)) );\n",
        "\n",
        "  // Copy input data to device\n",
        "  cudaCheck( cudaMemcpy( d_in, h_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int), cudaMemcpyHostToDevice) );\n",
        "\n",
        "  stencil_1d<<< (NUM_ELEMENTS + BLOCK_SIZE - 1)/BLOCK_SIZE, BLOCK_SIZE >>> (d_in, d_out);\n",
        "\n",
        "  cudaCheck( cudaMemcpy( h_out, d_out, NUM_ELEMENTS * sizeof(int), cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // Verify every out value is 7\n",
        "  for( i = 0; i < NUM_ELEMENTS; ++i )\n",
        "    if (h_out[i] != 7)\n",
        "    {\n",
        "      printf(\"Element h_out[%d] == %d != 7\\n\", i, h_out[i]);\n",
        "      break;\n",
        "    }\n",
        "\n",
        "  if (i == NUM_ELEMENTS)\n",
        "    printf(\"SUCCESS!\\n\");\n",
        "\n",
        "  // Free out memory\n",
        "  cudaFree(d_in);\n",
        "  cudaFree(d_out);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing stenciltest_sm_2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CamqRgGsHxU-",
        "outputId": "fe43b2d7-b02e-4fae-cad3-179ed762b875"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true stenciltest_sm_2.cu -o ./stenciltest_sm_2 -lcudadevrt"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9tmF1F4H1jY",
        "outputId": "72f7d782-378d-49b0-91dd-bb5c54e7a37c"
      },
      "source": [
        "for i in range(100):\n",
        "  !./stenciltest_sm_2"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n",
            "SUCCESS!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bodklbXLH-LF",
        "outputId": "c3fcc792-8b81-47a4-b9c9-508c78e46e49"
      },
      "source": [
        "!nvprof ./stenciltest_sm_2"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==1678== NVPROF is profiling process 1678, command: ./stenciltest_sm_2\n",
            "==1678== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "==1678== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
            "SUCCESS!\n",
            "==1678== Profiling application: ./stenciltest_sm_2\n",
            "==1678== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   45.89%  9.1200us         1  9.1200us  9.1200us  9.1200us  [CUDA memcpy HtoD]\n",
            "                   34.14%  6.7840us         1  6.7840us  6.7840us  6.7840us  [CUDA memcpy DtoH]\n",
            "                   19.97%  3.9680us         1  3.9680us  3.9680us  3.9680us  stencil_1d(int*, int*)\n",
            "      API calls:   99.44%  180.20ms         2  90.100ms  5.0490us  180.20ms  cudaMalloc\n",
            "                    0.26%  465.07us         1  465.07us  465.07us  465.07us  cuDeviceTotalMem\n",
            "                    0.12%  215.93us       101  2.1370us     153ns  95.719us  cuDeviceGetAttribute\n",
            "                    0.08%  141.33us         2  70.666us  16.060us  125.27us  cudaFree\n",
            "                    0.05%  98.870us         2  49.435us  25.767us  73.103us  cudaMemcpy\n",
            "                    0.03%  59.289us         1  59.289us  59.289us  59.289us  cudaLaunchKernel\n",
            "                    0.02%  28.669us         1  28.669us  28.669us  28.669us  cuDeviceGetName\n",
            "                    0.00%  8.3000us         1  8.3000us  8.3000us  8.3000us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.8640us         3     621ns     246ns     986ns  cuDeviceGetCount\n",
            "                    0.00%  1.5890us         2     794ns     312ns  1.2770us  cuDeviceGet\n",
            "                    0.00%     259ns         1     259ns     259ns     259ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9VI8UK6-pl9"
      },
      "source": [
        "Gracias a la utilización de la barrera se garantiza que todos los threads lleguen a tiempo y se realice una correcta ejecución. \n",
        "\n",
        "Vemos que en este caso no hay una diferencia notable en tiempos de ejecución, lo que puede deberse a un tamaño bajo de radio y vector. Cambiamos los valores y comparamos con la versión base y con la de memoria compartida con sincronía de threads."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyZfMttJ_5RL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1969a3c-cdc9-478b-9368-74a445aaaaeb"
      },
      "source": [
        "!sed -i '/#define RADIUS/c\\#define RADIUS 5' stencil1d_base.cu\n",
        "!sed -i '/#define NUM_ELEMENTS/c\\#define NUM_ELEMENTS (4096*16)' stencil1d_base.cu\n",
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true stencil1d_base.cu -o stencil1d_base -lcudadevrt\n",
        "!nvprof ./stencil1d_base"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==1726== NVPROF is profiling process 1726, command: ./stencil1d_base\n",
            "==1726== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "Element h_out[0] == 11 != 7\n",
            "==1726== Profiling application: ./stencil1d_base\n",
            "==1726== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   41.92%  38.912us         1  38.912us  38.912us  38.912us  [CUDA memcpy HtoD]\n",
            "                   38.30%  35.552us         1  35.552us  35.552us  35.552us  [CUDA memcpy DtoH]\n",
            "                   19.79%  18.367us         1  18.367us  18.367us  18.367us  stencil_1d(int*, int*)\n",
            "      API calls:   99.26%  193.92ms         2  96.962ms  5.1930us  193.92ms  cudaMalloc\n",
            "                    0.31%  601.76us         1  601.76us  601.76us  601.76us  cuDeviceTotalMem\n",
            "                    0.18%  358.76us         2  179.38us  82.407us  276.35us  cudaMemcpy\n",
            "                    0.11%  209.98us         2  104.99us  16.115us  193.86us  cudaFree\n",
            "                    0.11%  205.69us       101  2.0360us     142ns  84.221us  cuDeviceGetAttribute\n",
            "                    0.02%  31.215us         1  31.215us  31.215us  31.215us  cuDeviceGetName\n",
            "                    0.01%  25.326us         1  25.326us  25.326us  25.326us  cudaLaunchKernel\n",
            "                    0.00%  4.8560us         1  4.8560us  4.8560us  4.8560us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.7670us         3     922ns     339ns  1.3510us  cuDeviceGetCount\n",
            "                    0.00%  1.9930us         2     996ns     462ns  1.5310us  cuDeviceGet\n",
            "                    0.00%     276ns         1     276ns     276ns     276ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrlPuv4M_5LB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b792723e-e076-457d-c80e-5cc7ddba3b2a"
      },
      "source": [
        "!sed -i '/#define RADIUS/c\\#define RADIUS 5' stenciltest_sm_2.cu\n",
        "!sed -i '/#define NUM_ELEMENTS/c\\#define NUM_ELEMENTS (4096*16)' stenciltest_sm_2.cu\n",
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true stenciltest_sm_2.cu -o stenciltest_sm_2 -lcudadevrt\n",
        "!nvprof ./stenciltest_sm_2"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==1772== NVPROF is profiling process 1772, command: ./stenciltest_sm_2\n",
            "==1772== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "==1772== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
            "Element h_out[0] == 11 != 7\n",
            "==1772== Profiling application: ./stenciltest_sm_2\n",
            "==1772== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   50.74%  46.912us         1  46.912us  46.912us  46.912us  [CUDA memcpy HtoD]\n",
            "                   38.63%  35.712us         1  35.712us  35.712us  35.712us  [CUDA memcpy DtoH]\n",
            "                   10.63%  9.8240us         1  9.8240us  9.8240us  9.8240us  stencil_1d(int*, int*)\n",
            "      API calls:   99.37%  199.97ms         2  99.985ms  5.0200us  199.96ms  cudaMalloc\n",
            "                    0.24%  492.70us         1  492.70us  492.70us  492.70us  cuDeviceTotalMem\n",
            "                    0.15%  297.45us         2  148.72us  82.558us  214.89us  cudaMemcpy\n",
            "                    0.10%  208.71us       101  2.0660us     144ns  95.397us  cuDeviceGetAttribute\n",
            "                    0.08%  163.28us         2  81.641us  13.128us  150.15us  cudaFree\n",
            "                    0.03%  69.300us         1  69.300us  69.300us  69.300us  cudaLaunchKernel\n",
            "                    0.01%  28.251us         1  28.251us  28.251us  28.251us  cuDeviceGetName\n",
            "                    0.00%  7.5830us         1  7.5830us  7.5830us  7.5830us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.3530us         3     784ns     242ns  1.1130us  cuDeviceGetCount\n",
            "                    0.00%  1.7210us         2     860ns     571ns  1.1500us  cuDeviceGet\n",
            "                    0.00%     409ns         1     409ns     409ns     409ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU0BHCiQJIEd"
      },
      "source": [
        "#EJERCICIO OPCIONAL: Producto de matrices en GPU\n",
        "\n",
        "En este código, solo se utiliza un bloque con 16x16 threads y el reparto de trabajo a los threads se ha definido de manera bidimensional.\n",
        "En la ejecución serie de la CPU las operaciones realizadas son $O(N^3)$ mientras que para la GPU se ha reducido a $O(N^2)$ las operaciones que realiza cada thread.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOQ4Bjol1DRq"
      },
      "source": [
        "Primero ejecutamos el algoritmo proporcionado en CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HL_ZsWh6_Dy",
        "outputId": "759afafb-60b7-4fa4-97c7-6c8a21550553"
      },
      "source": [
        "%%writefile matrixMul_cpu.cu\n",
        "#include <stdio.h>\n",
        "#define N 16\n",
        "#define M 1\n",
        "\n",
        "void matrixMultCPU(int a[N][N], int b[N][N], int c[N][N]){\n",
        "    int n, m;\n",
        "    for (int i = 0; i < N; i++){\n",
        "        for (int j = 0; j < N; j++){\n",
        "            int sum = 0;\n",
        "            for (int k = 0; k < N; k++){\n",
        "                m = a[i][k];\n",
        "                n = b[k][j];\n",
        "                sum += m*n;\n",
        "            }\n",
        "            c[i][j] = sum;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    int a[N][N], b[N][N], c[N][N];\n",
        "    int cont, i , j;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    for (i = 0; i < N; i++){\n",
        "        cont = 0;\n",
        "        for (j = 0; j < N; j++){\n",
        "            a[i][j] = cont;\n",
        "            b[i][j] = cont;\n",
        "            cont++;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    int nIter = 1000;\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    for (int m = 0; m < nIter; m++){\n",
        "        matrixMultCPU(a, b, c);\n",
        "    }\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float msecTotal = 0.0;\n",
        "    cudaEventElapsedTime(&msecTotal, start, stop);\n",
        "\n",
        "    for (int y = 0; y < N; y++){\n",
        "        for (int x = 0; x < N; x++){\n",
        "            printf(\"[%d][%d]=%d \", y, x, c[x][y]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    float msecPerKernelExecution = msecTotal / nIter;\n",
        "    double flopsPerMMull = 2.0 * N * N * N;\n",
        "    double gigaFlops = (flopsPerMMull * 1.0e-9f) / (msecPerKernelExecution / 1000.0f);\n",
        "\n",
        "    printf(\"GigaFlops: %f\", gigaFlops);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing matrixMul_cpu.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucU3ablX7fvi",
        "outputId": "fa6f46c5-53b6-4b67-dcc5-22b35b570411"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true matrixMul_cpu.cu -o matrixMul_cpu -lcudadevrt\n",
        "!nvprof ./matrixMul_cpu"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==1816== NVPROF is profiling process 1816, command: ./matrixMul_cpu\n",
            "==1816== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "[0][0]=0 [0][1]=0 [0][2]=0 [0][3]=0 [0][4]=0 [0][5]=0 [0][6]=0 [0][7]=0 [0][8]=0 [0][9]=0 [0][10]=0 [0][11]=0 [0][12]=0 [0][13]=0 [0][14]=0 [0][15]=0 \n",
            "[1][0]=120 [1][1]=120 [1][2]=120 [1][3]=120 [1][4]=120 [1][5]=120 [1][6]=120 [1][7]=120 [1][8]=120 [1][9]=120 [1][10]=120 [1][11]=120 [1][12]=120 [1][13]=120 [1][14]=120 [1][15]=120 \n",
            "[2][0]=240 [2][1]=240 [2][2]=240 [2][3]=240 [2][4]=240 [2][5]=240 [2][6]=240 [2][7]=240 [2][8]=240 [2][9]=240 [2][10]=240 [2][11]=240 [2][12]=240 [2][13]=240 [2][14]=240 [2][15]=240 \n",
            "[3][0]=360 [3][1]=360 [3][2]=360 [3][3]=360 [3][4]=360 [3][5]=360 [3][6]=360 [3][7]=360 [3][8]=360 [3][9]=360 [3][10]=360 [3][11]=360 [3][12]=360 [3][13]=360 [3][14]=360 [3][15]=360 \n",
            "[4][0]=480 [4][1]=480 [4][2]=480 [4][3]=480 [4][4]=480 [4][5]=480 [4][6]=480 [4][7]=480 [4][8]=480 [4][9]=480 [4][10]=480 [4][11]=480 [4][12]=480 [4][13]=480 [4][14]=480 [4][15]=480 \n",
            "[5][0]=600 [5][1]=600 [5][2]=600 [5][3]=600 [5][4]=600 [5][5]=600 [5][6]=600 [5][7]=600 [5][8]=600 [5][9]=600 [5][10]=600 [5][11]=600 [5][12]=600 [5][13]=600 [5][14]=600 [5][15]=600 \n",
            "[6][0]=720 [6][1]=720 [6][2]=720 [6][3]=720 [6][4]=720 [6][5]=720 [6][6]=720 [6][7]=720 [6][8]=720 [6][9]=720 [6][10]=720 [6][11]=720 [6][12]=720 [6][13]=720 [6][14]=720 [6][15]=720 \n",
            "[7][0]=840 [7][1]=840 [7][2]=840 [7][3]=840 [7][4]=840 [7][5]=840 [7][6]=840 [7][7]=840 [7][8]=840 [7][9]=840 [7][10]=840 [7][11]=840 [7][12]=840 [7][13]=840 [7][14]=840 [7][15]=840 \n",
            "[8][0]=960 [8][1]=960 [8][2]=960 [8][3]=960 [8][4]=960 [8][5]=960 [8][6]=960 [8][7]=960 [8][8]=960 [8][9]=960 [8][10]=960 [8][11]=960 [8][12]=960 [8][13]=960 [8][14]=960 [8][15]=960 \n",
            "[9][0]=1080 [9][1]=1080 [9][2]=1080 [9][3]=1080 [9][4]=1080 [9][5]=1080 [9][6]=1080 [9][7]=1080 [9][8]=1080 [9][9]=1080 [9][10]=1080 [9][11]=1080 [9][12]=1080 [9][13]=1080 [9][14]=1080 [9][15]=1080 \n",
            "[10][0]=1200 [10][1]=1200 [10][2]=1200 [10][3]=1200 [10][4]=1200 [10][5]=1200 [10][6]=1200 [10][7]=1200 [10][8]=1200 [10][9]=1200 [10][10]=1200 [10][11]=1200 [10][12]=1200 [10][13]=1200 [10][14]=1200 [10][15]=1200 \n",
            "[11][0]=1320 [11][1]=1320 [11][2]=1320 [11][3]=1320 [11][4]=1320 [11][5]=1320 [11][6]=1320 [11][7]=1320 [11][8]=1320 [11][9]=1320 [11][10]=1320 [11][11]=1320 [11][12]=1320 [11][13]=1320 [11][14]=1320 [11][15]=1320 \n",
            "[12][0]=1440 [12][1]=1440 [12][2]=1440 [12][3]=1440 [12][4]=1440 [12][5]=1440 [12][6]=1440 [12][7]=1440 [12][8]=1440 [12][9]=1440 [12][10]=1440 [12][11]=1440 [12][12]=1440 [12][13]=1440 [12][14]=1440 [12][15]=1440 \n",
            "[13][0]=1560 [13][1]=1560 [13][2]=1560 [13][3]=1560 [13][4]=1560 [13][5]=1560 [13][6]=1560 [13][7]=1560 [13][8]=1560 [13][9]=1560 [13][10]=1560 [13][11]=1560 [13][12]=1560 [13][13]=1560 [13][14]=1560 [13][15]=1560 \n",
            "[14][0]=1680 [14][1]=1680 [14][2]=1680 [14][3]=1680 [14][4]=1680 [14][5]=1680 [14][6]=1680 [14][7]=1680 [14][8]=1680 [14][9]=1680 [14][10]=1680 [14][11]=1680 [14][12]=1680 [14][13]=1680 [14][14]=1680 [14][15]=1680 \n",
            "[15][0]=1800 [15][1]=1800 [15][2]=1800 [15][3]=1800 [15][4]=1800 [15][5]=1800 [15][6]=1800 [15][7]=1800 [15][8]=1800 [15][9]=1800 [15][10]=1800 [15][11]=1800 [15][12]=1800 [15][13]=1800 [15][14]=1800 [15][15]=1800 \n",
            "GigaFlops: 0.507847==1816== Profiling application: ./matrixMul_cpu\n",
            "==1816== Profiling result:\n",
            "No kernels were profiled.\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            "      API calls:   99.60%  186.31ms         2  93.155ms  1.1980us  186.31ms  cudaEventCreate\n",
            "                    0.27%  502.27us         1  502.27us  502.27us  502.27us  cuDeviceTotalMem\n",
            "                    0.09%  170.14us       101  1.6840us     156ns  70.451us  cuDeviceGetAttribute\n",
            "                    0.02%  34.315us         1  34.315us  34.315us  34.315us  cuDeviceGetName\n",
            "                    0.01%  18.083us         2  9.0410us  8.3880us  9.6950us  cudaEventRecord\n",
            "                    0.00%  9.0790us         1  9.0790us  9.0790us  9.0790us  cudaEventSynchronize\n",
            "                    0.00%  5.5660us         1  5.5660us  5.5660us  5.5660us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.4270us         1  2.4270us  2.4270us  2.4270us  cudaEventElapsedTime\n",
            "                    0.00%  2.0270us         3     675ns     170ns  1.0440us  cuDeviceGetCount\n",
            "                    0.00%  1.8310us         2     915ns     313ns  1.5180us  cuDeviceGet\n",
            "                    0.00%     318ns         1     318ns     318ns     318ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlgsgPbl7oso"
      },
      "source": [
        "Ahora ejecutamos el ejemplo para GPU usando memoria global."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjbJNj7jvEcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dac47bcd-587e-49be-8f9b-fab2ebb2123a"
      },
      "source": [
        "%%writefile matrixMul_gpu.cu\n",
        "#include <stdio.h>\n",
        "#define N 16\n",
        "#define M 1\n",
        "\n",
        "__global__ void matrixMultGPU(int *a, int *b, int *c){\n",
        "    int k;\n",
        "    float sum = 0;\n",
        "    int col = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "    int fil = threadIdx.y + blockDim.y * blockIdx.y;\n",
        "    if (col < N && fil < N){\n",
        "        for (k = 0; k < N; k++){\n",
        "            sum += a[fil*N+k] * b[k*N+col];\n",
        "        }\n",
        "        c[fil*N+col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    int a[N][N], b[N][N], c[N][N];\n",
        "    int *dev_a, *dev_b, *dev_c;\n",
        "    int cont, i , j;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    for (i = 0; i < N; i++){\n",
        "        cont = 0;\n",
        "        for (j = 0; j < N; j++){\n",
        "            a[i][j] = cont;\n",
        "            b[i][j] = cont;\n",
        "            cont++;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    int size = N*N*sizeof(int);\n",
        "    cudaMalloc((void **) &dev_a, size);\n",
        "    cudaMalloc((void **) &dev_b, size);\n",
        "    cudaMalloc((void **) &dev_c, size);\n",
        "\n",
        "    cudaMemcpy(dev_a, a, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 dimGrid(M, M);\n",
        "    dim3 dimBlock(N, N);\n",
        "\n",
        "    int nIter = 1000;\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    for (int m = 0; m < nIter; m++){\n",
        "        matrixMultGPU<<<dimGrid, dimBlock>>>(dev_a, dev_b, dev_c);\n",
        "    }\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    cudaMemcpy(c, dev_c, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    float msecTotal = 0.0;\n",
        "    cudaEventElapsedTime(&msecTotal, start, stop);\n",
        "\n",
        "    cudaFree(dev_a);\n",
        "    cudaFree(dev_b);\n",
        "    cudaFree(dev_c);\n",
        "\n",
        "    for (int y = 0; y < N; y++){\n",
        "        for (int x = 0; x < N; x++){\n",
        "            printf(\"[%d][%d]=%d \", y, x, c[x][y]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    float msecPerKernelExecution = msecTotal / nIter;\n",
        "    double flopsPerMMull = 2.0 * N * N * N;\n",
        "    double gigaFlops = (flopsPerMMull * 1.0e-9f) / (msecPerKernelExecution / 1000.0f);\n",
        "\n",
        "    printf(\"GigaFlops: %f\", gigaFlops);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matrixMul_gpu.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdBDoJo6weQ1",
        "outputId": "61ae591d-2d0d-412f-ce6b-4c78d618a75d"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true matrixMul_gpu.cu -o matrixMul_gpu -lcudadevrt\n",
        "!./matrixMul_gpu"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "[0][0]=0 [0][1]=0 [0][2]=0 [0][3]=0 [0][4]=0 [0][5]=0 [0][6]=0 [0][7]=0 [0][8]=0 [0][9]=0 [0][10]=0 [0][11]=0 [0][12]=0 [0][13]=0 [0][14]=0 [0][15]=0 \n",
            "[1][0]=120 [1][1]=120 [1][2]=120 [1][3]=120 [1][4]=120 [1][5]=120 [1][6]=120 [1][7]=120 [1][8]=120 [1][9]=120 [1][10]=120 [1][11]=120 [1][12]=120 [1][13]=120 [1][14]=120 [1][15]=120 \n",
            "[2][0]=240 [2][1]=240 [2][2]=240 [2][3]=240 [2][4]=240 [2][5]=240 [2][6]=240 [2][7]=240 [2][8]=240 [2][9]=240 [2][10]=240 [2][11]=240 [2][12]=240 [2][13]=240 [2][14]=240 [2][15]=240 \n",
            "[3][0]=360 [3][1]=360 [3][2]=360 [3][3]=360 [3][4]=360 [3][5]=360 [3][6]=360 [3][7]=360 [3][8]=360 [3][9]=360 [3][10]=360 [3][11]=360 [3][12]=360 [3][13]=360 [3][14]=360 [3][15]=360 \n",
            "[4][0]=480 [4][1]=480 [4][2]=480 [4][3]=480 [4][4]=480 [4][5]=480 [4][6]=480 [4][7]=480 [4][8]=480 [4][9]=480 [4][10]=480 [4][11]=480 [4][12]=480 [4][13]=480 [4][14]=480 [4][15]=480 \n",
            "[5][0]=600 [5][1]=600 [5][2]=600 [5][3]=600 [5][4]=600 [5][5]=600 [5][6]=600 [5][7]=600 [5][8]=600 [5][9]=600 [5][10]=600 [5][11]=600 [5][12]=600 [5][13]=600 [5][14]=600 [5][15]=600 \n",
            "[6][0]=720 [6][1]=720 [6][2]=720 [6][3]=720 [6][4]=720 [6][5]=720 [6][6]=720 [6][7]=720 [6][8]=720 [6][9]=720 [6][10]=720 [6][11]=720 [6][12]=720 [6][13]=720 [6][14]=720 [6][15]=720 \n",
            "[7][0]=840 [7][1]=840 [7][2]=840 [7][3]=840 [7][4]=840 [7][5]=840 [7][6]=840 [7][7]=840 [7][8]=840 [7][9]=840 [7][10]=840 [7][11]=840 [7][12]=840 [7][13]=840 [7][14]=840 [7][15]=840 \n",
            "[8][0]=960 [8][1]=960 [8][2]=960 [8][3]=960 [8][4]=960 [8][5]=960 [8][6]=960 [8][7]=960 [8][8]=960 [8][9]=960 [8][10]=960 [8][11]=960 [8][12]=960 [8][13]=960 [8][14]=960 [8][15]=960 \n",
            "[9][0]=1080 [9][1]=1080 [9][2]=1080 [9][3]=1080 [9][4]=1080 [9][5]=1080 [9][6]=1080 [9][7]=1080 [9][8]=1080 [9][9]=1080 [9][10]=1080 [9][11]=1080 [9][12]=1080 [9][13]=1080 [9][14]=1080 [9][15]=1080 \n",
            "[10][0]=1200 [10][1]=1200 [10][2]=1200 [10][3]=1200 [10][4]=1200 [10][5]=1200 [10][6]=1200 [10][7]=1200 [10][8]=1200 [10][9]=1200 [10][10]=1200 [10][11]=1200 [10][12]=1200 [10][13]=1200 [10][14]=1200 [10][15]=1200 \n",
            "[11][0]=1320 [11][1]=1320 [11][2]=1320 [11][3]=1320 [11][4]=1320 [11][5]=1320 [11][6]=1320 [11][7]=1320 [11][8]=1320 [11][9]=1320 [11][10]=1320 [11][11]=1320 [11][12]=1320 [11][13]=1320 [11][14]=1320 [11][15]=1320 \n",
            "[12][0]=1440 [12][1]=1440 [12][2]=1440 [12][3]=1440 [12][4]=1440 [12][5]=1440 [12][6]=1440 [12][7]=1440 [12][8]=1440 [12][9]=1440 [12][10]=1440 [12][11]=1440 [12][12]=1440 [12][13]=1440 [12][14]=1440 [12][15]=1440 \n",
            "[13][0]=1560 [13][1]=1560 [13][2]=1560 [13][3]=1560 [13][4]=1560 [13][5]=1560 [13][6]=1560 [13][7]=1560 [13][8]=1560 [13][9]=1560 [13][10]=1560 [13][11]=1560 [13][12]=1560 [13][13]=1560 [13][14]=1560 [13][15]=1560 \n",
            "[14][0]=1680 [14][1]=1680 [14][2]=1680 [14][3]=1680 [14][4]=1680 [14][5]=1680 [14][6]=1680 [14][7]=1680 [14][8]=1680 [14][9]=1680 [14][10]=1680 [14][11]=1680 [14][12]=1680 [14][13]=1680 [14][14]=1680 [14][15]=1680 \n",
            "[15][0]=1800 [15][1]=1800 [15][2]=1800 [15][3]=1800 [15][4]=1800 [15][5]=1800 [15][6]=1800 [15][7]=1800 [15][8]=1800 [15][9]=1800 [15][10]=1800 [15][11]=1800 [15][12]=1800 [15][13]=1800 [15][14]=1800 [15][15]=1800 \n",
            "GigaFlops: 0.797580"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmENwJd7x-Ai",
        "outputId": "3b46c312-22bc-400e-f6ea-2749d73d6ab3"
      },
      "source": [
        "!nvprof ./matrixMul_gpu"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==194== NVPROF is profiling process 194, command: ./matrixMul_gpu\n",
            "==194== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "[0][0]=0 [0][1]=0 [0][2]=0 [0][3]=0 [0][4]=0 [0][5]=0 [0][6]=0 [0][7]=0 [0][8]=0 [0][9]=0 [0][10]=0 [0][11]=0 [0][12]=0 [0][13]=0 [0][14]=0 [0][15]=0 \n",
            "[1][0]=120 [1][1]=120 [1][2]=120 [1][3]=120 [1][4]=120 [1][5]=120 [1][6]=120 [1][7]=120 [1][8]=120 [1][9]=120 [1][10]=120 [1][11]=120 [1][12]=120 [1][13]=120 [1][14]=120 [1][15]=120 \n",
            "[2][0]=240 [2][1]=240 [2][2]=240 [2][3]=240 [2][4]=240 [2][5]=240 [2][6]=240 [2][7]=240 [2][8]=240 [2][9]=240 [2][10]=240 [2][11]=240 [2][12]=240 [2][13]=240 [2][14]=240 [2][15]=240 \n",
            "[3][0]=360 [3][1]=360 [3][2]=360 [3][3]=360 [3][4]=360 [3][5]=360 [3][6]=360 [3][7]=360 [3][8]=360 [3][9]=360 [3][10]=360 [3][11]=360 [3][12]=360 [3][13]=360 [3][14]=360 [3][15]=360 \n",
            "[4][0]=480 [4][1]=480 [4][2]=480 [4][3]=480 [4][4]=480 [4][5]=480 [4][6]=480 [4][7]=480 [4][8]=480 [4][9]=480 [4][10]=480 [4][11]=480 [4][12]=480 [4][13]=480 [4][14]=480 [4][15]=480 \n",
            "[5][0]=600 [5][1]=600 [5][2]=600 [5][3]=600 [5][4]=600 [5][5]=600 [5][6]=600 [5][7]=600 [5][8]=600 [5][9]=600 [5][10]=600 [5][11]=600 [5][12]=600 [5][13]=600 [5][14]=600 [5][15]=600 \n",
            "[6][0]=720 [6][1]=720 [6][2]=720 [6][3]=720 [6][4]=720 [6][5]=720 [6][6]=720 [6][7]=720 [6][8]=720 [6][9]=720 [6][10]=720 [6][11]=720 [6][12]=720 [6][13]=720 [6][14]=720 [6][15]=720 \n",
            "[7][0]=840 [7][1]=840 [7][2]=840 [7][3]=840 [7][4]=840 [7][5]=840 [7][6]=840 [7][7]=840 [7][8]=840 [7][9]=840 [7][10]=840 [7][11]=840 [7][12]=840 [7][13]=840 [7][14]=840 [7][15]=840 \n",
            "[8][0]=960 [8][1]=960 [8][2]=960 [8][3]=960 [8][4]=960 [8][5]=960 [8][6]=960 [8][7]=960 [8][8]=960 [8][9]=960 [8][10]=960 [8][11]=960 [8][12]=960 [8][13]=960 [8][14]=960 [8][15]=960 \n",
            "[9][0]=1080 [9][1]=1080 [9][2]=1080 [9][3]=1080 [9][4]=1080 [9][5]=1080 [9][6]=1080 [9][7]=1080 [9][8]=1080 [9][9]=1080 [9][10]=1080 [9][11]=1080 [9][12]=1080 [9][13]=1080 [9][14]=1080 [9][15]=1080 \n",
            "[10][0]=1200 [10][1]=1200 [10][2]=1200 [10][3]=1200 [10][4]=1200 [10][5]=1200 [10][6]=1200 [10][7]=1200 [10][8]=1200 [10][9]=1200 [10][10]=1200 [10][11]=1200 [10][12]=1200 [10][13]=1200 [10][14]=1200 [10][15]=1200 \n",
            "[11][0]=1320 [11][1]=1320 [11][2]=1320 [11][3]=1320 [11][4]=1320 [11][5]=1320 [11][6]=1320 [11][7]=1320 [11][8]=1320 [11][9]=1320 [11][10]=1320 [11][11]=1320 [11][12]=1320 [11][13]=1320 [11][14]=1320 [11][15]=1320 \n",
            "[12][0]=1440 [12][1]=1440 [12][2]=1440 [12][3]=1440 [12][4]=1440 [12][5]=1440 [12][6]=1440 [12][7]=1440 [12][8]=1440 [12][9]=1440 [12][10]=1440 [12][11]=1440 [12][12]=1440 [12][13]=1440 [12][14]=1440 [12][15]=1440 \n",
            "[13][0]=1560 [13][1]=1560 [13][2]=1560 [13][3]=1560 [13][4]=1560 [13][5]=1560 [13][6]=1560 [13][7]=1560 [13][8]=1560 [13][9]=1560 [13][10]=1560 [13][11]=1560 [13][12]=1560 [13][13]=1560 [13][14]=1560 [13][15]=1560 \n",
            "[14][0]=1680 [14][1]=1680 [14][2]=1680 [14][3]=1680 [14][4]=1680 [14][5]=1680 [14][6]=1680 [14][7]=1680 [14][8]=1680 [14][9]=1680 [14][10]=1680 [14][11]=1680 [14][12]=1680 [14][13]=1680 [14][14]=1680 [14][15]=1680 \n",
            "[15][0]=1800 [15][1]=1800 [15][2]=1800 [15][3]=1800 [15][4]=1800 [15][5]=1800 [15][6]=1800 [15][7]=1800 [15][8]=1800 [15][9]=1800 [15][10]=1800 [15][11]=1800 [15][12]=1800 [15][13]=1800 [15][14]=1800 [15][15]=1800 \n",
            "GigaFlops: 0.790807==194== Profiling application: ./matrixMul_gpu\n",
            "==194== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.87%  5.1915ms      1000  5.1910us  5.0230us  12.992us  matrixMultGPU(int*, int*, int*)\n",
            "                    0.08%  4.1600us         2  2.0800us  1.7600us  2.4000us  [CUDA memcpy HtoD]\n",
            "                    0.05%  2.4950us         1  2.4950us  2.4950us  2.4950us  [CUDA memcpy DtoH]\n",
            "      API calls:   95.71%  252.48ms         2  126.24ms  1.1900us  252.48ms  cudaEventCreate\n",
            "                    2.04%  5.3948ms         1  5.3948ms  5.3948ms  5.3948ms  cudaEventSynchronize\n",
            "                    1.75%  4.6095ms      1000  4.6090us  3.8000us  33.573us  cudaLaunchKernel\n",
            "                    0.20%  535.85us         1  535.85us  535.85us  535.85us  cuDeviceTotalMem\n",
            "                    0.10%  274.15us       101  2.7140us     160ns  146.77us  cuDeviceGetAttribute\n",
            "                    0.09%  238.23us         3  79.408us  2.2770us  231.42us  cudaMalloc\n",
            "                    0.06%  165.12us         3  55.041us  2.6620us  153.60us  cudaFree\n",
            "                    0.02%  65.405us         3  21.801us  12.587us  31.857us  cudaMemcpy\n",
            "                    0.01%  24.649us         1  24.649us  24.649us  24.649us  cuDeviceGetName\n",
            "                    0.00%  7.4480us         1  7.4480us  7.4480us  7.4480us  cuDeviceGetPCIBusId\n",
            "                    0.00%  7.2670us         2  3.6330us  3.2490us  4.0180us  cudaEventRecord\n",
            "                    0.00%  2.9300us         1  2.9300us  2.9300us  2.9300us  cudaEventElapsedTime\n",
            "                    0.00%  2.8390us         3     946ns     397ns  1.5960us  cuDeviceGetCount\n",
            "                    0.00%  1.6370us         2     818ns     351ns  1.2860us  cuDeviceGet\n",
            "                    0.00%     481ns         1     481ns     481ns     481ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcRkQU69C9pN"
      },
      "source": [
        "Vemos que el tiempo de ejecución es mucho mejor en GPU. Ahora variamos el número de hilos por bloque y ajustamos el tamaño de particionado, al igual que hicimos en el ejemplo de la suma de vectores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcmLBwho75SE",
        "outputId": "a2352040-1e66-4849-bab2-40c2e184b95c"
      },
      "source": [
        "%%writefile matrixMul_gpu2.cu\n",
        "#include <stdio.h>\n",
        "#define N 64\n",
        "#define MAX 8\n",
        "\n",
        "__global__ void matrixMultGPU(int *a, int *b, int *c){\n",
        "    int k;\n",
        "    float sum = 0;\n",
        "    int col = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "    int fil = threadIdx.y + blockDim.y * blockIdx.y;\n",
        "    if (col < N && fil < N){\n",
        "        for (k = 0; k < N; k++){\n",
        "            sum += a[fil*N+k] * b[k*N+col];\n",
        "        }\n",
        "        c[fil*N+col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    int a[N][N], b[N][N], c[N][N];\n",
        "    int *dev_a, *dev_b, *dev_c;\n",
        "    int cont, i , j;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    for (i = 0; i < N; i++){\n",
        "        cont = 0;\n",
        "        for (j = 0; j < N; j++){\n",
        "            a[i][j] = cont;\n",
        "            b[i][j] = cont;\n",
        "            cont++;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    int size = N*N*sizeof(int);\n",
        "    cudaMalloc((void **) &dev_a, size);\n",
        "    cudaMalloc((void **) &dev_b, size);\n",
        "    cudaMalloc((void **) &dev_c, size);\n",
        "\n",
        "    cudaMemcpy(dev_a, a, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 dimGrid((N + MAX - 1)/MAX, (N + MAX - 1)/MAX);\n",
        "    dim3 dimBlock(MAX, MAX);\n",
        "\n",
        "    int nIter = 1000;\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    for (int m = 0; m < nIter; m++){\n",
        "        matrixMultGPU<<<dimGrid, dimBlock>>>(dev_a, dev_b, dev_c);\n",
        "    }\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    cudaMemcpy(c, dev_c, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    float msecTotal = 0.0;\n",
        "    cudaEventElapsedTime(&msecTotal, start, stop);\n",
        "\n",
        "    cudaFree(dev_a);\n",
        "    cudaFree(dev_b);\n",
        "    cudaFree(dev_c);\n",
        "\n",
        "    //for (int y = 0; y < N; y++){\n",
        "    //    for (int x = 0; x < N; x++){\n",
        "    //        printf(\"[%d][%d]=%d \", y, x, c[x][y]);\n",
        "    //    }\n",
        "    //    printf(\"\\n\");\n",
        "    //}\n",
        "\n",
        "    float msecPerKernelExecution = msecTotal / nIter;\n",
        "    double flopsPerMMull = 2.0 * N * N * N;\n",
        "    double gigaFlops = (flopsPerMMull * 1.0e-9f) / (msecPerKernelExecution / 1000.0f);\n",
        "\n",
        "    printf(\"GigaFlops: %f\", gigaFlops);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matrixMul_gpu2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxEbuaBF81Hl",
        "outputId": "bb0bf23f-a633-41eb-e8f6-3b822b5ca62a"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true matrixMul_gpu2.cu -o matrixMul_gpu2 -lcudadevrt\n",
        "!nvprof ./matrixMul_gpu2"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==242== NVPROF is profiling process 242, command: ./matrixMul_gpu2\n",
            "==242== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "GigaFlops: 20.692172==242== Profiling application: ./matrixMul_gpu2\n",
            "==242== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.93%  20.558ms      1000  20.558us  20.000us  21.375us  matrixMultGPU(int*, int*, int*)\n",
            "                    0.05%  10.528us         2  5.2640us  5.1840us  5.3440us  [CUDA memcpy HtoD]\n",
            "                    0.02%  4.7670us         1  4.7670us  4.7670us  4.7670us  [CUDA memcpy DtoH]\n",
            "      API calls:   88.59%  202.92ms         2  101.46ms  1.0410us  202.92ms  cudaEventCreate\n",
            "                    8.74%  20.016ms         1  20.016ms  20.016ms  20.016ms  cudaEventSynchronize\n",
            "                    2.15%  4.9243ms      1000  4.9240us  3.9770us  33.042us  cudaLaunchKernel\n",
            "                    0.23%  535.64us         1  535.64us  535.64us  535.64us  cuDeviceTotalMem\n",
            "                    0.08%  192.91us       101  1.9100us     166ns  77.869us  cuDeviceGetAttribute\n",
            "                    0.07%  155.46us         3  51.820us  2.0410us  149.57us  cudaMalloc\n",
            "                    0.06%  140.44us         3  46.814us  2.7760us  129.55us  cudaFree\n",
            "                    0.04%  97.153us         3  32.384us  18.898us  39.198us  cudaMemcpy\n",
            "                    0.01%  26.547us         1  26.547us  26.547us  26.547us  cuDeviceGetName\n",
            "                    0.01%  21.700us         1  21.700us  21.700us  21.700us  cuDeviceGetPCIBusId\n",
            "                    0.01%  11.470us         2  5.7350us  3.4580us  8.0120us  cudaEventRecord\n",
            "                    0.00%  2.8920us         1  2.8920us  2.8920us  2.8920us  cudaEventElapsedTime\n",
            "                    0.00%  2.0560us         3     685ns     192ns  1.0480us  cuDeviceGetCount\n",
            "                    0.00%  1.8180us         2     909ns     542ns  1.2760us  cuDeviceGet\n",
            "                    0.00%     354ns         1     354ns     354ns     354ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Xb4PI9f9jZA"
      },
      "source": [
        "Comparamos el producto de NxN con el uso de memoria compartida y con optimizaciones:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXBalU6W9iFZ",
        "outputId": "e82432e0-ac82-4dc0-8e29-9ae5a2561ca5"
      },
      "source": [
        "%%writefile matrixMul_gpu3.cu\n",
        "#include <stdio.h>\n",
        "#define N 64\n",
        "#define MAX 8\n",
        "\n",
        "__global__ void matrixMultGPU(int *a, int *b, int *c, int n){\n",
        "    int k;\n",
        "    float sum = 0;\n",
        "    int thx = threadIdx.x;\n",
        "    int thy = threadIdx.y;\n",
        "    int col = thx + blockDim.x * blockIdx.x;\n",
        "    int fil = thy + blockDim.y * blockIdx.y;\n",
        "\n",
        "    __shared__ float Aij[MAX][MAX];\n",
        "    __shared__ float Bij[MAX][MAX];\n",
        "\n",
        "    for (int lim = 0; lim < (n + MAX - 1)/MAX; lim++){\n",
        "        if ((thy + (lim*MAX)) < n  && col < n){\n",
        "            Aij[thy][thx] = a[(col*n) + (thy + (lim*MAX))];\n",
        "        }\n",
        "        else {\n",
        "            Aij[thy][thx] = 0.0;\n",
        "        }\n",
        "        if ((thx + (lim*MAX)) < n  && fil < n){\n",
        "            Bij[thy][thx] = b[((thx + (lim*MAX))*n) + fil];\n",
        "        }\n",
        "        else{\n",
        "            Bij[thy][thx] = 0.0;\n",
        "        }\n",
        "        __syncthreads();\n",
        "    #pragma unroll\n",
        "        for (k = 0; k < MAX; k++){\n",
        "            sum += Aij[k][thx] * Bij[thy][k];\n",
        "        }\n",
        "\n",
        "        __syncthreads();\n",
        "    }\n",
        "    if (col < n && fil < n){\n",
        "        c[col * n + fil] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    int a[N][N], b[N][N], c[N][N];\n",
        "    int *dev_a, *dev_b, *dev_c;\n",
        "    int cont, i , j;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    for (i = 0; i < N; i++){\n",
        "        cont = 0;\n",
        "        for (j = 0; j < N; j++){\n",
        "            a[i][j] = cont;\n",
        "            b[i][j] = cont;\n",
        "            cont++;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    int size = N*N*sizeof(int);\n",
        "    cudaMalloc((void **) &dev_a, size);\n",
        "    cudaMalloc((void **) &dev_b, size);\n",
        "    cudaMalloc((void **) &dev_c, size);\n",
        "\n",
        "    cudaMemcpy(dev_a, a, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 dimGrid((N + MAX - 1)/MAX, (N + MAX - 1)/MAX);\n",
        "    dim3 dimBlock(MAX, MAX);\n",
        "\n",
        "    int nIter = 1000;\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    for (int m = 0; m < nIter; m++){\n",
        "        matrixMultGPU<<<dimGrid, dimBlock>>>(dev_a, dev_b, dev_c, N);\n",
        "    }\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    cudaMemcpy(c, dev_c, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    float msecTotal = 0.0;\n",
        "    cudaEventElapsedTime(&msecTotal, start, stop);\n",
        "\n",
        "    cudaFree(dev_a);\n",
        "    cudaFree(dev_b);\n",
        "    cudaFree(dev_c);\n",
        "\n",
        "    //for (int y = 0; y < N; y++){\n",
        "    //    for (int x = 0; x < N; x++){\n",
        "    //        printf(\"[%d][%d]=%d \", y, x, c[x][y]);\n",
        "    //    }\n",
        "    //    printf(\"\\n\");\n",
        "    //}\n",
        "\n",
        "    float msecPerKernelExecution = msecTotal / nIter;\n",
        "    double flopsPerMMull = 2.0 * N * N * N;\n",
        "    double gigaFlops = (flopsPerMMull * 1.0e-9f) / (msecPerKernelExecution / 1000.0f);\n",
        "\n",
        "    printf(\"GigaFlops: %f\", gigaFlops);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing matrixMul_gpu3.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9WNnL2GA1RK",
        "outputId": "8fafd7e7-792f-4c87-94b3-71d0612cebab"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true matrixMul_gpu3.cu -o matrixMul_gpu3 -lcudadevrt\n",
        "!nvprof ./matrixMul_gpu3"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==288== NVPROF is profiling process 288, command: ./matrixMul_gpu3\n",
            "==288== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "==288== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
            "GigaFlops: 24.461508==288== Profiling application: ./matrixMul_gpu3\n",
            "==288== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.84%  16.283ms      1000  16.282us  15.936us  17.888us  matrixMultGPU(int*, int*, int*, int)\n",
            "                    0.13%  21.120us         2  10.560us  5.3440us  15.776us  [CUDA memcpy HtoD]\n",
            "                    0.03%  4.6720us         1  4.6720us  4.6720us  4.6720us  [CUDA memcpy DtoH]\n",
            "      API calls:   89.96%  200.06ms         2  100.03ms  1.0310us  200.06ms  cudaEventCreate\n",
            "                    7.18%  15.971ms         1  15.971ms  15.971ms  15.971ms  cudaEventSynchronize\n",
            "                    2.29%  5.0857ms      1000  5.0850us  4.1660us  77.029us  cudaLaunchKernel\n",
            "                    0.26%  576.67us         1  576.67us  576.67us  576.67us  cuDeviceTotalMem\n",
            "                    0.11%  253.07us       101  2.5050us     171ns  105.71us  cuDeviceGetAttribute\n",
            "                    0.07%  147.90us         3  49.299us  2.3170us  141.45us  cudaMalloc\n",
            "                    0.07%  146.61us         3  48.871us  2.6980us  135.84us  cudaFree\n",
            "                    0.05%  100.95us         3  33.649us  23.512us  39.614us  cudaMemcpy\n",
            "                    0.01%  26.708us         1  26.708us  26.708us  26.708us  cuDeviceGetName\n",
            "                    0.00%  10.998us         2  5.4990us  3.5240us  7.4740us  cudaEventRecord\n",
            "                    0.00%  5.6800us         1  5.6800us  5.6800us  5.6800us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.7140us         1  2.7140us  2.7140us  2.7140us  cudaEventElapsedTime\n",
            "                    0.00%  2.3700us         3     790ns     240ns  1.1640us  cuDeviceGetCount\n",
            "                    0.00%  1.6700us         2     835ns     440ns  1.2300us  cuDeviceGet\n",
            "                    0.00%     423ns         1     423ns     423ns     423ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agd10FrQB2vQ"
      },
      "source": [
        "### Resultados de la ejecución de multiplicación de matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5Kk6Y4wB--U"
      },
      "source": [
        "Sin memoria compartida:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPZSCOh2B2H7",
        "outputId": "df4bd8b7-27bc-4a48-fee9-c36723b227b6"
      },
      "source": [
        "!for i in 16 32 64 128 512; do sed -i \"/#define N/c\\#define N $i\" matrixMul_gpu2.cu && /usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true matrixMul_gpu2.cu -o matrixMul_gpu2 -lcudadevrt && nvprof ./matrixMul_gpu2; done"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==336== NVPROF is profiling process 336, command: ./matrixMul_gpu2\n",
            "==336== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "GigaFlops: 0.783968==336== Profiling application: ./matrixMul_gpu2\n",
            "==336== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.87%  5.3743ms      1000  5.3740us  5.2160us  12.224us  matrixMultGPU(int*, int*, int*)\n",
            "                    0.08%  4.2240us         2  2.1120us  1.7600us  2.4640us  [CUDA memcpy HtoD]\n",
            "                    0.05%  2.5280us         1  2.5280us  2.5280us  2.5280us  [CUDA memcpy DtoH]\n",
            "      API calls:   94.60%  194.28ms         2  97.138ms  1.0190us  194.27ms  cudaEventCreate\n",
            "                    2.55%  5.2283ms         1  5.2283ms  5.2283ms  5.2283ms  cudaEventSynchronize\n",
            "                    2.31%  4.7351ms      1000  4.7350us  4.0320us  26.309us  cudaLaunchKernel\n",
            "                    0.26%  529.53us         1  529.53us  529.53us  529.53us  cuDeviceTotalMem\n",
            "                    0.09%  187.41us       101  1.8550us     159ns  77.419us  cuDeviceGetAttribute\n",
            "                    0.08%  165.46us         3  55.153us  2.1970us  159.56us  cudaMalloc\n",
            "                    0.06%  124.94us         3  41.646us  2.8500us  114.57us  cudaFree\n",
            "                    0.03%  55.384us         3  18.461us  11.475us  26.743us  cudaMemcpy\n",
            "                    0.02%  31.287us         1  31.287us  31.287us  31.287us  cuDeviceGetPCIBusId\n",
            "                    0.01%  23.319us         1  23.319us  23.319us  23.319us  cuDeviceGetName\n",
            "                    0.00%  7.0770us         2  3.5380us  3.0700us  4.0070us  cudaEventRecord\n",
            "                    0.00%  2.3300us         1  2.3300us  2.3300us  2.3300us  cudaEventElapsedTime\n",
            "                    0.00%  1.7930us         3     597ns     216ns  1.0380us  cuDeviceGetCount\n",
            "                    0.00%  1.4140us         2     707ns     231ns  1.1830us  cuDeviceGet\n",
            "                    0.00%     352ns         1     352ns     352ns     352ns  cuDeviceGetUuid\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==381== NVPROF is profiling process 381, command: ./matrixMul_gpu2\n",
            "==381== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "GigaFlops: 4.862944==381== Profiling application: ./matrixMul_gpu2\n",
            "==381== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.91%  8.9480ms      1000  8.9480us  8.8310us  12.096us  matrixMultGPU(int*, int*, int*)\n",
            "                    0.06%  5.2160us         2  2.6080us  2.4320us  2.7840us  [CUDA memcpy HtoD]\n",
            "                    0.03%  2.9120us         1  2.9120us  2.9120us  2.9120us  [CUDA memcpy DtoH]\n",
            "      API calls:   93.09%  191.74ms         2  95.870ms  1.6750us  191.74ms  cudaEventCreate\n",
            "                    4.05%  8.3327ms         1  8.3327ms  8.3327ms  8.3327ms  cudaEventSynchronize\n",
            "                    2.31%  4.7601ms      1000  4.7600us  3.7600us  33.242us  cudaLaunchKernel\n",
            "                    0.26%  534.08us         1  534.08us  534.08us  534.08us  cuDeviceTotalMem\n",
            "                    0.08%  167.09us         3  55.695us  2.6870us  156.21us  cudaFree\n",
            "                    0.08%  165.80us       101  1.6410us     156ns  67.979us  cuDeviceGetAttribute\n",
            "                    0.08%  158.74us         3  52.913us  2.3680us  152.59us  cudaMalloc\n",
            "                    0.04%  72.111us         3  24.037us  13.086us  34.589us  cudaMemcpy\n",
            "                    0.01%  22.862us         1  22.862us  22.862us  22.862us  cuDeviceGetName\n",
            "                    0.00%  8.1790us         2  4.0890us  3.7970us  4.3820us  cudaEventRecord\n",
            "                    0.00%  7.1080us         1  7.1080us  7.1080us  7.1080us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.5750us         1  2.5750us  2.5750us  2.5750us  cudaEventElapsedTime\n",
            "                    0.00%  2.0990us         3     699ns     194ns  1.2830us  cuDeviceGetCount\n",
            "                    0.00%  1.2660us         2     633ns     328ns     938ns  cuDeviceGet\n",
            "                    0.00%     321ns         1     321ns     321ns     321ns  cuDeviceGetUuid\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==426== NVPROF is profiling process 426, command: ./matrixMul_gpu2\n",
            "==426== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "GigaFlops: 20.657082==426== Profiling application: ./matrixMul_gpu2\n",
            "==426== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.93%  20.554ms      1000  20.553us  19.935us  21.568us  matrixMultGPU(int*, int*, int*)\n",
            "                    0.05%  10.656us         2  5.3280us  5.3120us  5.3440us  [CUDA memcpy HtoD]\n",
            "                    0.02%  4.6720us         1  4.6720us  4.6720us  4.6720us  [CUDA memcpy DtoH]\n",
            "      API calls:   87.28%  180.24ms         2  90.119ms  1.0230us  180.24ms  cudaEventCreate\n",
            "                    9.68%  19.992ms         1  19.992ms  19.992ms  19.992ms  cudaEventSynchronize\n",
            "                    2.43%  5.0247ms      1000  5.0240us  3.8560us  36.984us  cudaLaunchKernel\n",
            "                    0.26%  546.99us         1  546.99us  546.99us  546.99us  cuDeviceTotalMem\n",
            "                    0.10%  207.23us         3  69.075us  2.2440us  200.97us  cudaMalloc\n",
            "                    0.09%  179.86us       101  1.7800us     151ns  67.076us  cuDeviceGetAttribute\n",
            "                    0.07%  144.09us         3  48.030us  2.9760us  126.90us  cudaFree\n",
            "                    0.05%  110.25us         3  36.750us  18.799us  70.499us  cudaMemcpy\n",
            "                    0.02%  37.957us         1  37.957us  37.957us  37.957us  cuDeviceGetName\n",
            "                    0.00%  7.9370us         2  3.9680us  3.6530us  4.2840us  cudaEventRecord\n",
            "                    0.00%  6.6820us         1  6.6820us  6.6820us  6.6820us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.5840us         1  2.5840us  2.5840us  2.5840us  cudaEventElapsedTime\n",
            "                    0.00%  1.9090us         3     636ns     210ns     900ns  cuDeviceGetCount\n",
            "                    0.00%  1.6560us         2     828ns     321ns  1.3350us  cuDeviceGet\n",
            "                    0.00%     288ns         1     288ns     288ns     288ns  cuDeviceGetUuid\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==471== NVPROF is profiling process 471, command: ./matrixMul_gpu2\n",
            "==471== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "GigaFlops: 35.226710==471== Profiling application: ./matrixMul_gpu2\n",
            "==471== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.93%  114.01ms      1000  114.01us  112.41us  124.13us  matrixMultGPU(int*, int*, int*)\n",
            "                    0.06%  62.944us         2  31.472us  16.352us  46.592us  [CUDA memcpy HtoD]\n",
            "                    0.01%  11.296us         1  11.296us  11.296us  11.296us  [CUDA memcpy DtoH]\n",
            "      API calls:   60.38%  182.98ms         2  91.490ms     896ns  182.98ms  cudaEventCreate\n",
            "                   37.54%  113.76ms         1  113.76ms  113.76ms  113.76ms  cudaEventSynchronize\n",
            "                    1.63%  4.9316ms      1000  4.9310us  3.9650us  47.557us  cudaLaunchKernel\n",
            "                    0.20%  601.54us         1  601.54us  601.54us  601.54us  cuDeviceTotalMem\n",
            "                    0.06%  192.61us         3  64.202us  33.065us  82.497us  cudaMemcpy\n",
            "                    0.06%  190.19us         3  63.395us  2.2130us  181.22us  cudaMalloc\n",
            "                    0.06%  168.02us         3  56.005us  3.0680us  154.07us  cudaFree\n",
            "                    0.05%  164.33us       101  1.6270us     148ns  66.994us  cuDeviceGetAttribute\n",
            "                    0.01%  25.540us         2  12.770us  3.3290us  22.211us  cudaEventRecord\n",
            "                    0.01%  22.123us         1  22.123us  22.123us  22.123us  cuDeviceGetName\n",
            "                    0.00%  7.5640us         1  7.5640us  7.5640us  7.5640us  cuDeviceGetPCIBusId\n",
            "                    0.00%  3.4600us         1  3.4600us  3.4600us  3.4600us  cudaEventElapsedTime\n",
            "                    0.00%  1.9200us         3     640ns     201ns  1.0650us  cuDeviceGetCount\n",
            "                    0.00%  1.5990us         2     799ns     216ns  1.3830us  cuDeviceGet\n",
            "                    0.00%     318ns         1     318ns     318ns     318ns  cuDeviceGetUuid\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==516== NVPROF is profiling process 516, command: ./matrixMul_gpu2\n",
            "==516== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "GigaFlops: 61.576186==516== Profiling application: ./matrixMul_gpu2\n",
            "==516== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.99%  4.35480s      1000  4.3548ms  4.1878ms  6.5119ms  matrixMultGPU(int*, int*, int*)\n",
            "                    0.01%  338.04us         2  169.02us  142.91us  195.13us  [CUDA memcpy HtoD]\n",
            "                    0.00%  88.031us         1  88.031us  88.031us  88.031us  [CUDA memcpy DtoH]\n",
            "      API calls:   95.75%  4.35449s         1  4.35449s  4.35449s  4.35449s  cudaEventSynchronize\n",
            "                    4.09%  186.04ms         2  93.018ms     993ns  186.04ms  cudaEventCreate\n",
            "                    0.10%  4.5900ms      1000  4.5900us  3.8660us  25.306us  cudaLaunchKernel\n",
            "                    0.03%  1.2684ms         3  422.79us  99.542us  879.53us  cudaMemcpy\n",
            "                    0.01%  515.25us         1  515.25us  515.25us  515.25us  cuDeviceTotalMem\n",
            "                    0.01%  470.17us         3  156.72us  133.92us  195.54us  cudaFree\n",
            "                    0.01%  375.15us         3  125.05us  112.24us  144.61us  cudaMalloc\n",
            "                    0.00%  166.87us       101  1.6520us     155ns  68.463us  cuDeviceGetAttribute\n",
            "                    0.00%  48.390us         1  48.390us  48.390us  48.390us  cuDeviceGetName\n",
            "                    0.00%  8.5430us         2  4.2710us  3.2530us  5.2900us  cudaEventRecord\n",
            "                    0.00%  7.1450us         1  7.1450us  7.1450us  7.1450us  cuDeviceGetPCIBusId\n",
            "                    0.00%  5.2710us         1  5.2710us  5.2710us  5.2710us  cudaEventElapsedTime\n",
            "                    0.00%  2.1440us         3     714ns     211ns  1.0460us  cuDeviceGetCount\n",
            "                    0.00%  1.4340us         2     717ns     437ns     997ns  cuDeviceGet\n",
            "                    0.00%     314ns         1     314ns     314ns     314ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w591hNMTCROP"
      },
      "source": [
        "<style type=\"text/css\">\n",
        ".tg  {border-collapse:collapse;border-spacing:0;}\n",
        ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
        "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
        ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
        "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
        ".tg .tg-n2k3{background-color:#FFF;color:#000000;font-family:\"Palatino Linotype\", \"Book Antiqua\", Palatino, serif !important;;\n",
        "  text-align:center;vertical-align:top}\n",
        ".tg .tg-6kwm{background-color:#FFF;border-color:inherit;color:#000000;\n",
        "  font-family:\"Palatino Linotype\", \"Book Antiqua\", Palatino, serif !important;;text-align:center;vertical-align:top}\n",
        ".tg .tg-bifh{background-color:#FFF;color:#000000;font-family:\"Palatino Linotype\", \"Book Antiqua\", Palatino, serif !important;;\n",
        "  text-align:center;vertical-align:middle}\n",
        ".tg .tg-h01i{background-color:#FFF;border-color:inherit;color:#000000;\n",
        "  font-family:\"Palatino Linotype\", \"Book Antiqua\", Palatino, serif !important;;text-align:center;vertical-align:middle}\n",
        "</style>\n",
        "<table class=\"tg\">\n",
        "<thead>\n",
        "  <tr>\n",
        "    <th class=\"tg-6kwm\">Sin usar memoria compartida</th>\n",
        "    <th class=\"tg-6kwm\" colspan=\"3\">Tiempo de Ejecución (msec)</th>\n",
        "    <th class=\"tg-h01i\"></th>\n",
        "    <th class=\"tg-bifh\"></th>\n",
        "  </tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "  <tr>\n",
        "    <td class=\"tg-6kwm\">Tamaño de la matriz</td>\n",
        "    <td class=\"tg-6kwm\">CPU- &gt;GPU</td>\n",
        "    <td class=\"tg-6kwm\">GPU- &gt;CPU</td>\n",
        "    <td class=\"tg-6kwm\">Ejecución kernel</td>\n",
        "    <td class=\"tg-6kwm\">Ratio comparado con 128x128</td>\n",
        "    <td class=\"tg-n2k3\">GFLOPs</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-6kwm\">16x16</td>\n",
        "    <td class=\"tg-h01i\">0.004</td>\n",
        "    <td class=\"tg-h01i\">0.002</td>\n",
        "    <td class=\"tg-h01i\">5.374</td>\n",
        "    <td class=\"tg-h01i\">0.047</td>\n",
        "    <td class=\"tg-bifh\">0.784</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-6kwm\">32x32</td>\n",
        "    <td class=\"tg-h01i\">0.005</td>\n",
        "    <td class=\"tg-h01i\">0.002</td>\n",
        "    <td class=\"tg-h01i\">8.948</td>\n",
        "    <td class=\"tg-h01i\">0.078</td>\n",
        "    <td class=\"tg-bifh\">4.863</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-n2k3\">64x64</td>\n",
        "    <td class=\"tg-bifh\">0.011</td>\n",
        "    <td class=\"tg-bifh\">0.005</td>\n",
        "    <td class=\"tg-bifh\">20.553</td>\n",
        "    <td class=\"tg-bifh\">0.180</td>\n",
        "    <td class=\"tg-bifh\">20.657</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-n2k3\">128x128</td>\n",
        "    <td class=\"tg-bifh\">0.063</td>\n",
        "    <td class=\"tg-bifh\">0.012</td>\n",
        "    <td class=\"tg-bifh\">114.010</td>\n",
        "    <td class=\"tg-n2k3\">1</td>\n",
        "    <td class=\"tg-bifh\">35.227</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-n2k3\">512x512</td>\n",
        "    <td class=\"tg-bifh\">0.338</td>\n",
        "    <td class=\"tg-bifh\">0.088</td>\n",
        "    <td class=\"tg-bifh\">4354.800</td>\n",
        "    <td class=\"tg-bifh\">38.167</td>\n",
        "    <td class=\"tg-bifh\">61.576</td>\n",
        "  </tr>\n",
        "</tbody>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaB6NAsHCDhd"
      },
      "source": [
        "Con memoria compartida:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6ce9CdTBZAL",
        "outputId": "a83a44d3-5368-4653-828b-71f38a67c165"
      },
      "source": [
        "!for i in 16 32 64 128 512; do sed -i \"/#define N/c\\#define N $i\" matrixMul_gpu3.cu && /usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true matrixMul_gpu3.cu -o matrixMul_gpu3 -lcudadevrt && nvprof ./matrixMul_gpu3; done"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==564== NVPROF is profiling process 564, command: ./matrixMul_gpu3\n",
            "==564== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "==564== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
            "GigaFlops: 0.842624==564== Profiling application: ./matrixMul_gpu3\n",
            "==564== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.79%  3.0111ms      1000  3.0110us  2.9750us  4.6080us  matrixMultGPU(int*, int*, int*, int)\n",
            "                    0.14%  4.2560us         2  2.1280us  1.4720us  2.7840us  [CUDA memcpy HtoD]\n",
            "                    0.07%  1.9840us         1  1.9840us  1.9840us  1.9840us  [CUDA memcpy DtoH]\n",
            "      API calls:   94.75%  183.74ms         2  91.870ms  1.4990us  183.74ms  cudaEventCreate\n",
            "                    4.64%  8.9913ms      1000  8.9910us  4.6430us  243.68us  cudaLaunchKernel\n",
            "                    0.26%  499.15us         1  499.15us  499.15us  499.15us  cuDeviceTotalMem\n",
            "                    0.10%  196.99us         3  65.663us  3.7850us  186.80us  cudaMalloc\n",
            "                    0.10%  187.83us         3  62.609us  2.7780us  161.97us  cudaFree\n",
            "                    0.08%  161.74us       101  1.6010us     161ns  66.433us  cuDeviceGetAttribute\n",
            "                    0.04%  73.751us         3  24.583us  12.389us  38.072us  cudaMemcpy\n",
            "                    0.01%  27.150us         1  27.150us  27.150us  27.150us  cuDeviceGetName\n",
            "                    0.01%  15.931us         2  7.9650us  5.4870us  10.444us  cudaEventRecord\n",
            "                    0.00%  7.7970us         1  7.7970us  7.7970us  7.7970us  cudaEventSynchronize\n",
            "                    0.00%  5.6470us         1  5.6470us  5.6470us  5.6470us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.5320us         1  2.5320us  2.5320us  2.5320us  cudaEventElapsedTime\n",
            "                    0.00%  2.1370us         3     712ns     216ns  1.1170us  cuDeviceGetCount\n",
            "                    0.00%  1.5640us         2     782ns     487ns  1.0770us  cuDeviceGet\n",
            "                    0.00%     262ns         1     262ns     262ns     262ns  cuDeviceGetUuid\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==609== NVPROF is profiling process 609, command: ./matrixMul_gpu3\n",
            "==609== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "==609== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
            "GigaFlops: 7.148317==609== Profiling application: ./matrixMul_gpu3\n",
            "==609== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.88%  5.4371ms      1000  5.4370us  5.2470us  12.832us  matrixMultGPU(int*, int*, int*, int)\n",
            "                    0.08%  4.1920us         2  2.0960us  1.9200us  2.2720us  [CUDA memcpy HtoD]\n",
            "                    0.04%  2.2400us         1  2.2400us  2.2400us  2.2400us  [CUDA memcpy DtoH]\n",
            "      API calls:   94.88%  182.05ms         2  91.027ms     836ns  182.05ms  cudaEventCreate\n",
            "                    2.61%  5.0176ms      1000  5.0170us  4.1290us  50.397us  cudaLaunchKernel\n",
            "                    1.92%  3.6894ms         1  3.6894ms  3.6894ms  3.6894ms  cudaEventSynchronize\n",
            "                    0.26%  493.73us         1  493.73us  493.73us  493.73us  cuDeviceTotalMem\n",
            "                    0.09%  181.35us         3  60.449us  2.1750us  175.26us  cudaMalloc\n",
            "                    0.09%  165.22us       101  1.6350us     168ns  68.212us  cuDeviceGetAttribute\n",
            "                    0.08%  162.27us         3  54.090us  5.1530us  146.70us  cudaFree\n",
            "                    0.03%  65.108us         3  21.702us  10.386us  33.700us  cudaMemcpy\n",
            "                    0.01%  23.172us         1  23.172us  23.172us  23.172us  cuDeviceGetName\n",
            "                    0.01%  9.6220us         2  4.8110us  3.3150us  6.3070us  cudaEventRecord\n",
            "                    0.00%  7.7340us         1  7.7340us  7.7340us  7.7340us  cuDeviceGetPCIBusId\n",
            "                    0.00%  4.4950us         1  4.4950us  4.4950us  4.4950us  cudaEventElapsedTime\n",
            "                    0.00%  2.0330us         3     677ns     240ns  1.2480us  cuDeviceGetCount\n",
            "                    0.00%  1.2580us         2     629ns     360ns     898ns  cuDeviceGet\n",
            "                    0.00%     308ns         1     308ns     308ns     308ns  cuDeviceGetUuid\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==654== NVPROF is profiling process 654, command: ./matrixMul_gpu3\n",
            "==654== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "==654== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
            "GigaFlops: 35.739217==654== Profiling application: ./matrixMul_gpu3\n",
            "==654== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.90%  10.539ms      1000  10.538us  10.304us  11.775us  matrixMultGPU(int*, int*, int*, int)\n",
            "                    0.07%  7.4560us         2  3.7280us  3.6480us  3.8080us  [CUDA memcpy HtoD]\n",
            "                    0.03%  3.3600us         1  3.3600us  3.3600us  3.3600us  [CUDA memcpy DtoH]\n",
            "      API calls:   92.71%  197.04ms         2  98.522ms  1.1170us  197.04ms  cudaEventCreate\n",
            "                    4.38%  9.3017ms         1  9.3017ms  9.3017ms  9.3017ms  cudaEventSynchronize\n",
            "                    2.36%  5.0165ms      1000  5.0160us  3.9850us  62.524us  cudaLaunchKernel\n",
            "                    0.26%  550.42us         1  550.42us  550.42us  550.42us  cuDeviceTotalMem\n",
            "                    0.09%  188.01us       101  1.8610us     161ns  80.696us  cuDeviceGetAttribute\n",
            "                    0.08%  176.44us         3  58.811us  2.2220us  151.54us  cudaMalloc\n",
            "                    0.06%  131.46us         3  43.821us  2.9620us  120.26us  cudaFree\n",
            "                    0.04%  76.341us         3  25.447us  15.860us  37.656us  cudaMemcpy\n",
            "                    0.01%  28.257us         1  28.257us  28.257us  28.257us  cuDeviceGetName\n",
            "                    0.00%  7.8310us         2  3.9150us  3.5430us  4.2880us  cudaEventRecord\n",
            "                    0.00%  5.8800us         1  5.8800us  5.8800us  5.8800us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.5970us         3     865ns     273ns  1.4070us  cuDeviceGetCount\n",
            "                    0.00%  2.5620us         1  2.5620us  2.5620us  2.5620us  cudaEventElapsedTime\n",
            "                    0.00%  1.6600us         2     830ns     522ns  1.1380us  cuDeviceGet\n",
            "                    0.00%     382ns         1     382ns     382ns     382ns  cuDeviceGetUuid\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==701== NVPROF is profiling process 701, command: ./matrixMul_gpu3\n",
            "==701== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "==701== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
            "GigaFlops: 76.449020==701== Profiling application: ./matrixMul_gpu3\n",
            "==701== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.94%  50.631ms      1000  50.630us  48.255us  53.215us  matrixMultGPU(int*, int*, int*, int)\n",
            "                    0.04%  21.503us         2  10.751us  10.687us  10.816us  [CUDA memcpy HtoD]\n",
            "                    0.02%  7.6800us         1  7.6800us  7.6800us  7.6800us  [CUDA memcpy DtoH]\n",
            "      API calls:   76.42%  180.35ms         2  90.176ms  1.3290us  180.35ms  cudaEventCreate\n",
            "                   17.96%  42.374ms         1  42.374ms  42.374ms  42.374ms  cudaEventSynchronize\n",
            "                    5.01%  11.825ms      1000  11.824us  4.1010us  1.3693ms  cudaLaunchKernel\n",
            "                    0.22%  529.43us         1  529.43us  529.43us  529.43us  cuDeviceTotalMem\n",
            "                    0.12%  288.14us         3  96.047us  36.336us  129.83us  cudaMemcpy\n",
            "                    0.10%  237.29us         3  79.097us  3.6980us  227.12us  cudaMalloc\n",
            "                    0.08%  190.55us       101  1.8860us     155ns  91.487us  cuDeviceGetAttribute\n",
            "                    0.06%  151.37us         3  50.455us  3.0030us  137.91us  cudaFree\n",
            "                    0.01%  24.928us         1  24.928us  24.928us  24.928us  cuDeviceGetName\n",
            "                    0.01%  12.506us         2  6.2530us  5.6290us  6.8770us  cudaEventRecord\n",
            "                    0.00%  5.7650us         1  5.7650us  5.7650us  5.7650us  cuDeviceGetPCIBusId\n",
            "                    0.00%  3.3580us         1  3.3580us  3.3580us  3.3580us  cudaEventElapsedTime\n",
            "                    0.00%  2.5760us         3     858ns     245ns  1.3160us  cuDeviceGetCount\n",
            "                    0.00%  1.5750us         2     787ns     433ns  1.1420us  cuDeviceGet\n",
            "                    0.00%     322ns         1     322ns     322ns     322ns  cuDeviceGetUuid\n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "==746== NVPROF is profiling process 746, command: ./matrixMul_gpu3\n",
            "==746== Warning: Auto boost enabled on device 0. Profiling results may be inconsistent.\n",
            "==746== Warning: Profiling results might be incorrect with current version of nvcc compiler used to compile cuda app. Compile with nvcc compiler 9.0 or later version to get correct profiling results. Ignore this warning if code is already compiled with the recommended nvcc version \n",
            "GigaFlops: 101.834072==746== Profiling application: ./matrixMul_gpu3\n",
            "==746== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.99%  2.63154s      1000  2.6315ms  2.6218ms  2.6430ms  matrixMultGPU(int*, int*, int*, int)\n",
            "                    0.01%  183.96us         2  91.982us  91.966us  91.998us  [CUDA memcpy HtoD]\n",
            "                    0.00%  101.85us         1  101.85us  101.85us  101.85us  [CUDA memcpy DtoH]\n",
            "      API calls:   93.34%  2.63098s         1  2.63098s  2.63098s  2.63098s  cudaEventSynchronize\n",
            "                    6.40%  180.33ms         2  90.163ms  1.0660us  180.32ms  cudaEventCreate\n",
            "                    0.17%  4.7176ms      1000  4.7170us  3.7260us  58.545us  cudaLaunchKernel\n",
            "                    0.04%  1.0633ms         3  354.42us  101.03us  781.98us  cudaMemcpy\n",
            "                    0.02%  482.91us         1  482.91us  482.91us  482.91us  cuDeviceTotalMem\n",
            "                    0.02%  454.31us         3  151.44us  137.43us  177.99us  cudaFree\n",
            "                    0.01%  399.52us         3  133.17us  104.16us  181.05us  cudaMalloc\n",
            "                    0.01%  161.71us       101  1.6010us     156ns  67.017us  cuDeviceGetAttribute\n",
            "                    0.00%  22.063us         1  22.063us  22.063us  22.063us  cuDeviceGetName\n",
            "                    0.00%  7.8530us         1  7.8530us  7.8530us  7.8530us  cuDeviceGetPCIBusId\n",
            "                    0.00%  7.8330us         2  3.9160us  3.2640us  4.5690us  cudaEventRecord\n",
            "                    0.00%  4.8700us         1  4.8700us  4.8700us  4.8700us  cudaEventElapsedTime\n",
            "                    0.00%  1.7970us         3     599ns     176ns  1.0070us  cuDeviceGetCount\n",
            "                    0.00%  1.3220us         2     661ns     215ns  1.1070us  cuDeviceGet\n",
            "                    0.00%     272ns         1     272ns     272ns     272ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApWeMA7SGZDN"
      },
      "source": [
        "<style type=\"text/css\">\n",
        ".tg  {border-collapse:collapse;border-spacing:0;}\n",
        ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
        "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
        ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
        "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
        ".tg .tg-n2k3{background-color:#FFF;color:#000000;font-family:\"Palatino Linotype\", \"Book Antiqua\", Palatino, serif !important;;\n",
        "  text-align:center;vertical-align:top}\n",
        ".tg .tg-6kwm{background-color:#FFF;border-color:inherit;color:#000000;\n",
        "  font-family:\"Palatino Linotype\", \"Book Antiqua\", Palatino, serif !important;;text-align:center;vertical-align:top}\n",
        ".tg .tg-bifh{background-color:#FFF;color:#000000;font-family:\"Palatino Linotype\", \"Book Antiqua\", Palatino, serif !important;;\n",
        "  text-align:center;vertical-align:middle}\n",
        ".tg .tg-h01i{background-color:#FFF;border-color:inherit;color:#000000;\n",
        "  font-family:\"Palatino Linotype\", \"Book Antiqua\", Palatino, serif !important;;text-align:center;vertical-align:middle}\n",
        "</style>\n",
        "<table class=\"tg\">\n",
        "<thead>\n",
        "  <tr>\n",
        "    <th class=\"tg-6kwm\">Con memoria compartida</th>\n",
        "    <th class=\"tg-6kwm\" colspan=\"3\">Tiempo de Ejecución (msec)</th>\n",
        "    <th class=\"tg-h01i\"></th>\n",
        "    <th class=\"tg-bifh\"></th>\n",
        "  </tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "  <tr>\n",
        "    <td class=\"tg-6kwm\">Tamaño de la matriz</td>\n",
        "    <td class=\"tg-6kwm\">CPU- &gt;GPU</td>\n",
        "    <td class=\"tg-6kwm\">GPU- &gt;CPU</td>\n",
        "    <td class=\"tg-6kwm\">Ejecución kernel</td>\n",
        "    <td class=\"tg-6kwm\">Ratio comparado con 128x128</td>\n",
        "    <td class=\"tg-n2k3\">GFLOPs</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-6kwm\">16x16</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-6kwm\">32x32</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-h01i\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-n2k3\">64x64</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-n2k3\">128x128</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-n2k3\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td class=\"tg-n2k3\">512x512</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "    <td class=\"tg-bifh\">-</td>\n",
        "  </tr>\n",
        "</tbody>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv0tV9ofCG62"
      },
      "source": [
        "# MÁS: Revisión de los ejemplos instalados con CUDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mq9bEvZ6JUpq",
        "outputId": "9f08cf0e-7fef-456e-ae8f-01acfcd72833"
      },
      "source": [
        "%cd /usr/local/cuda/samples\n",
        "%ls"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/cuda-11.1/samples\n",
            "\u001b[0m\u001b[01;34m0_Simple\u001b[0m/     \u001b[01;34m2_Graphics\u001b[0m/  \u001b[01;34m4_Finance\u001b[0m/      \u001b[01;34m6_Advanced\u001b[0m/       \u001b[01;34mbin\u001b[0m/     EULA.txt\n",
            "\u001b[01;34m1_Utilities\u001b[0m/  \u001b[01;34m3_Imaging\u001b[0m/   \u001b[01;34m5_Simulations\u001b[0m/  \u001b[01;34m7_CUDALibraries\u001b[0m/  \u001b[01;34mcommon\u001b[0m/  Makefile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tR1tfxYzJeVc",
        "outputId": "33a8484c-240d-4223-df8b-09cdece15880"
      },
      "source": [
        "%cd  0_Simple/matrixMul/\n",
        "%ls\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/cuda-11.1/samples/0_Simple/matrixMul\n",
            "Makefile  matrixMul.cu  NsightEclipse.xml  readme.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "m485XaWJJ43M",
        "outputId": "ae112cb5-7e1a-4d3f-be5b-61b12568c35a"
      },
      "source": [
        "%pwd\n",
        "\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/usr/local/cuda-11.1/samples/0_Simple/matrixMul'"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2AAxHSoKX_K",
        "outputId": "08131680-e388-44ce-ebec-05c92491622d"
      },
      "source": [
        "!cat matrixMul.cu\n",
        "\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/**\n",
            " * Copyright 1993-2015 NVIDIA Corporation.  All rights reserved.\n",
            " *\n",
            " * Please refer to the NVIDIA end user license agreement (EULA) associated\n",
            " * with this source code for terms and conditions that govern your use of\n",
            " * this software. Any use, reproduction, disclosure, or distribution of\n",
            " * this software and related documentation outside the terms of the EULA\n",
            " * is strictly prohibited.\n",
            " *\n",
            " */\n",
            "\n",
            "/**\n",
            " * Matrix multiplication: C = A * B.\n",
            " * Host code.\n",
            " *\n",
            " * This sample implements matrix multiplication which makes use of shared memory\n",
            " * to ensure data reuse, the matrix multiplication is done using tiling approach.\n",
            " * It has been written for clarity of exposition to illustrate various CUDA programming\n",
            " * principles, not with the goal of providing the most performant generic kernel for matrix multiplication.\n",
            " * See also:\n",
            " * V. Volkov and J. Demmel, \"Benchmarking GPUs to tune dense linear algebra,\"\n",
            " * in Proc. 2008 ACM/IEEE Conf. on Supercomputing (SC '08),\n",
            " * Piscataway, NJ: IEEE Press, 2008, pp. Art. 31:1-11.\n",
            " */\n",
            "\n",
            "// System includes\n",
            "#include <stdio.h>\n",
            "#include <assert.h>\n",
            "\n",
            "// CUDA runtime\n",
            "#include <cuda_runtime.h>\n",
            "\n",
            "// Helper functions and utilities to work with CUDA\n",
            "#include <helper_functions.h>\n",
            "#include <helper_cuda.h>\n",
            "\n",
            "/**\n",
            " * Matrix multiplication (CUDA Kernel) on the device: C = A * B\n",
            " * wA is A's width and wB is B's width\n",
            " */\n",
            "template <int BLOCK_SIZE> __global__ void MatrixMulCUDA(float *C, float *A,\n",
            "                                                        float *B, int wA,\n",
            "                                                        int wB) {\n",
            "    // Block index\n",
            "    int bx = blockIdx.x;\n",
            "    int by = blockIdx.y;\n",
            "\n",
            "    // Thread index\n",
            "    int tx = threadIdx.x;\n",
            "    int ty = threadIdx.y;\n",
            "\n",
            "    // Index of the first sub-matrix of A processed by the block\n",
            "    int aBegin = wA * BLOCK_SIZE * by;\n",
            "\n",
            "    // Index of the last sub-matrix of A processed by the block\n",
            "    int aEnd   = aBegin + wA - 1;\n",
            "\n",
            "    // Step size used to iterate through the sub-matrices of A\n",
            "    int aStep  = BLOCK_SIZE;\n",
            "\n",
            "    // Index of the first sub-matrix of B processed by the block\n",
            "    int bBegin = BLOCK_SIZE * bx;\n",
            "\n",
            "    // Step size used to iterate through the sub-matrices of B\n",
            "    int bStep  = BLOCK_SIZE * wB;\n",
            "\n",
            "    // Csub is used to store the element of the block sub-matrix\n",
            "    // that is computed by the thread\n",
            "    float Csub = 0;\n",
            "\n",
            "    // Loop over all the sub-matrices of A and B\n",
            "    // required to compute the block sub-matrix\n",
            "    for (int a = aBegin, b = bBegin;\n",
            "            a <= aEnd;\n",
            "            a += aStep, b += bStep) {\n",
            "        // Declaration of the shared memory array As used to\n",
            "        // store the sub-matrix of A\n",
            "        __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];\n",
            "\n",
            "        // Declaration of the shared memory array Bs used to\n",
            "        // store the sub-matrix of B\n",
            "        __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n",
            "\n",
            "        // Load the matrices from device memory\n",
            "        // to shared memory; each thread loads\n",
            "        // one element of each matrix\n",
            "        As[ty][tx] = A[a + wA * ty + tx];\n",
            "        Bs[ty][tx] = B[b + wB * ty + tx];\n",
            "\n",
            "        // Synchronize to make sure the matrices are loaded\n",
            "        __syncthreads();\n",
            "\n",
            "        // Multiply the two matrices together;\n",
            "        // each thread computes one element\n",
            "        // of the block sub-matrix\n",
            "#pragma unroll\n",
            "\n",
            "        for (int k = 0; k < BLOCK_SIZE; ++k) {\n",
            "            Csub += As[ty][k] * Bs[k][tx];\n",
            "        }\n",
            "\n",
            "        // Synchronize to make sure that the preceding\n",
            "        // computation is done before loading two new\n",
            "        // sub-matrices of A and B in the next iteration\n",
            "        __syncthreads();\n",
            "    }\n",
            "\n",
            "    // Write the block sub-matrix to device memory;\n",
            "    // each thread writes one element\n",
            "    int c = wB * BLOCK_SIZE * by + BLOCK_SIZE * bx;\n",
            "    C[c + wB * ty + tx] = Csub;\n",
            "}\n",
            "\n",
            "void ConstantInit(float *data, int size, float val) {\n",
            "    for (int i = 0; i < size; ++i) {\n",
            "        data[i] = val;\n",
            "    }\n",
            "}\n",
            "\n",
            "/**\n",
            " * Run a simple test of matrix multiplication using CUDA\n",
            " */\n",
            "int MatrixMultiply(int argc, char **argv,\n",
            "                   int block_size, const dim3 &dimsA,\n",
            "                   const dim3 &dimsB) {\n",
            "    // Allocate host memory for matrices A and B\n",
            "    unsigned int size_A = dimsA.x * dimsA.y;\n",
            "    unsigned int mem_size_A = sizeof(float) * size_A;\n",
            "    float *h_A;\n",
            "    checkCudaErrors(cudaMallocHost(&h_A, mem_size_A));\n",
            "    unsigned int size_B = dimsB.x * dimsB.y;\n",
            "    unsigned int mem_size_B = sizeof(float) * size_B;\n",
            "    float *h_B;\n",
            "    checkCudaErrors(cudaMallocHost(&h_B, mem_size_B));\n",
            "    cudaStream_t stream;\n",
            "\n",
            "    // Initialize host memory\n",
            "    const float valB = 0.01f;\n",
            "    ConstantInit(h_A, size_A, 1.0f);\n",
            "    ConstantInit(h_B, size_B, valB);\n",
            "\n",
            "    // Allocate device memory\n",
            "    float *d_A, *d_B, *d_C;\n",
            "\n",
            "    // Allocate host matrix C\n",
            "    dim3 dimsC(dimsB.x, dimsA.y, 1);\n",
            "    unsigned int mem_size_C = dimsC.x * dimsC.y * sizeof(float);\n",
            "    float *h_C;\n",
            "    checkCudaErrors(cudaMallocHost(&h_C, mem_size_C));\n",
            "\n",
            "    if (h_C == NULL) {\n",
            "        fprintf(stderr, \"Failed to allocate host matrix C!\\n\");\n",
            "        exit(EXIT_FAILURE);\n",
            "    }\n",
            "\n",
            "    checkCudaErrors(cudaMalloc(reinterpret_cast<void **>(&d_A), mem_size_A));\n",
            "    checkCudaErrors(cudaMalloc(reinterpret_cast<void **>(&d_B), mem_size_B));\n",
            "    checkCudaErrors(cudaMalloc(reinterpret_cast<void **>(&d_C), mem_size_C));\n",
            "    // Allocate CUDA events that we'll use for timing\n",
            "    cudaEvent_t start, stop;\n",
            "    checkCudaErrors(cudaEventCreate(&start));\n",
            "    checkCudaErrors(cudaEventCreate(&stop));\n",
            "\n",
            "    checkCudaErrors(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));\n",
            "\n",
            "    // copy host memory to device\n",
            "    checkCudaErrors(cudaMemcpyAsync(d_A, h_A, mem_size_A, cudaMemcpyHostToDevice, stream));\n",
            "    checkCudaErrors(cudaMemcpyAsync(d_B, h_B, mem_size_B, cudaMemcpyHostToDevice, stream));\n",
            "\n",
            "    // Setup execution parameters\n",
            "    dim3 threads(block_size, block_size);\n",
            "    dim3 grid(dimsB.x / threads.x, dimsA.y / threads.y);\n",
            "\n",
            "    // Create and start timer\n",
            "    printf(\"Computing result using CUDA Kernel...\\n\");\n",
            "\n",
            "    // Performs warmup operation using matrixMul CUDA kernel\n",
            "    if (block_size == 16) {\n",
            "        MatrixMulCUDA<16> <<< grid, threads, 0, stream>>>(d_C, d_A, d_B,\n",
            "                                                dimsA.x, dimsB.x);\n",
            "    } else {\n",
            "        MatrixMulCUDA<32> <<< grid, threads, 0, stream>>>(d_C, d_A, d_B,\n",
            "                                                dimsA.x, dimsB.x);\n",
            "    }\n",
            "\n",
            "    printf(\"done\\n\");\n",
            "    checkCudaErrors(cudaStreamSynchronize(stream));\n",
            "\n",
            "    // Record the start event\n",
            "    checkCudaErrors(cudaEventRecord(start, stream));\n",
            "\n",
            "    // Execute the kernel\n",
            "    int nIter = 300;\n",
            "\n",
            "    for (int j = 0; j < nIter; j++) {\n",
            "        if (block_size == 16) {\n",
            "            MatrixMulCUDA<16> <<<grid, threads, 0, stream>>>(d_C, d_A, d_B,\n",
            "                                                    dimsA.x, dimsB.x);\n",
            "        } else {\n",
            "            MatrixMulCUDA<32> <<<grid, threads, 0, stream>>>(d_C, d_A, d_B,\n",
            "                                                    dimsA.x, dimsB.x);\n",
            "        }\n",
            "    }\n",
            "\n",
            "    // Record the stop event\n",
            "    checkCudaErrors(cudaEventRecord(stop, stream));\n",
            "\n",
            "    // Wait for the stop event to complete\n",
            "    checkCudaErrors(cudaEventSynchronize(stop));\n",
            "\n",
            "    float msecTotal = 0.0f;\n",
            "    checkCudaErrors(cudaEventElapsedTime(&msecTotal, start, stop));\n",
            "\n",
            "    // Compute and print the performance\n",
            "    float msecPerMatrixMul = msecTotal / nIter;\n",
            "    double flopsPerMatrixMul = 2.0 * static_cast<double>(dimsA.x) *\n",
            "                               static_cast<double>(dimsA.y) *\n",
            "                               static_cast<double>(dimsB.x);\n",
            "    double gigaFlops = (flopsPerMatrixMul * 1.0e-9f) /\n",
            "                       (msecPerMatrixMul / 1000.0f);\n",
            "    printf(\n",
            "        \"Performance= %.2f GFlop/s, Time= %.3f msec, Size= %.0f Ops,\" \\\n",
            "        \" WorkgroupSize= %u threads/block\\n\",\n",
            "        gigaFlops,\n",
            "        msecPerMatrixMul,\n",
            "        flopsPerMatrixMul,\n",
            "        threads.x * threads.y);\n",
            "\n",
            "    // Copy result from device to host\n",
            "    checkCudaErrors(cudaMemcpyAsync(h_C, d_C, mem_size_C, cudaMemcpyDeviceToHost, stream));\n",
            "    checkCudaErrors(cudaStreamSynchronize(stream));\n",
            "\n",
            "    printf(\"Checking computed result for correctness: \");\n",
            "    bool correct = true;\n",
            "\n",
            "    // test relative error by the formula\n",
            "    //     |<x, y>_cpu - <x,y>_gpu|/<|x|, |y|>  < eps\n",
            "    double eps = 1.e-6;  // machine zero\n",
            "\n",
            "    for (int i = 0; i < static_cast<int>(dimsC.x * dimsC.y); i++) {\n",
            "        double abs_err = fabs(h_C[i] - (dimsA.x * valB));\n",
            "        double dot_length = dimsA.x;\n",
            "        double abs_val = fabs(h_C[i]);\n",
            "        double rel_err = abs_err / abs_val / dot_length;\n",
            "\n",
            "        if (rel_err > eps) {\n",
            "            printf(\"Error! Matrix[%05d]=%.8f, ref=%.8f error term is > %E\\n\",\n",
            "                   i, h_C[i], dimsA.x * valB, eps);\n",
            "            correct = false;\n",
            "        }\n",
            "    }\n",
            "\n",
            "    printf(\"%s\\n\", correct ? \"Result = PASS\" : \"Result = FAIL\");\n",
            "\n",
            "    // Clean up memory\n",
            "    checkCudaErrors(cudaFreeHost(h_A));\n",
            "    checkCudaErrors(cudaFreeHost(h_B));\n",
            "    checkCudaErrors(cudaFreeHost(h_C));\n",
            "    checkCudaErrors(cudaFree(d_A));\n",
            "    checkCudaErrors(cudaFree(d_B));\n",
            "    checkCudaErrors(cudaFree(d_C));\n",
            "    checkCudaErrors(cudaEventDestroy(start));\n",
            "    checkCudaErrors(cudaEventDestroy(stop));\n",
            "    printf(\"\\nNOTE: The CUDA Samples are not meant for performance\"\\\n",
            "           \"measurements. Results may vary when GPU Boost is enabled.\\n\");\n",
            "\n",
            "    if (correct) {\n",
            "        return EXIT_SUCCESS;\n",
            "    } else {\n",
            "        return EXIT_FAILURE;\n",
            "    }\n",
            "}\n",
            "\n",
            "\n",
            "/**\n",
            " * Program main\n",
            " */\n",
            "int main(int argc, char **argv) {\n",
            "    printf(\"[Matrix Multiply Using CUDA] - Starting...\\n\");\n",
            "\n",
            "    if (checkCmdLineFlag(argc, (const char **)argv, \"help\") ||\n",
            "            checkCmdLineFlag(argc, (const char **)argv, \"?\")) {\n",
            "        printf(\"Usage -device=n (n >= 0 for deviceID)\\n\");\n",
            "        printf(\"      -wA=WidthA -hA=HeightA (Width x Height of Matrix A)\\n\");\n",
            "        printf(\"      -wB=WidthB -hB=HeightB (Width x Height of Matrix B)\\n\");\n",
            "        printf(\"  Note: Outer matrix dimensions of A & B matrices\" \\\n",
            "               \" must be equal.\\n\");\n",
            "\n",
            "        exit(EXIT_SUCCESS);\n",
            "    }\n",
            "\n",
            "    // This will pick the best possible CUDA capable device, otherwise\n",
            "    // override the device ID based on input provided at the command line\n",
            "    int dev = findCudaDevice(argc, (const char **)argv);\n",
            "\n",
            "    int block_size = 32;\n",
            "\n",
            "    dim3 dimsA(5 * 2 * block_size, 5 * 2 * block_size, 1);\n",
            "    dim3 dimsB(5 * 4 * block_size, 5 * 2 * block_size, 1);\n",
            "\n",
            "    // width of Matrix A\n",
            "    if (checkCmdLineFlag(argc, (const char **)argv, \"wA\")) {\n",
            "        dimsA.x = getCmdLineArgumentInt(argc, (const char **)argv, \"wA\");\n",
            "    }\n",
            "\n",
            "    // height of Matrix A\n",
            "    if (checkCmdLineFlag(argc, (const char **)argv, \"hA\")) {\n",
            "        dimsA.y = getCmdLineArgumentInt(argc, (const char **)argv, \"hA\");\n",
            "    }\n",
            "\n",
            "    // width of Matrix B\n",
            "    if (checkCmdLineFlag(argc, (const char **)argv, \"wB\")) {\n",
            "        dimsB.x = getCmdLineArgumentInt(argc, (const char **)argv, \"wB\");\n",
            "    }\n",
            "\n",
            "    // height of Matrix B\n",
            "    if (checkCmdLineFlag(argc, (const char **)argv, \"hB\")) {\n",
            "        dimsB.y = getCmdLineArgumentInt(argc, (const char **)argv, \"hB\");\n",
            "    }\n",
            "\n",
            "    if (dimsA.x != dimsB.y) {\n",
            "        printf(\"Error: outer matrix dimensions must be equal. (%d != %d)\\n\",\n",
            "               dimsA.x, dimsB.y);\n",
            "        exit(EXIT_FAILURE);\n",
            "    }\n",
            "\n",
            "    printf(\"MatrixA(%d,%d), MatrixB(%d,%d)\\n\", dimsA.x, dimsA.y,\n",
            "                                               dimsB.x, dimsB.y);\n",
            "\n",
            "    int matrix_result = MatrixMultiply(argc, argv, block_size, dimsA, dimsB);\n",
            "\n",
            "    exit(matrix_result);\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFPG0ssAl_lY"
      },
      "source": [
        ""
      ],
      "execution_count": 65,
      "outputs": []
    }
  ]
}